{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "53949bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rag_pinecone\n",
    "from rag_pinecone import Rag_pinecone,create_embeddings\n",
    "%run rag_pinecone.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "04463fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = Rag_pinecone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d516639e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "    {\n",
       "        \"name\": \"tweens\",\n",
       "        \"metric\": \"dotproduct\",\n",
       "        \"host\": \"tweens-17uwznq.svc.aped-4627-b74a.pinecone.io\",\n",
       "        \"spec\": {\n",
       "            \"serverless\": {\n",
       "                \"cloud\": \"aws\",\n",
       "                \"region\": \"us-east-1\"\n",
       "            }\n",
       "        },\n",
       "        \"status\": {\n",
       "            \"ready\": true,\n",
       "            \"state\": \"Ready\"\n",
       "        },\n",
       "        \"vector_type\": \"dense\",\n",
       "        \"dimension\": 384,\n",
       "        \"deletion_protection\": \"disabled\",\n",
       "        \"tags\": null\n",
       "    }\n",
       "]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg.get_list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b70907c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{\n",
       "     \"name\": \"first_tokenization_namespace1\",\n",
       "     \"record_count\": \"116\"\n",
       " },\n",
       " {\n",
       "     \"name\": \"second_tokenization_namespace2\",\n",
       "     \"record_count\": \"116\"\n",
       " },\n",
       " {\n",
       "     \"name\": \"third_tokenization_namespace3\",\n",
       "     \"record_count\": \"116\"\n",
       " }]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg.get_list_namespaces(\"tweens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f1509f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg.create_index(\"tweens\",\"all-MiniLM-L6-v2\",384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4684912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg.set_index(\"tweens\")\n",
    "rg.set_namespace(\"third_tokenization_namespace3\")\n",
    "rg.insert_embeddings(\n",
    "            name_tokens= \"TOKENS_THESIS\",             ### TOKEN_THESIS \n",
    "            name_tokenization= \"first_tokenization\",  ### FOR EACH TOKES EXIST A TOKENIZATION \n",
    "            embeding_function= create_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6561a274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg.insert_embeddings(\n",
    "            name_tokens= \"TOKENS_THESIS\",             ### TOKEN_THESIS \n",
    "            name_tokenization= \"first_tokenization\",  ### FOR EACH TOKES EXIST A TOKENIZATION \n",
    "            name_index= \"tweens\", \n",
    "            namespace= \"second_tokenization_namespace2\",\n",
    "            embeding_function= create_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8a621cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug1\n",
      "debug2\n",
      "debug2.2\n",
      "debug2.3\n",
      "debug.2.4\n",
      "debug2.5\n",
      "debug2.6\n",
      "debug2.1\n",
      "debug3\n"
     ]
    }
   ],
   "source": [
    "rg.set_index(\"tweens\")\n",
    "rg.set_namespace(\"third_tokenization_namespace3\")\n",
    "rg.set_client(\"gpt-4o-mini\")\n",
    "system_template = \"\"\"you are expert testing reinforcement learning using NLP techniques\n",
    "and you technique is using a conditioned transformer\"\"\"\n",
    "rg.set_system_template(system_template=system_template)\n",
    "result = rg.search_query(\"what is conditioned transformer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "537d636d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '3. MarTest: An NLP Approach for test sequence generation3.4. Model-Conditioned TransformerIn this approach, any natural language processing (NLP) technique can be employed to generateabstract sequences associated with a certain probability of failure. However, as previously stated,this work proposes the use of a conditioned transformer to identify risk regions.Given the dataset D, composed of 3-tuples (ˆτ,¯r,Pf ), we have a training set for a Model-Conditioned Transformer, as described in Section 2.2.1. Once the conditioned transformer istrained, it can be used to generate abstract sequences given a rewardr and a failure probabilityPf.These sequences, composed of abstract statesˆS interpreted by the transformer as tokensw,can also be seen as regions associated with a certain likelihood of failure and low reward. Theselection and handling of these regions may vary depending on the objective. In Section 5.4, we'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"matches\"][0][\"metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cae56063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3. MarTest: An NLP Approach for test sequence generation3.4. Model-Conditioned TransformerIn this approach, any natural language processing (NLP) technique can be employed to generateabstract sequences associated with a certain probability of failure. However, as previously stated,this work proposes the use of a conditioned transformer to identify risk regions.Given the dataset D, composed of 3-tuples (ˆτ,¯r,Pf ), we have a training set for a Model-Conditioned Transformer, as described in Section 2.2.1. Once the conditioned transformer istrained, it can be used to generate abstract sequences given a rewardr and a failure probabilityPf.These sequences, composed of abstract statesˆS interpreted by the transformer as tokensw,can also be seen as regions associated with a certain likelihood of failure and low reward. Theselection and handling of these regions may vary depending on the objective. In Section 5.4, we \n",
      " --------------------------------------------------\n",
      "1 2.2. Natural Language ProcessingFigure 2.2: Transformer achitecture, Improving Language Understanding by Generative Pre-Training.Model-Conditioned TransformerIn this work, we employ the Model-Conditioned Transformer, a GPT-based architecture thatprocesses no only conventional word token but also additional conditioning variables. Analogousto music-generation models that incorporate parameters such as tempo and velocity, our modelreceives two supplementary numerical inputs:1. Performance reward.2. Failure probability.Both values are generated during agent execution: the performance reward quantifies theagent’s effectiveness, while the failure probability estimates the likelihood of erroneous actions.Specifically, both the performance reward and the failure probability are first processed by anauxiliary neural network to produce conditioning vectors. These vectors are then added element- \n",
      " --------------------------------------------------\n",
      "2 handwriting generation, machine translation, image captioning, and parsing [5].TransformerThe Transformer architecture (Vaswani et al., 2017), introduced in the seminal paper \"AttentionIs All You Need\" (2017), revolutionized the field of NLP and later impacted other domains likecomputer vision. It replaced traditional sequential processing models, such as RNNs and LSTMs,by introducing self-attention mechanisms, which enable efficient parallelization and long-rangedependency modeling see Figure 2.1.7 \n",
      " --------------------------------------------------\n",
      "3 viiiList of Figures2.1. The transformer architecture, Attention Is All You Need, 2017. . . . . . . . . . . 82.2. Transformer achitecture, Improving Language Understanding by Generative Pre-Training. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92.3. ConditionedTransformer, rewardˆrandfailureprobability Pf arefirstprocessedbyan auxiliary neural network, then added element-wise to the word-token embeddings. 104.1. Martest pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165.1. CartPole-v0. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235.2. LunarLander-v2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255.3. Training and Evaluation Loss per Epoch, Shakti . . . . . . . . . . . . . . . . . . 275.4. Training and Evaluation Loss per Epoch, Kapil . . . . . . . . . . . . . . . . . . . 27 \n",
      " --------------------------------------------------\n",
      "4 5.3. Conditioned transformer configurations and metricsFigure 5.2: LunarLander-v2.environment based on Python 3.7. The required dependencies are listed in Appendix A.6.5.2.3. Sigve Rokenes (rokenes)RankednineteenontheOpenAIGymleaderboard, thisagentisreportedtosolvetheenvironmentin 1,590 episodes. The code [12, rokenes], dated from January 2019, requires setting up a customenvironment based on Python 3.7. The necessary dependencies are listed in Appendix A.8.5.3. Conditioned transformer configurations and metricsNext, we present the configurations and training results of the conditioned transformer for eachexperiment. We reportvocab_size, the number of tokens in the model’s vocabulary;n_embd, thedimensionality of the token embeddings and hidden states;n_layer, the number of transformerdecoder blocks (also called layers); and n_head, the number of attention heads in each self-attention layer.25 \n",
      " --------------------------------------------------\n",
      "5 interactions symbolically.• Chapter 3 – Approach:Describes the conceptual framework of the proposed method,including the symbolic vocabulary, abstract corpus construction, and the output modelbased on a conditioned transformer. Assumptions and design decisions are also discussed.• Chapter 4 – Implementation:Presents the detailed structure of theMarTest-Pipeline,explaining each module in the pipeline, from the processing of logs and Q-values to thegeneration of abstract episodes, classification models, and the conditioned transformer usedto produce test case regions.• Chapter 5 – Experimental Design: Details the experimental setup, including theselected agents from theCartPole and LunarLander environments. It also describes howconcrete test cases are generated from the abstract representations and evaluates the agentsusing statistical analysis. The chapter concludes with a discussion on threats to validity. \n",
      " --------------------------------------------------\n",
      "6 2.2. Natural Language ProcessingRNNs process data sequentially, meaning the output at a given step depends not only on thecurrent input but also on information from previous steps, they have a hidden state that servesas a \"memory,\" capturing information from previous time steps.The architecture for RNNs given a input sequenceX = [x1,x2,...,x T] where T is the sequencelength, we have:ht = f(Wtht−1 + Wxxt + b)yt = g(Wyht + c)Where Wt,Wx,Wy,b,c are parameters of the model andg is the output activation function(e.g, softmax for classification)LSTMsLong Short-Term Memory (LSTMs) are a type of Recurrent Neural Network (RNN) knownas gated RNNs. They are based on the idea of introducing self-loops to create paths throughwhich the gradient can flow for long durations. LSTMs have been found to be extremely suc-cessful in many applications, such as unconstrained handwriting recognition, speech recognition, \n",
      " --------------------------------------------------\n",
      "7 previous module. Once the model is trained, we can also generate abstract sequences given areward valuer and a failure probabilityPf, thereby identifying regions with potential risk offailure or low reward, as described in Section 3.4.We use thetransformers library fromHugging Face, which integrates with PyTorch. Specifi-cally, we base our implementation on thetransformers.GPT2LMHeadModel class, which followsthe GPT-2 architecture—a decoder-only Transformer commonly used for text generation. Wemodified this architecture to also receive the average reward and failure probability as inputs inorder to predict tokens (i.e., abstract states or regions) associated with specific levels of risk orfailure likelihood. We refer to this modified model as theConditioned Transformer.The Conditioned Transformer is implemented by extending theGPT2LMHeadModel class to18 \n",
      " --------------------------------------------------\n",
      "8 2. BackgroundFigure 2.3: Conditioned Transformer, rewardˆr and failure probabilityPf are first processed byan auxiliary neural network, then added element-wise to the word-token embeddings.2.3. Abstract ClassesThe definition of Abstracts Classes relies in the definition of equivalences class, a partition of aset. A State Abstraction is defined as a mapping from an original states ∈S to an abstractstate sϕ ∈Sϕϕ: S →Sϕwhere Sϕ ∈P(S) and fulfills equivalence conditions (reflexive, symmetric, and transitive) [1].2.3.1. Definition π∗-irrelevance abstractions1 and s2 are in the same abstract classϕ(s1) = ϕ(s2) if π∗(s1) = π(s2) where π∗is the optimalpolicy.2.3.2. Definition Q∗-irrelevance abstractions1 ands2 are in the same abstract classϕ(s1) = ϕ(s2) if for all actiona∈A, Q∗(s1,a) = Q∗(s2,a)where Q∗(s,a) is the optimalQ-value function the maximum expected reward.2.3.3. Relax Q∗-irrelevance abstractions1 and s2 are in the same abstract classϕ(s1) = ϕ(s2) if: \n",
      " --------------------------------------------------\n",
      "9 4.6. Module 5a new class namedCondGPT2. The core idea is to preprocess the average reward and failureprobability through a small feedforward network, producing a vector representation that is thenadded to the token embeddings of the abstract sequence before being passed to the GPT-2 model.For further details, see Section A.2.4.6.2. Test Regions CasesAt the end of the pipeline, we obtain an output in the form of a list ofpossible abstractepisodes—that is, sequences of abstract states and actions (e.g.,’w2 1 w1 2 w3 1 w6 ... w51 True’) interpreted by the transformer astokens or words. However, since these tokenswi represent abstract classes, they can also be understood assequences of regions andactions.Given that the transformer has been designed to receive the average reward and fail-ure probabilityas input parameters, we interpret these region–action sequences aspossibleepisodes conditioned on those parameters. \n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i,text in enumerate(result[\"matches\"]):\n",
    "    print(i,text[\"metadata\"][\"text\"],\"\\n\",\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c451c98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a5efd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key = os.getenv(\"pinecone_token\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0fc1a583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3556d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(\"tweens\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
