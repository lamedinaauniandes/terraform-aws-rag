{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53949bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rag_pinecone\n",
    "from rag_pinecone import Rag_tools\n",
    "from embedding_functions import (create_embeddings,create_embeddings_openai)\n",
    "%run rag_pinecone.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04463fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_tools = Rag_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d516639e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "    {\n",
       "        \"name\": \"rag-openai-3small\",\n",
       "        \"metric\": \"dotproduct\",\n",
       "        \"host\": \"rag-openai-3small-17uwznq.svc.aped-4627-b74a.pinecone.io\",\n",
       "        \"spec\": {\n",
       "            \"serverless\": {\n",
       "                \"cloud\": \"aws\",\n",
       "                \"region\": \"us-east-1\"\n",
       "            }\n",
       "        },\n",
       "        \"status\": {\n",
       "            \"ready\": true,\n",
       "            \"state\": \"Ready\"\n",
       "        },\n",
       "        \"vector_type\": \"dense\",\n",
       "        \"dimension\": 256,\n",
       "        \"deletion_protection\": \"disabled\",\n",
       "        \"tags\": null\n",
       "    },\n",
       "    {\n",
       "        \"name\": \"tweens\",\n",
       "        \"metric\": \"dotproduct\",\n",
       "        \"host\": \"tweens-17uwznq.svc.aped-4627-b74a.pinecone.io\",\n",
       "        \"spec\": {\n",
       "            \"serverless\": {\n",
       "                \"cloud\": \"aws\",\n",
       "                \"region\": \"us-east-1\"\n",
       "            }\n",
       "        },\n",
       "        \"status\": {\n",
       "            \"ready\": true,\n",
       "            \"state\": \"Ready\"\n",
       "        },\n",
       "        \"vector_type\": \"dense\",\n",
       "        \"dimension\": 384,\n",
       "        \"deletion_protection\": \"disabled\",\n",
       "        \"tags\": null\n",
       "    }\n",
       "]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg_tools.get_list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70907c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{\n",
       "     \"name\": \"firt_tokenization_namespace_open_ai\",\n",
       "     \"record_count\": \"116\"\n",
       " }]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg_tools.get_list_namespaces(\"rag-openai-3small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fa49c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_tools.set_index(\"rag-openai-3small\")\n",
    "rg_tools.set_namespace(\"firt_tokenization_namespace_open_ai\")\n",
    "rg_tools.insert_embeddings(\n",
    "            name_tokens= \"TOKENS_THESIS\",             ### TOKEN_THESIS \n",
    "            name_tokenization= \"first_tokenization\",  ### FOR EACH TOKES EXIST A TOKENIZATION \n",
    "            embeding_function= create_embeddings_openai,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4684912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg_tools.set_index(\"tweens\")\n",
    "rg_tools.set_namespace(\"_tokenization_namespace3\")\n",
    "rg_tools.insert_embeddings(\n",
    "            name_tokens= \"TOKENS_THESIS\",             ### TOKEN_THESIS \n",
    "            name_tokenization= \"first_tokenization\",  ### FOR EACH TOKES EXIST A TOKENIZATION \n",
    "            embeding_function= create_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6561a274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg_tools.insert_embeddings(\n",
    "            name_tokens= \"TOKENS_THESIS\",             ### TOKEN_THESIS \n",
    "            name_tokenization= \"first_tokenization\",  ### FOR EACH TOKES EXIST A TOKENIZATION \n",
    "            name_index= \"tweens\", \n",
    "            namespace= \"second_tokenization_namespace2\",\n",
    "            embeding_function= create_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a621cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Laboratorio\\Landing\\terraform-aws-rag\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        debug -> function embedding: \n",
      "        256\n",
      "        ['what is conditioned transformer? A conditioned transformer refers to a variant of the transformer architecture that is designed to generate outputs based on certain conditioning inputs. Transformers, originally introduced in the paper \"Attention is All You Need,\" have become the foundation for many natural language processing (NLP) models due to their ability to capture complex relationships in sequence data through the use of self-attention mechanisms.\\n\\nIn a conditioned transformer setup, the model is conditioned on specific information or context to guide its output generation. This conditioning can take various forms, including:\\n\\n1. **Text Conditioning**: Providing a prompt or context in the form of text that influences the generated response. For example, starting with a specific phrase or question can condition the output to follow that theme.\\n\\n2. **Task Conditioning**: Modifying the model to handle specific tasks by providing indicators or tokens that signal the nature of the task (e.g., summarization, translation, question-answering).\\n\\n3. **Additional Inputs**: Incorporating other forms of input, such as structured data, user preferences, or previous interactions, to guide the model\\'s behavior in a reinforcement learning context.\\n\\nConditioned transformers are useful for a range of applications, allowing for more controlled and context-aware communications, enhancing performance in tasks requiring fine-tuning based on user inputs, and improving generalization in reinforcement learning scenarios.\\n\\nBy using conditioning effectively, it is possible to leverage the powerful capabilities of transformers while mitigating some of the challenges in generating coherent, contextually appropriate responses.']\n",
      "        text-embedding-3-small\n",
      "     \n"
     ]
    }
   ],
   "source": [
    "import rag_pinecone\n",
    "from rag_pinecone import Rag_queries\n",
    "from embedding_functions import (create_embeddings,create_embeddings_openai)\n",
    "%run rag_pinecone.py\n",
    "rg = Rag_queries()\n",
    "rg.set_index(\"rag-openai-3small\")\n",
    "rg.set_namespace(\"firt_tokenization_namespace_open_ai\")\n",
    "rg.set_client(\"gpt-4o-mini\")\n",
    "system_template = \"\"\"you are expert testing reinforcement learning using NLP techniques\n",
    "and you technique is using a conditioned transformer\"\"\"\n",
    "rg.set_system_template(system_template=system_template)\n",
    "result = rg.search_query(\"what is conditioned transformer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cae56063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.2. Natural Language ProcessingFigure 2.2: Transformer achitecture, Improving Language Understanding by Generative Pre-Training.Model-Conditioned TransformerIn this work, we employ the Model-Conditioned Transformer, a GPT-based architecture thatprocesses no only conventional word token but also additional conditioning variables. Analogousto music-generation models that incorporate parameters such as tempo and velocity, our modelreceives two supplementary numerical inputs:1. Performance reward.2. Failure probability.Both values are generated during agent execution: the performance reward quantifies theagent’s effectiveness, while the failure probability estimates the likelihood of erroneous actions.Specifically, both the performance reward and the failure probability are first processed by anauxiliary neural network to produce conditioning vectors. These vectors are then added element- \n",
      " --------------------------------------------------\n",
      "1 and transformer-based models. Rather than conditioning the transformer on simple rewardstatistics, future models could be trained to evaluate or even generate abstract behavior trajec-tories as structured linguistic sequences—potentially assigning meaning and structure to patternsof interaction.45 \n",
      " --------------------------------------------------\n",
      "2 3. MarTest: An NLP Approach for test sequence generation3.4. Model-Conditioned TransformerIn this approach, any natural language processing (NLP) technique can be employed to generateabstract sequences associated with a certain probability of failure. However, as previously stated,this work proposes the use of a conditioned transformer to identify risk regions.Given the dataset D, composed of 3-tuples (ˆτ,¯r,Pf ), we have a training set for a Model-Conditioned Transformer, as described in Section 2.2.1. Once the conditioned transformer istrained, it can be used to generate abstract sequences given a rewardr and a failure probabilityPf.These sequences, composed of abstract statesˆS interpreted by the transformer as tokensw,can also be seen as regions associated with a certain likelihood of failure and low reward. Theselection and handling of these regions may vary depending on the objective. In Section 5.4, we \n",
      " --------------------------------------------------\n",
      "3 viiiList of Figures2.1. The transformer architecture, Attention Is All You Need, 2017. . . . . . . . . . . 82.2. Transformer achitecture, Improving Language Understanding by Generative Pre-Training. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92.3. ConditionedTransformer, rewardˆrandfailureprobability Pf arefirstprocessedbyan auxiliary neural network, then added element-wise to the word-token embeddings. 104.1. Martest pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165.1. CartPole-v0. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235.2. LunarLander-v2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255.3. Training and Evaluation Loss per Epoch, Shakti . . . . . . . . . . . . . . . . . . 275.4. Training and Evaluation Loss per Epoch, Kapil . . . . . . . . . . . . . . . . . . . 27 \n",
      " --------------------------------------------------\n",
      "4 5.3. Conditioned transformer configurations and metricsFigure 5.2: LunarLander-v2.environment based on Python 3.7. The required dependencies are listed in Appendix A.6.5.2.3. Sigve Rokenes (rokenes)RankednineteenontheOpenAIGymleaderboard, thisagentisreportedtosolvetheenvironmentin 1,590 episodes. The code [12, rokenes], dated from January 2019, requires setting up a customenvironment based on Python 3.7. The necessary dependencies are listed in Appendix A.8.5.3. Conditioned transformer configurations and metricsNext, we present the configurations and training results of the conditioned transformer for eachexperiment. We reportvocab_size, the number of tokens in the model’s vocabulary;n_embd, thedimensionality of the token embeddings and hidden states;n_layer, the number of transformerdecoder blocks (also called layers); and n_head, the number of attention heads in each self-attention layer.25 \n",
      " --------------------------------------------------\n",
      "5 handwriting generation, machine translation, image captioning, and parsing [5].TransformerThe Transformer architecture (Vaswani et al., 2017), introduced in the seminal paper \"AttentionIs All You Need\" (2017), revolutionized the field of NLP and later impacted other domains likecomputer vision. It replaced traditional sequential processing models, such as RNNs and LSTMs,by introducing self-attention mechanisms, which enable efficient parallelization and long-rangedependency modeling see Figure 2.1.7 \n",
      " --------------------------------------------------\n",
      "6 previous module. Once the model is trained, we can also generate abstract sequences given areward valuer and a failure probabilityPf, thereby identifying regions with potential risk offailure or low reward, as described in Section 3.4.We use thetransformers library fromHugging Face, which integrates with PyTorch. Specifi-cally, we base our implementation on thetransformers.GPT2LMHeadModel class, which followsthe GPT-2 architecture—a decoder-only Transformer commonly used for text generation. Wemodified this architecture to also receive the average reward and failure probability as inputs inorder to predict tokens (i.e., abstract states or regions) associated with specific levels of risk orfailure likelihood. We refer to this modified model as theConditioned Transformer.The Conditioned Transformer is implemented by extending theGPT2LMHeadModel class to18 \n",
      " --------------------------------------------------\n",
      "7 2. BackgroundFigure 2.1: The transformer architecture, Attention Is All You Need, 2017.Transformer decoder onlyTheTransformerDecoder-OnlyArchitecture, popularizedbyGPT(GenerativePre-trainedTrans-former), is a variant of the original Transformer architecture introduced by Vaswani et al. in\"Attention is All You Need\" (2017). GPT uses only the decoder part of the Transformer for itsdesign, focusing on autoregressive text generation tasks[10]. For the achitecture see Figure 2.28 \n",
      " --------------------------------------------------\n",
      "8 extend the definition of our dataset.3.3. DatasetLet D be the training dataset for the conditioned Transformer model, such that(ˆτ,¯r,Pf ) ∈Dif ˆτ ∈ ˆDϕ, ¯r is the average reward ofˆτ, andPf is the failure probability ofˆτ.3.3.1. Average rewardSince ˆτ may represent multiple concrete sequences—that is, it may happen thatϕ(τ1) = ϕ(τ2) =··· = ϕ(τn) = ˆτ for sequences τ1,τ2,...,τ n of state-action tuples—we take the total rewardsassociated with eachτi and compute their average to obtain the mean reward¯r.3.3.2. Failure probabilityWe can associate eachˆτ ∈ ˆDϕ with a failure probability. Given a sequence of concrete trajectoriesτ1,τ2,...,τ n executed by an agent, we know that each trajectory either resulted in failure or not.12 \n",
      " --------------------------------------------------\n",
      "9 environment. Lastly, incorporating richer conditioning signals into the Transformer, or exploringarchitectures specifically tailored for causal reasoning in RL environments, may improve thequality of the generated test cases and help uncover more nuanced weaknesses in agent policies.This work was guided by the intuition thatreward may be a consequence of a shared languagebetween the agent and the environment. While this idea is still in an early and exploratory stage,it suggests several possible directions for future research.One such direction is to investigate the relationship between the level of abstraction usedto represent agent-environment interactions and the rewards obtained. It may be possible toformulate an optimization process—either analytical or learned—that selects the abstractionlevel most appropriate for a given task or agent architecture.Another line of inquiry involves a closer integration between abstract symbolic representations \n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i,text in enumerate(result[\"matches\"]):\n",
    "    print(i,text[\"metadata\"][\"text\"],\"\\n\",\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c451c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding_functions import create_embeddings_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e7182f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAIEmbeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcreate_embeddings_openai\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhola mundo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-embedding-3-small\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Laboratorio\\Landing\\terraform-aws-rag\\pinecone_rag\\1_codes_rag_pinecone\\embedding_functions.py:16\u001b[39m, in \u001b[36mcreate_embeddings_openai\u001b[39m\u001b[34m(tokens, model, **kwargs)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_embeddings_openai\u001b[39m(tokens,model,**kwargs): \n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" \u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m    generate embeddings with open AI \u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m     model  = OpenAIEmbeddings(\n\u001b[32m     20\u001b[39m         model=model,\n\u001b[32m     21\u001b[39m         api_key= kwargs[\u001b[33m\"\u001b[39m\u001b[33mapi_key\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     22\u001b[39m     )\n\u001b[32m     23\u001b[39m     embeddings = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: model.encode(x).tolist(),tokens))\n",
      "\u001b[31mNameError\u001b[39m: name 'OpenAIEmbeddings' is not defined"
     ]
    }
   ],
   "source": [
    "create_embeddings_openai([\"hola mundo\"],\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd42bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ad358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca66a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5efd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key = os.getenv(\"pinecone_token\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc1a583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3556d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(\"tweens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be00a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "luis\n",
      "no existe el nombre\n"
     ]
    }
   ],
   "source": [
    "def proof(apellido,**kwargs): \n",
    "    if \"nombre\" in kwargs: \n",
    "        print(kwargs[\"nombre\"])\n",
    "    else: \n",
    "        print(\"no existe el nombre\")\n",
    "\n",
    "proof(nombre=\"luis\",apellido=\"medina\")\n",
    "proof(apellido=\"medina\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
