{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9bf058b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone,ServerlessSpec\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import(\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    SentenceTransformersTokenTextSplitter\n",
    ")\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f61f4dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = Path(os.getcwd()).parent.parent\n",
    "PATH_ENV = os.path.join(BASE_DIR,\".env\")\n",
    "DATA_DIRECTORY = os.path.join(BASE_DIR,r\"2_data_names_spaces\")\n",
    "TOKENS_DIRECTORY = os.path.join(DATA_DIRECTORY,r\"investigations\\tokens\")\n",
    "load_dotenv(override=True,dotenv_path=PATH_ENV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5826cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_tokenization': {'values': ['\\'\"\\\\\\'Maestría en Ingeniería \\\\\\\\\\\\\\\\nde Sistemas \\\\\\\\\\\\\\\\ny Computación\\\\\\\\\\\\\\\\nDissertation\\\\\\\\\\\\\\\\nConstructing a language for testing\\\\\\\\\\\\\\\\nReinforcement Learning programs using\\\\\\\\\\\\\\\\nNLP techniques\\\\\\\\\\\\\\\\nLUIS ALEJANDRO MEDINA\\\\\\\\\\\\\\\\nJuly 19, 2025\\\\\\\\\\\\\\\\nThis thesis is submitted in partial fulfillment of the requirements\\\\\\\\\\\\\\\\nfor a degree of Master in Systems and Computing Engineering\\\\\\\\\\\\\\\\n(MISIS).\\\\\\\\\\\\\\\\nThesis Committee:\\\\\\\\\\\\\\\\nProf. Nicolás Cardozo (Promotor) Universidad de los Andes, Colombia\\\\\\\\\\\\\\\\nProf. Reviewer 1Ivana Dusparic Trinity College Dublin, Ireland\\\\\\\\\\\\\\\\nProf. Reviewer 2Ruben Manrique Universidad de los Andes, Colombia\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nConstructing a language for testing Reinforcement Learning pro-\\\\\\\\\\\\\\\\ngrams using NLP techniques\\\\\\\\\\\\\\\\n© YEAR LUIS ALEJANDRO MEDINA\\\\\\\\\\\\\\\\nSystems and Computing Engineering Department\\\\\\\\\\\\\\\\nFLAG lab\\\\\\\\\\\\\\\\nFaculty of Engineering\\\\\\\\\\\\\\\\nUniversidad de los Andes\\\\\\\\\\\\\\\\nBogotá, Colombia\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'With all my heart: To my dear dad, Luis Hernando Medina,\\\\\\\\\\\\\\\\neverything I have achieved is thanks to you. To all my family,\\\\\\\\\\\\\\\\nthank you for never abandoning me. And to my beloved Martín, I\\\\\\\\\\\\\\\\nwill always love you.\\\\\\\\\\\\\\\\n... And I will always be two steps behind you. “Two Steps Behind”, Def Leppard.\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'iv\\\\\\\\\\\\\\\\nAbstract\\\\\\\\\\\\\\\\nReinforcementLearning(RL)hasgarneredsignificantattentionfromtheresearchcommunitydue\\\\\\\\\\\\\\\\nto its expanding applications across various fields, including recommendation systems, robotics,\\\\\\\\\\\\\\\\nautonomous vehicles, and more. Consequently, there has been a growing interest in testing Deep\\\\\\\\\\\\\\\\nReinforcement Learning (DRL) agents and identifying potential faults that could lead to critical\\\\\\\\\\\\\\\\nfailures during their execution.\\\\\\\\\\\\\\\\nVarious techniques exist for testing systems, including white-box, black-box, and data-quality\\\\\\\\\\\\\\\\ntesting methods. White-box techniques rely on the assumption that complete information about\\\\\\\\\\\\\\\\nthe system (e.g., code, data, and architecture) is available. Black-box techniques, on the other\\\\\\\\\\\\\\\\nhand, operate under the premise that no internal details of the system are accessible, focusing\\\\\\\\\\\\\\\\ninstead on assessing the system’s performance and correctness based solely on specified require-\\\\\\\\\\\\\\\\nments. Due to the potentially vast amount of data generated by these systems, data-quality\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'techniques leverage this information to infer system behavior and detect possible issues.\\\\\\\\\\\\\\\\nIn (RL), various methods exist for the testing and evaluation of agents. Adversarial attacks,\\\\\\\\\\\\\\\\nfocusing on perturbing the raw input of the RL agent, Mutation testing and debuggers focus\\\\\\\\\\\\\\\\non assessing the code, statistical methods for building performance metrics and Search-based\\\\\\\\\\\\\\\\ntesting focusing on the data produced by the agent as genetics algorithms, Plasticity maps, and\\\\\\\\\\\\\\\\nConstructing Language and Natural Language Process (NLP) techniques.\\\\\\\\\\\\\\\\nConstructing language models using NLP techniques for testing in Reinforcement Learning is\\\\\\\\\\\\\\\\nbased on the idea of communication between the agent and the environment, thereby establishing\\\\\\\\\\\\\\\\na shared language. An inference language is key to finding ways to detect possible agent failures,\\\\\\\\\\\\\\\\nincorrect behaviors, and low-reward outcomes.\\\\\\\\\\\\\\\\nOur evaluation uses different implementations from two RL application benchmarks—CartPole\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'and LunarLander—to generate testing scenarios for each. We analyze two key metrics: the\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'average rewardand theprobability of fault. This is important because agents can achieve\\\\\\\\\\\\\\\\nhigh average rewards that may overcompensate for cases with lower performance. However, our\\\\\\\\\\\\\\\\nresults suggest that the proposed technique is capable of identifyingregions of lower reward.\\\\\\\\\\\\\\\\nFor example, in the case of theShakti agent, the average reward during normal execution\\\\\\\\\\\\\\\\nis 185.3, whereas in our testing scenarios, it drops to 96.98. This indicates that our test case\\\\\\\\\\\\\\\\ngenerator can uncover regions of reduced performance.\\\\\\\\\\\\\\\\nOn the other hand, even for theKapil agent, which consistently shows a perfect success rate\\\\\\\\\\\\\\\\nduring normal execution, we were able to identify testing regions where the success rate drops\\\\\\\\\\\\\\\\nsignificantly—to 42.8%.\\\\\\\\\\\\\\\\nThe results indicate that the proposed test case generator was able to identify failure-prone\\\\\\\\\\\\\\\\nregions and lower rewards regions in multiple agents— even in those with perfect performance\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'under normal conditions. Statistical analysis confirmed that, for most agents, the generated test\\\\\\\\\\\\\\\\ncases led to significantly lower success rates or reduced average rewards compared to standard\\\\\\\\\\\\\\\\nexecution.\\\\\\\\\\\\\\\\nv\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'vi\\\\\\\\\\\\\\\\nContents\\\\\\\\\\\\\\\\nAbstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv\\\\\\\\\\\\\\\\nList of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii\\\\\\\\\\\\\\\\n1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\\\\\\\\\\\\\\\n2. Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\\\\\\\\\\\\\\\n2.1. Reinforment Learning, Makov process desicion (MDP) . . . . . . . . . . . . . . . 5\\\\\\\\\\\\\\\\n2.2. Natural Language Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\\\\\\\\\\\\\\\n2.2.1. Early Neural Network-Based Models . . . . . . . . . . . . . . . . . . . . . 6\\\\\\\\\\\\\\\\n2.3. Abstract Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\\\\\\\\\\\\\\\n2.3.1. Definition π∗-irrelevance abstraction . . . . . . . . . . . . . . . . . . . . . 10\\\\\\\\\\\\\\\\n2.3.2. Definition Q∗-irrelevance abstraction . . . . . . . . . . . . . . . . . . . . . 10\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'2.3.3. Relax Q∗-irrelevance abstraction . . . . . . . . . . . . . . . . . . . . . . . 10\\\\\\\\\\\\\\\\n3. MarTest: An NLP Approach for test sequence generation . . . . . . . . . . . . . . . 11\\\\\\\\\\\\\\\\n3.1. Vocabulary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\\\\\\\\\\\\\\\n3.2. Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\\\\\\\\\\\\\\\n3.3. Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\\\\\\\\\\\\\\\n3.3.1. Average reward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\\\\\\\\\\\\\\\n3.3.2. Failure probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\\\\\\\\\\\\\\\n3.3.3. Random Forest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\\\\\\\\\\\\\\\n3.4. Model-Conditioned Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\\\\\\\\\\\\\\\n3.4.1. Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'4. MarTest Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\\\\\\\\\\\\\\\n4.1. Pipeline Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\\\\\\\\\\\\\\\n4.2. Module 1: Data acquisition and preparation . . . . . . . . . . . . . . . . . . . . . 16\\\\\\\\\\\\\\\\n4.2.1. Inputs training and execution logs . . . . . . . . . . . . . . . . . . . . . . 16\\\\\\\\\\\\\\\\n4.2.2. Data Balacing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\\\\\\\\\\\\\\\n4.2.3. Q-Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\\\\\\\\\\\\\\\n4.3. Module 2: Abstract Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\\\\\\\\\\\\\\\n4.3.1. Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\\\\\\\\\\\\\\\n4.4. Module 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\\\\\\\\\\\\\\\n4.4.1. Abstract Episodes generation . . . . . . . . . . . . . . . . . . . . . . . . . 17\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'4.5. Module 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\\\\\\\\\\\\\\\n4.5.1. Random Forest Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\\\\\\\\\\\\\\\n4.6. Module 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\\\\\\\\\\\\\\\n4.6.1. Conditioned Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\\\\\\\\\\\\\\\n4.6.2. Test Regions Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\\\\\\\\\\\\\\\n5. Experimental Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\\\\\\\\\\\\\\\n5.1. CartPole-v0 Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'CONTENTS\\\\\\\\\\\\\\\\n5.2. LunarLander-v2 Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\\\\\\\\\\\\\\\n5.2.1. Sanket Thakur (sanket) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\\\\\\\\\\\\\\\n5.2.2. Mahmood Khordoo (khordoo) . . . . . . . . . . . . . . . . . . . . . . . . 24\\\\\\\\\\\\\\\\n5.2.3. Sigve Rokenes (rokenes) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\\\\\\\\\\\\\\\n5.3. Conditioned transformer configurations and metrics . . . . . . . . . . . . . . . . 25\\\\\\\\\\\\\\\\n5.3.1. Conditional transformer parameters for cartpole . . . . . . . . . . . . . . 26\\\\\\\\\\\\\\\\n5.3.2. Conditional transformer parameters for Lunar-Lander . . . . . . . . . . . 26\\\\\\\\\\\\\\\\n5.3.3. Training and validations loss . . . . . . . . . . . . . . . . . . . . . . . . . 26\\\\\\\\\\\\\\\\n5.4. Concrete Test Case Generator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\\\\\\\\\\\\\\\n5.5. An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5.6. Evaluation and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\\\\\\\\\\\\\\\n5.6.1. Cartpole . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\\\\\\\\\\\\\\\n5.6.2. Lunar lander . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\\\\\\\\\\\\\\\n5.6.3. A Small Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\\\\\\\\\\\\\\\n5.6.4. Threats to Validity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\\\\\\\\\\\\\\\n6. Conclusion, Limitations and Future Work . . . . . . . . . . . . . . . . . . . . . . . . 43\\\\\\\\\\\\\\\\n6.1. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\\\\\\\\\\\\\\\n6.2. Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\\\\\\\\\\\\\\\n6.3. Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\\\\\\\\\\\\\\\nA. Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'A.1. MartTest Pipeline Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\\\\\\\\\\\\\\\nA.2. Conditioned Transformer Model CondGPT2 . . . . . . . . . . . . . . . . . . . . . 52\\\\\\\\\\\\\\\\nA.3. Requirements Shakti . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\\\\\\\\\\\\\\\nA.4. Requirements Kapil . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\\\\\\\\\\\\\\\\nA.5. Requirements Nihal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\\\\\\\\\\\\\\\nA.6. Requirements Khordoo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\\\\\\\\\\\\\\\nA.7. Requirements Sanket . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\\\\\\\\\\\\\\\nA.8. Requirements Rokenes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\\\\\\\\\\\\\\\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\\\\\\\\\\\\\\\nvii\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'viii\\\\\\\\\\\\\\\\nList of Figures\\\\\\\\\\\\\\\\n2.1. The transformer architecture, Attention Is All You Need, 2017. . . . . . . . . . . 8\\\\\\\\\\\\\\\\n2.2. Transformer achitecture, Improving Language Understanding by Generative Pre-\\\\\\\\\\\\\\\\nTraining. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\\\\\\\\\\\\\\\n2.3. ConditionedTransformer, rewardˆrandfailureprobability Pf arefirstprocessedby\\\\\\\\\\\\\\\\nan auxiliary neural network, then added element-wise to the word-token embeddings. 10\\\\\\\\\\\\\\\\n4.1. Martest pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\\\\\\\\\\\\\\\n5.1. CartPole-v0. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\\\\\\\\\\\\\\\n5.2. LunarLander-v2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\\\\\\\\\\\\\\\n5.3. Training and Evaluation Loss per Epoch, Shakti . . . . . . . . . . . . . . . . . . 27\\\\\\\\\\\\\\\\n5.4. Training and Evaluation Loss per Epoch, Kapil . . . . . . . . . . . . . . . . . . . 27\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5.5. Training and Evaluation Loss per Epoch, Nihal . . . . . . . . . . . . . . . . . . . 28\\\\\\\\\\\\\\\\n5.6. Training and Evaluation Loss per Epoch, Sanket . . . . . . . . . . . . . . . . . . 28\\\\\\\\\\\\\\\\n5.7. Training and Evaluation Loss per Epoch, Khordoo . . . . . . . . . . . . . . . . . 29\\\\\\\\\\\\\\\\n5.8. Training and Evaluation Loss per Epoch, Rokenes . . . . . . . . . . . . . . . . . . 29\\\\\\\\\\\\\\\\n5.9. Initial state of the CartPole environment with cart position−0.73915684, cart\\\\\\\\\\\\\\\\nvelocity −0.89601650, pole angle−0.03282847, and pole angular velocity0.23753740. 37\\\\\\\\\\\\\\\\n5.10.Final state of the CartPole environment with cart position−2.41205427, cart ve-\\\\\\\\\\\\\\\\nlocity −1.42968531, pole angle−0.06067818, and pole angular velocity−0.02501499. 38\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'1\\\\\\\\\\\\\\\\nChapter 1\\\\\\\\\\\\\\\\nIntroduction\\\\\\\\\\\\\\\\nReinforment Learning (RL) and Natural Language Processing (NLP) have seen increasing inter-\\\\\\\\\\\\\\\\nest and research advances in recent years. The combination of deep learning and RL, referred to\\\\\\\\\\\\\\\\nas Deep Reinforcement Learning (DRL), has achieved success to solve various decision-making\\\\\\\\\\\\\\\\nproblems such as autonomous driving and robotics, while NLP has experienced a notable suc-\\\\\\\\\\\\\\\\ncess with the introduction of transformer architecture, particularly the decoder-only transformer\\\\\\\\\\\\\\\\nmodel and other architectures, such as Recurrent Neural Networks (RNNs) and Long Short-Term\\\\\\\\\\\\\\\\nMemory (LSTMs) with their primary distinction lying in their capacity to handle memory with\\\\\\\\\\\\\\\\nlong sequences and parallel processing.\\\\\\\\\\\\\\\\nDRL has recently been applied in many practical contexts. For instance, Netflix uses it to\\\\\\\\\\\\\\\\nrecommend which movie to show to a user in order to maximize engagement (System of recom-\\\\\\\\\\\\\\\\nmendation) [3], Microsoft developed Personalizer, a service developer that can be use for content\\\\\\'\"\\'',\n",
       "   '\\'\\\\\\'\\\\\\\\\\\\\\'recomendation [3]. Other applications include the mastery of strategy games such as Dota 2 and\\\\\\\\\\\\\\\\nStarCraft 2 in which it has defeated human players [2].\\\\\\\\\\\\\\\\nWith the introduction of the paper \"Attention is All you need\" Vaswani et al. [15] published\\\\\\\\\\\\\\\\nin 2017 where they introduce the transformer model laying down the foundation for modern\\\\\\\\\\\\\\\\nNLP and sequence modeling, such techniques have influenced the development of architectures\\\\\\\\\\\\\\\\nlike BERT, GPT, and other state-of-the-art NLP models, gaining huge popularity in all fields.\\\\\\\\\\\\\\\\nThe utility and use of NLP-based tools has been a great advantage for society and probably a\\\\\\\\\\\\\\\\nprincipal engine for progress.\\\\\\\\\\\\\\\\nFor the constantly growing use of RL models in many fields, there has been a growing need\\\\\\\\\\\\\\'\\\\\\'\\'',\n",
       "   '\\'\"\\\\\\'1. Introduction\\\\\\\\\\\\\\\\nof testing such models. Therefore, we have built a novel way for testing RL agents by bridging\\\\\\\\\\\\\\\\ntwo fields. To reconcile both fields, we need to reconcile the infinite state spaces in the case of\\\\\\\\\\\\\\\\n(continuous) RL agents, and the finite vocabulary/alphabet used in NLP models.\\\\\\\\\\\\\\\\nWe put forward a language as a means to reconcile RL and NLP. Through the execution of an\\\\\\\\\\\\\\\\nagent, this is in continuous interaction with the environment establishing a language the proposed\\\\\\\\\\\\\\\\napproach build such language is based on the mathematical concept of abstraction, often called\\\\\\\\\\\\\\\\n’equivalences classes’ or ’abstracts classes’. The vocabulary for testing an RL program consist of\\\\\\\\\\\\\\\\nabstract classes inputs for NLP models, wich then can produce scenarios (i.e., paths of abstract\\\\\\\\\\\\\\\\nstates - abstract paths) with possible fails or lower rewards.\\\\\\\\\\\\\\\\nNLP models model/generate abstracts paths, that is, sequences of state representative ab-\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'stracts classes and the actions. Afterwards we validate that the generated abstract paths indeed\\\\\\\\\\\\\\\\nare possible and lead to a diminished agent performance. To do this, we developed a test gen-\\\\\\\\\\\\\\\\nerator cases program extracts regions from abstract paths and disrupt representative states in\\\\\\\\\\\\\\\\neach class to observe the agent’s behavior.\\\\\\\\\\\\\\\\nWe evaluate our approach using agents selected from the GitHub OpenAI Gym leaderboard.\\\\\\\\\\\\\\\\nFor each agent, we first generate logs from both training and execution, which serve as inputs to\\\\\\\\\\\\\\\\nour implementation,MarTest-Pipeline. Based on the forecasted regions produced by the pipeline,\\\\\\\\\\\\\\\\nwe then construct concrete test cases to evaluate the agent’s performance.\\\\\\\\\\\\\\\\nOur results show that the implementation is able to identify regions associated with lower\\\\\\\\\\\\\\\\naverage rewards and reduced success rates. Moreover, we demonstrate that the performance on\\\\\\\\\\\\\\\\nthe testing samples is statistically significantly lower than that observed under normal execution,\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'highlighting that high performance of an agent trained in a normal execution does not guarantee\\\\\\\\\\\\\\\\nrobustness under targeted testing conditions.\\\\\\\\\\\\\\\\nThesis Outline\\\\\\\\\\\\\\\\nThe remainder of this thesis is organized as follows:\\\\\\\\\\\\\\\\n• Chapter 2 – Background:Introduces the theoretical foundations relevant to this work,\\\\\\\\\\\\\\\\nincluding reinforcement learning and Markov decision processes (MDPs), natural language\\\\\\\\\\\\\\\\nprocessing models, and various forms of abstraction used to represent agent-environment\\\\\\\\\\\\\\\\n2\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'interactions symbolically.\\\\\\\\\\\\\\\\n• Chapter 3 – Approach:Describes the conceptual framework of the proposed method,\\\\\\\\\\\\\\\\nincluding the symbolic vocabulary, abstract corpus construction, and the output model\\\\\\\\\\\\\\\\nbased on a conditioned transformer. Assumptions and design decisions are also discussed.\\\\\\\\\\\\\\\\n• Chapter 4 – Implementation:Presents the detailed structure of theMarTest-Pipeline,\\\\\\\\\\\\\\\\nexplaining each module in the pipeline, from the processing of logs and Q-values to the\\\\\\\\\\\\\\\\ngeneration of abstract episodes, classification models, and the conditioned transformer used\\\\\\\\\\\\\\\\nto produce test case regions.\\\\\\\\\\\\\\\\n• Chapter 5 – Experimental Design: Details the experimental setup, including the\\\\\\\\\\\\\\\\nselected agents from theCartPole and LunarLander environments. It also describes how\\\\\\\\\\\\\\\\nconcrete test cases are generated from the abstract representations and evaluates the agents\\\\\\\\\\\\\\\\nusing statistical analysis. The chapter concludes with a discussion on threats to validity.\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'• Chapter 6 – Limitations and Future Work:Discusses the current limitations of the\\\\\\\\\\\\\\\\napproach and outlines possible future directions, including the refinement of abstraction\\\\\\\\\\\\\\\\nlevels, integrationoflearning-basedrepresentations, anddeeperunificationofreinforcement\\\\\\\\\\\\\\\\nlearning and natural language processing techniques.\\\\\\\\\\\\\\\\n3\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'1. Introduction\\\\\\\\\\\\\\\\n4\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5\\\\\\\\\\\\\\\\nChapter 2\\\\\\\\\\\\\\\\nBackground\\\\\\\\\\\\\\\\nTo motivate the problem and to provide an overview of our approach we give the definitions of\\\\\\\\\\\\\\\\nthe concepts required to understand this work, regarding RL, NLP and Abstract classes.\\\\\\\\\\\\\\\\n2.1. Reinforment Learning, Makov process desicion (MDP)\\\\\\\\\\\\\\\\nReinforcement Learning trains an agent that interacts with an environmento maximize the ob-\\\\\\\\\\\\\\\\nserved reward obtained from the environment after action execution, RL uses a trial and error\\\\\\\\\\\\\\\\nstrategy to explore the environment. At every state, the agent chooses an action to execute from\\\\\\\\\\\\\\\\nthe set of all possible actions for the state, and receive a corresponding reward for the executed\\\\\\\\\\\\\\\\naction.\\\\\\\\\\\\\\\\nA Markov Decision Process (MDP) is defined as a 4-tuple< S,A,T,R > where S is the set\\\\\\\\\\\\\\\\nof continuous non-enumerable states (|S|≥| N|) ,A is the set of (continuous) actions,T is the\\\\\\\\\\\\\\\\ntransition function whereT : SXAXS →[0,1] such thatT(s,a,s ′) determines the probability\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'of reaching a states′ by performing an actiona in state s, R : SXA →R a reward function\\\\\\\\\\\\\\\\nthat determines a reward for a pair of an action and a state. At each time step, the agent uses\\\\\\\\\\\\\\\\na mapping from state to probabilities of selecting each possible action. This mapping is called\\\\\\\\\\\\\\\\nthe agent’s policyπ: SXA →[0,1] considered the solution of the MDP.\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'2. Background\\\\\\\\\\\\\\\\nMarkov Property\\\\\\\\\\\\\\\\nMarkov Propertyis defined as the environment’s response att+ 1 depends only on the state and\\\\\\\\\\\\\\\\naction representations att:\\\\\\\\\\\\\\\\nPr{Rt+1 = r,St+1 = s′|S0,A0,R1,....Rt,St,At}= Pr{Rt+1 = r,St+1|St,At}\\\\\\\\\\\\\\\\nfor allr,s′,St,At. [13].\\\\\\\\\\\\\\\\n2.2. Natural Language Processing\\\\\\\\\\\\\\\\nNatural Language Processing (NLP)[6] is a subfield of artificial intelligence (AI) and linguistics\\\\\\\\\\\\\\\\nfocused on enabling computers to understand, interpret, and generate human language in a\\\\\\\\\\\\\\\\nway that is both meaningful and useful. The key goals of NLP are understanding language,\\\\\\\\\\\\\\\\ngenerating language and interacting with humans, the applications of NLP techniques are on\\\\\\\\\\\\\\\\ntext classification, machine translation, speech recognition, text generation, among others. NLP\\\\\\\\\\\\\\\\ntechniques has been used in other fields as DNA (or RNA) sequences, the field is often referred to\\\\\\\\\\\\\\\\nas Computational Biology or Bioinformatics, specically in the subfield of Genomic Data Analysis\\\\\\'\"\\'',\n",
       "   '\\'\\\\\\'\\\\\\\\\\\\\\'where DNA and RNA sequences can be thought of as \"biological texts\"[9].\\\\\\\\\\\\\\\\nThere are a considerably amount of models in NLP, we will mention those are in the beginning\\\\\\\\\\\\\\\\na good start for the purpose of our approach.\\\\\\\\\\\\\\\\n2.2.1. Early Neural Network-Based Models\\\\\\\\\\\\\\\\nEarly Neural Network-Based Models refer to the first wave of neural network architectures and\\\\\\\\\\\\\\\\nmethods applied NLP tasks before the dominance of Transformer-based models. These models\\\\\\\\\\\\\\\\nprimarily relied on feedforward neural networks, recurrent architectures, and word embeddings,\\\\\\\\\\\\\\\\noffering improvements over traditional statistical and rule-based methods.\\\\\\\\\\\\\\\\nRNNs\\\\\\\\\\\\\\\\nRecurrent Neural Networks (RNNs) belong to a family of neural networks designed for processing\\\\\\\\\\\\\\\\nsequential data. They can process inputs of variable size, such as images, and can scale to much\\\\\\\\\\\\\\\\nlonger sequences than networks without sequence-based specialization [5].\\\\\\\\\\\\\\\\n6\\\\\\\\\\\\\\'\\\\\\'\\'',\n",
       "   '\\'\\\\\\'\\\\\\\\\\\\\\'2.2. Natural Language Processing\\\\\\\\\\\\\\\\nRNNs process data sequentially, meaning the output at a given step depends not only on the\\\\\\\\\\\\\\\\ncurrent input but also on information from previous steps, they have a hidden state that serves\\\\\\\\\\\\\\\\nas a \"memory,\" capturing information from previous time steps.\\\\\\\\\\\\\\\\nThe architecture for RNNs given a input sequenceX = [x1,x2,...,x T] where T is the sequence\\\\\\\\\\\\\\\\nlength, we have:\\\\\\\\\\\\\\\\nht = f(Wtht−1 + Wxxt + b)\\\\\\\\\\\\\\\\nyt = g(Wyht + c)\\\\\\\\\\\\\\\\nWhere Wt,Wx,Wy,b,c are parameters of the model andg is the output activation function\\\\\\\\\\\\\\\\n(e.g, softmax for classification)\\\\\\\\\\\\\\\\nLSTMs\\\\\\\\\\\\\\\\nLong Short-Term Memory (LSTMs) are a type of Recurrent Neural Network (RNN) known\\\\\\\\\\\\\\\\nas gated RNNs. They are based on the idea of introducing self-loops to create paths through\\\\\\\\\\\\\\\\nwhich the gradient can flow for long durations. LSTMs have been found to be extremely suc-\\\\\\\\\\\\\\\\ncessful in many applications, such as unconstrained handwriting recognition, speech recognition,\\\\\\\\\\\\\\'\\\\\\'\\'',\n",
       "   '\\'\\\\\\'\\\\\\\\\\\\\\'handwriting generation, machine translation, image captioning, and parsing [5].\\\\\\\\\\\\\\\\nTransformer\\\\\\\\\\\\\\\\nThe Transformer architecture (Vaswani et al., 2017), introduced in the seminal paper \"Attention\\\\\\\\\\\\\\\\nIs All You Need\" (2017), revolutionized the field of NLP and later impacted other domains like\\\\\\\\\\\\\\\\ncomputer vision. It replaced traditional sequential processing models, such as RNNs and LSTMs,\\\\\\\\\\\\\\\\nby introducing self-attention mechanisms, which enable efficient parallelization and long-range\\\\\\\\\\\\\\\\ndependency modeling see Figure 2.1.\\\\\\\\\\\\\\\\n7\\\\\\\\\\\\\\'\\\\\\'\\'',\n",
       "   '\\'\\\\\\'\\\\\\\\\\\\\\'2. Background\\\\\\\\\\\\\\\\nFigure 2.1: The transformer architecture, Attention Is All You Need, 2017.\\\\\\\\\\\\\\\\nTransformer decoder only\\\\\\\\\\\\\\\\nTheTransformerDecoder-OnlyArchitecture, popularizedbyGPT(GenerativePre-trainedTrans-\\\\\\\\\\\\\\\\nformer), is a variant of the original Transformer architecture introduced by Vaswani et al. in\\\\\\\\\\\\\\\\n\"Attention is All You Need\" (2017). GPT uses only the decoder part of the Transformer for its\\\\\\\\\\\\\\\\ndesign, focusing on autoregressive text generation tasks[10]. For the achitecture see Figure 2.2\\\\\\\\\\\\\\\\n8\\\\\\\\\\\\\\'\\\\\\'\\'',\n",
       "   '\\'\"\\\\\\'2.2. Natural Language Processing\\\\\\\\\\\\\\\\nFigure 2.2: Transformer achitecture, Improving Language Understanding by Generative Pre-\\\\\\\\\\\\\\\\nTraining.\\\\\\\\\\\\\\\\nModel-Conditioned Transformer\\\\\\\\\\\\\\\\nIn this work, we employ the Model-Conditioned Transformer, a GPT-based architecture that\\\\\\\\\\\\\\\\nprocesses no only conventional word token but also additional conditioning variables. Analogous\\\\\\\\\\\\\\\\nto music-generation models that incorporate parameters such as tempo and velocity, our model\\\\\\\\\\\\\\\\nreceives two supplementary numerical inputs:\\\\\\\\\\\\\\\\n1. Performance reward.\\\\\\\\\\\\\\\\n2. Failure probability.\\\\\\\\\\\\\\\\nBoth values are generated during agent execution: the performance reward quantifies the\\\\\\\\\\\\\\\\nagent’s effectiveness, while the failure probability estimates the likelihood of erroneous actions.\\\\\\\\\\\\\\\\nSpecifically, both the performance reward and the failure probability are first processed by an\\\\\\\\\\\\\\\\nauxiliary neural network to produce conditioning vectors. These vectors are then added element-\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'wise to the word-token embeddings, and the resulting combined representations are passed into\\\\\\\\\\\\\\\\nthe transformer decoder, see Figure 2.3 .\\\\\\\\\\\\\\\\n9\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'2. Background\\\\\\\\\\\\\\\\nFigure 2.3: Conditioned Transformer, rewardˆr and failure probabilityPf are first processed by\\\\\\\\\\\\\\\\nan auxiliary neural network, then added element-wise to the word-token embeddings.\\\\\\\\\\\\\\\\n2.3. Abstract Classes\\\\\\\\\\\\\\\\nThe definition of Abstracts Classes relies in the definition of equivalences class, a partition of a\\\\\\\\\\\\\\\\nset. A State Abstraction is defined as a mapping from an original states ∈S to an abstract\\\\\\\\\\\\\\\\nstate sϕ ∈Sϕ\\\\\\\\\\\\\\\\nϕ: S →Sϕ\\\\\\\\\\\\\\\\nwhere Sϕ ∈P(S) and fulfills equivalence conditions (reflexive, symmetric, and transitive) [1].\\\\\\\\\\\\\\\\n2.3.1. Definition π∗-irrelevance abstraction\\\\\\\\\\\\\\\\ns1 and s2 are in the same abstract classϕ(s1) = ϕ(s2) if π∗(s1) = π(s2) where π∗is the optimal\\\\\\\\\\\\\\\\npolicy.\\\\\\\\\\\\\\\\n2.3.2. Definition Q∗-irrelevance abstraction\\\\\\\\\\\\\\\\ns1 ands2 are in the same abstract classϕ(s1) = ϕ(s2) if for all actiona∈A, Q∗(s1,a) = Q∗(s2,a)\\\\\\\\\\\\\\\\nwhere Q∗(s,a) is the optimalQ-value function the maximum expected reward.\\\\\\\\\\\\\\\\n2.3.3. Relax Q∗-irrelevance abstraction\\\\\\\\\\\\\\\\ns1 and s2 are in the same abstract classϕ(s1) = ϕ(s2) if:\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'∀a∈A: ⌈Q∗(s1,a)/d⌉= ⌈Q∗(s2,a)/d⌉\\\\\\\\\\\\\\\\nwhere d is a control parameter (abstraction level) [1].\\\\\\\\\\\\\\\\n10\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'11\\\\\\\\\\\\\\\\nChapter 3\\\\\\\\\\\\\\\\nMarTest: An NLP Approach for test sequence generation\\\\\\\\\\\\\\\\nWe aim to identify the principal potential failures or “vulnerabilities” that an agent may en-\\\\\\\\\\\\\\\\ncounter during execution of its policy within an environment. To this end, we leverage all\\\\\\\\\\\\\\\\navailable execution data—hereinafter referred to as logs (in the context of data testing). These\\\\\\\\\\\\\\\\nlogs consist of records of episodes, i.e. sequences of states, actions and rewards, from which we\\\\\\\\\\\\\\\\nseek to predict possible failures.\\\\\\\\\\\\\\\\nAs noted above, if the state spaceS is uncountable then the set of all possible episodes is\\\\\\\\\\\\\\\\nlikewise uncountable|Sn|>|N|for n= 1,2,3,4....\\\\\\\\\\\\\\\\nHence, the task of enumerating all potential failures is unfeasible. As a first step toward\\\\\\\\\\\\\\\\ntractability, we introduce a reduction via abstract classes, mappingSinto a countable abstraction\\\\\\\\\\\\\\\\nSϕ:\\\\\\\\\\\\\\\\nϕ: S →Sϕ and |Sϕ|<|N|\\\\\\\\\\\\\\\\nOncethisreductionisimplemented, theproblemcanbeusingtheresultingcountablespaceand\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'addressed from a language-processing perspective by regarding each abstract class as a distinct\\\\\\\\\\\\\\\\nlexical token.\\\\\\\\\\\\\\\\n3.1. Vocabulary\\\\\\\\\\\\\\\\nWe define a vocabulary for getting a finite set of sequences:\\\\\\\\\\\\\\\\nlet V be a vocabulary, ifw∈V then w= ˆs or w= a for ˆs∈ˆS and ˆS is a set of partitions ofS\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'3. MarTest: An NLP Approach for test sequence generation\\\\\\\\\\\\\\\\nNote that defining the vocabulary is the first step in framing our problem as an NLP problem.\\\\\\\\\\\\\\\\nThen, we define the collection of all texts, known as the corpus.\\\\\\\\\\\\\\\\n3.2. Corpus\\\\\\\\\\\\\\\\nLet τ be a concrete sequence as s0a0s1a1...snan of state-action tuples and ϕ an abstraction\\\\\\\\\\\\\\\\nfunction, we sayˆτ is an abstraction sequence or abstract path for a sequenceτ under function\\\\\\\\\\\\\\\\nϕ(τ) = ˆτ if:\\\\\\\\\\\\\\\\nfor ˆτ = ˆS0a0 ˆS1a1... ˆSnan we have thatϕ(si) = ˆSi for i= 1,2,3...n\\\\\\\\\\\\\\\\nLet ˆDϕ be a corpus or abstract dataset such thatˆDϕ is composed of abstract pathsˆτ.\\\\\\\\\\\\\\\\nIf we were to take the setˆDϕ as the training set for an NLP model, the model would most\\\\\\\\\\\\\\\\nlikely imitate the agent’s behavior in an abstract way, since we would be training it on behavior\\\\\\\\\\\\\\\\npatterns associated with each abstract class. However, our goal is not to replicate the agent’s\\\\\\\\\\\\\\\\nbehavior, but rather to identify low-reward regions and potential failures. For this purpose, we\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'extend the definition of our dataset.\\\\\\\\\\\\\\\\n3.3. Dataset\\\\\\\\\\\\\\\\nLet D be the training dataset for the conditioned Transformer model, such that(ˆτ,¯r,Pf ) ∈D\\\\\\\\\\\\\\\\nif ˆτ ∈ ˆDϕ, ¯r is the average reward ofˆτ, andPf is the failure probability ofˆτ.\\\\\\\\\\\\\\\\n3.3.1. Average reward\\\\\\\\\\\\\\\\nSince ˆτ may represent multiple concrete sequences—that is, it may happen thatϕ(τ1) = ϕ(τ2) =\\\\\\\\\\\\\\\\n··· = ϕ(τn) = ˆτ for sequences τ1,τ2,...,τ n of state-action tuples—we take the total rewards\\\\\\\\\\\\\\\\nassociated with eachτi and compute their average to obtain the mean reward¯r.\\\\\\\\\\\\\\\\n3.3.2. Failure probability\\\\\\\\\\\\\\\\nWe can associate eachˆτ ∈ ˆDϕ with a failure probability. Given a sequence of concrete trajectories\\\\\\\\\\\\\\\\nτ1,τ2,...,τ n executed by an agent, we know that each trajectory either resulted in failure or not.\\\\\\\\\\\\\\\\n12\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'3.3. Dataset\\\\\\\\\\\\\\\\nThat is, we can construct a list of pairs(τ1,j1),(τ2,j2),..., (τn,jn), wherejk = 1 if the execution\\\\\\\\\\\\\\\\nof τk was successful, andjk = 0 otherwise.\\\\\\\\\\\\\\\\nTherefore, wecanderiveacorrespondinglistofpairs (ˆτ1,j1),(ˆτ2,j2),..., (ˆτn,jn), whereϕ(τk) =\\\\\\\\\\\\\\\\nˆτk for k= 1,2,...,n . In this way, we obtain a dataset of abstract trajectories labeled as success\\\\\\\\\\\\\\\\nor failure. With the appropriate model, we can then predict the failure probability for eachˆτ.\\\\\\\\\\\\\\\\n3.3.3. Random Forest\\\\\\\\\\\\\\\\nTo predict the failure probability of an abstract trajectoryˆτ, we follow the approach proposed by\\\\\\\\\\\\\\\\nZolfagharian [1, Section IV, Subsection 7], which consists of representingˆτ as a binary sequence\\\\\\\\\\\\\\\\ndetermined by the presence of abstract states withinˆτ. This representation, together with the\\\\\\\\\\\\\\\\ncorresponding failure label, serves as input for aRandom Forestmodel.\\\\\\\\\\\\\\\\nAssume that ˆS0, ˆS1,..., ˆSm are all the abstract classes produced by a mappingϕ. Then, for a\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'given abstract trajectoryˆτ, we can construct the following binary representation:\\\\\\\\\\\\\\\\nAbstract Sequence ˆS0 ˆS1 ˆS2 ... ˆSm\\\\\\\\\\\\\\\\nˆτ 0 1 1 ... 0\\\\\\\\\\\\\\\\nTable 3.1: Binary representation of an abstract trajectory\\\\\\\\\\\\\\\\nTherefore, by constructing a binary representation for each sequence followed by the agent, and\\\\\\\\\\\\\\\\nlabeling the corresponding performance outcome as either failure (1) or success (0), we obtain a\\\\\\\\\\\\\\\\ndataset suitable for training a classification algorithm:\\\\\\\\\\\\\\\\nIndex ˆS0 ˆS1 ˆS2 ... ˆSm Failure\\\\\\\\\\\\\\\\ni 0 1 1 ... 0 1\\\\\\\\\\\\\\\\ni+ 1 1 0 0 ... 0 0\\\\\\\\\\\\\\\\nTable 3.2: Training data representation for a classification model\\\\\\\\\\\\\\\\n13\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'3. MarTest: An NLP Approach for test sequence generation\\\\\\\\\\\\\\\\n3.4. Model-Conditioned Transformer\\\\\\\\\\\\\\\\nIn this approach, any natural language processing (NLP) technique can be employed to generate\\\\\\\\\\\\\\\\nabstract sequences associated with a certain probability of failure. However, as previously stated,\\\\\\\\\\\\\\\\nthis work proposes the use of a conditioned transformer to identify risk regions.\\\\\\\\\\\\\\\\nGiven the dataset D, composed of 3-tuples (ˆτ,¯r,Pf ), we have a training set for a Model-\\\\\\\\\\\\\\\\nConditioned Transformer, as described in Section 2.2.1. Once the conditioned transformer is\\\\\\\\\\\\\\\\ntrained, it can be used to generate abstract sequences given a rewardr and a failure probability\\\\\\\\\\\\\\\\nPf.\\\\\\\\\\\\\\\\nThese sequences, composed of abstract statesˆS interpreted by the transformer as tokensw,\\\\\\\\\\\\\\\\ncan also be seen as regions associated with a certain likelihood of failure and low reward. The\\\\\\\\\\\\\\\\nselection and handling of these regions may vary depending on the objective. In Section 5.4, we\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'introduce a method for generating test cases based on these sequences as part of our experimental\\\\\\\\\\\\\\\\nsetup.\\\\\\\\\\\\\\\\n3.4.1. Assumptions\\\\\\\\\\\\\\\\nIn this work, we focus on RL agents with discrete actions we could extend the theory for infinity\\\\\\\\\\\\\\\\nof set of actions, but for simplicity we will do in a future work.\\\\\\\\\\\\\\\\n14\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'15\\\\\\\\\\\\\\\\nChapter 4\\\\\\\\\\\\\\\\nMarTest Pipeline\\\\\\\\\\\\\\\\nIn this chapter, we present the implementation of our approach, called MarTest-Pipeline, a\\\\\\\\\\\\\\\\nsoftware tool designed to produce risk regions for testing reinforcement learning agents. The\\\\\\\\\\\\\\\\nimplementation is available athttps://github.com/lamedinaauniandes/Martest-pipeline.\\\\\\\\\\\\\\\\nWe describe the purpose and functionality of each module at a high level, emphasizing that all\\\\\\\\\\\\\\\\ncomponents are configurable and support module-specific arguments. This overview is intended\\\\\\\\\\\\\\\\ntohelpthereadernavigatetheimplementationwithgreaterclarity. Fordetailsregardingsoftware\\\\\\\\\\\\\\\\nrequirements and environment setup, please refer to Appendix A.1.\\\\\\\\\\\\\\\\n4.1. Pipeline Implementation\\\\\\\\\\\\\\\\nOur implementation comprises five principal modules, each corresponding to a distinct stage in\\\\\\\\\\\\\\\\nthe log-processing pipeline for any agent (shown in Figure 4.1).\\\\\\\\\\\\\\\\nEach module performs its designated processing tasks and produces the artifacts required by\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'the subsequent module. This modular design offers us considerable flexibility for future work:\\\\\\\\\\\\\\\\nfor instance, researchers wishing to focus on the abstract-class graph can directly leverage the\\\\\\\\\\\\\\\\noutputs generated by the second module,2. Abstract Classes, as the basis for their analyses.\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'4. MarTest Pipeline\\\\\\\\\\\\\\\\nFigure 4.1: Martest pipeline .\\\\\\\\\\\\\\\\n4.2. Module 1: Data acquisition and preparation\\\\\\\\\\\\\\\\n4.2.1. Inputs training and execution logs\\\\\\\\\\\\\\\\nThe pipeline begins with two types of logs: training logs, which record the agent’s behavior\\\\\\\\\\\\\\\\nduring the training phase, and execution logs, which capture its behavior after training (i.e.,\\\\\\\\\\\\\\\\nduring real-world deployment). Although it is not strictly necessary to provide both types of\\\\\\\\\\\\\\\\nlogs, combining them can help balance the dataset. In particular, if the execution logs contain an\\\\\\\\\\\\\\\\ninsufficient number of failure episodes —as may happen in some cases where the agent performs\\\\\\\\\\\\\\\\nperfectly with no observed failures— an effective strategy is to augment them with records from\\\\\\\\\\\\\\\\nthe training phase.\\\\\\\\\\\\\\\\n4.2.2. Data Balacing\\\\\\\\\\\\\\\\nIn this stage of the pipeline, we specify a mixture ratiop∈[0,1] and sample from the training\\\\\\\\\\\\\\\\nlogs accordingly. Since early training episodes may not accurately represent the agent’s behavior,\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'we drawm episodes from training data wherem = p·Ntrain and are sample according to the\\\\\\\\\\\\\\\\nprobability distribution over episodes introduced by Zolfagharian [1, Sección IV,Ec. (6)]:\\\\\\\\\\\\\\\\nP(ei) = i\\\\\\\\\\\\\\\\nNtrain∑\\\\\\\\\\\\\\\\nj=1\\\\\\\\\\\\\\\\nj\\\\\\\\\\\\\\\\n, i = 1,2,...,m,\\\\\\\\\\\\\\\\nwhere Ntrain denotes the total number of training episodes. This weighting favors latter (more\\\\\\\\\\\\\\\\nrepresentative) episodes while still incorporating early-stage data with more state exploration.\\\\\\\\\\\\\\\\nThe output of this stage is an execution log combining episodes of both training and execution.\\\\\\\\\\\\\\\\n16\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'4.3. Module 2: Abstract Classes\\\\\\\\\\\\\\\\n4.2.3. Q-Values\\\\\\\\\\\\\\\\nDuring this step in the process, we calculate the total reward obtain for each state and action\\\\\\\\\\\\\\\\nfor all episodes in the logs, based in the definition of theaction-value for a states, actiona, and\\\\\\\\\\\\\\\\npolicy π [13, Sección 3.7, Ec. (3.11)] :\\\\\\\\\\\\\\\\nqπ(s,a) = Eπ\\\\\\\\\\\\\\\\n[\\\\\\\\\\\\\\\\nGt |St = s, At = a\\\\\\\\\\\\\\\\n]\\\\\\\\\\\\\\\\n= Eπ\\\\\\\\\\\\\\\\n[∞∑\\\\\\\\\\\\\\\\nk=0\\\\\\\\\\\\\\\\nγkRt+k+1\\\\\\\\\\\\\\\\n⏐⏐⏐St = s, At = a\\\\\\\\\\\\\\\\n]\\\\\\\\\\\\\\\\nThis equation calculates the value expected to take an actiona in state s, accounting for\\\\\\\\\\\\\\\\nfuture rewards, in our case we do not use expected values because the policy has already been\\\\\\\\\\\\\\\\nimplemented. Furthermore, as we have already obtained the rewards, we do not discount the\\\\\\\\\\\\\\\\nrewards. As a consequence we calculate the q-values as:γ = 1,\\\\\\\\\\\\\\\\nqπ(s,a) =\\\\\\\\\\\\\\\\n∞∑\\\\\\\\\\\\\\\\nk=0\\\\\\\\\\\\\\\\nRt+k+1\\\\\\\\\\\\\\\\nThe end result of this is the q-table with the q-value for each state-action pair.\\\\\\\\\\\\\\\\n4.3. Module 2: Abstract Classes\\\\\\\\\\\\\\\\n4.3.1. Construction\\\\\\\\\\\\\\\\nOnce we have the Q-table, we proceed to build the abstract classes using the definition of Relaxed\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'Q∗-irrelevance abstraction (see Section 2.3.3). This module generates a JSON file containing a\\\\\\\\\\\\\\\\ndictionary, where the keys are the representatives of the abstract classes and the values are lists\\\\\\\\\\\\\\\\nof the states grouped within each class.\\\\\\\\\\\\\\\\nSince the definition ofQ∗-irrelevance abstraction depends on the abstraction level—a config-\\\\\\\\\\\\\\\\nurable parameter—the user can set this value according to the desired granularity.\\\\\\\\\\\\\\\\nNote that at this stage, we are constructing the vocabulary referenced in Section 3.1.\\\\\\\\\\\\\\\\n4.4. Module 3\\\\\\\\\\\\\\\\n4.4.1. Abstract Episodes generation\\\\\\\\\\\\\\\\nIn this module, we proceed to build the corpusˆDϕ, as defined in Section 3.2, and compute the\\\\\\\\\\\\\\\\naverage reward of each abstract episode. The output consists of CSV files containing abstract\\\\\\\\\\\\\\\\n17\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'4. MarTest Pipeline\\\\\\\\\\\\\\\\nsequences along with their corresponding average rewards.\\\\\\\\\\\\\\\\n4.5. Module 4\\\\\\\\\\\\\\\\n4.5.1. Random Forest Model\\\\\\\\\\\\\\\\nIn this module, we compute the failure probability for each abstract sequence obtained in the\\\\\\\\\\\\\\\\nprevious module. To achieve this, we generate CSV files containing the binary representations of\\\\\\\\\\\\\\\\neach abstract sequence along with their corresponding failure labels, as described in Section 3.3.3,\\\\\\\\\\\\\\\\nin order to estimate the failure probability of each abstract sequence.\\\\\\\\\\\\\\\\nFinally, we generate CSV files containing the datasetD, as described in Section 3.3, which\\\\\\\\\\\\\\\\nserves as input for the Transformer model.\\\\\\\\\\\\\\\\nInthismoduleweusethelibrary sklearnbyscikit-learn, asimpleandefficienttoolforpredictive\\\\\\\\\\\\\\\\nanalysis, we use the classsklearn.ensemble.RandomForestClassifierprecisely and show us a great\\\\\\\\\\\\\\\\nprecision and performance.\\\\\\\\\\\\\\\\n4.6. Module 5\\\\\\\\\\\\\\\\n4.6.1. Conditioned Transformer\\\\\\\\\\\\\\\\nIn this module, we train the Conditioned Transformer using the datasetD produced in the\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'previous module. Once the model is trained, we can also generate abstract sequences given a\\\\\\\\\\\\\\\\nreward valuer and a failure probabilityPf, thereby identifying regions with potential risk of\\\\\\\\\\\\\\\\nfailure or low reward, as described in Section 3.4.\\\\\\\\\\\\\\\\nWe use thetransformers library fromHugging Face, which integrates with PyTorch. Specifi-\\\\\\\\\\\\\\\\ncally, we base our implementation on thetransformers.GPT2LMHeadModel class, which follows\\\\\\\\\\\\\\\\nthe GPT-2 architecture—a decoder-only Transformer commonly used for text generation. We\\\\\\\\\\\\\\\\nmodified this architecture to also receive the average reward and failure probability as inputs in\\\\\\\\\\\\\\\\norder to predict tokens (i.e., abstract states or regions) associated with specific levels of risk or\\\\\\\\\\\\\\\\nfailure likelihood. We refer to this modified model as theConditioned Transformer.\\\\\\\\\\\\\\\\nThe Conditioned Transformer is implemented by extending theGPT2LMHeadModel class to\\\\\\\\\\\\\\\\n18\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'4.6. Module 5\\\\\\\\\\\\\\\\na new class namedCondGPT2. The core idea is to preprocess the average reward and failure\\\\\\\\\\\\\\\\nprobability through a small feedforward network, producing a vector representation that is then\\\\\\\\\\\\\\\\nadded to the token embeddings of the abstract sequence before being passed to the GPT-2 model.\\\\\\\\\\\\\\\\nFor further details, see Section A.2.\\\\\\\\\\\\\\\\n4.6.2. Test Regions Cases\\\\\\\\\\\\\\\\nAt the end of the pipeline, we obtain an output in the form of a list ofpossible abstract\\\\\\\\\\\\\\\\nepisodes—that is, sequences of abstract states and actions (e.g.,’w2 1 w1 2 w3 1 w6 ... w5\\\\\\\\\\\\\\\\n1 True’) interpreted by the transformer astokens or words. However, since these tokens\\\\\\\\\\\\\\\\nwi represent abstract classes, they can also be understood assequences of regions and\\\\\\\\\\\\\\\\nactions.\\\\\\\\\\\\\\\\nGiven that the transformer has been designed to receive the average reward and fail-\\\\\\\\\\\\\\\\nure probabilityas input parameters, we interpret these region–action sequences aspossible\\\\\\\\\\\\\\\\nepisodes conditioned on those parameters.\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'Based on this representation, multiple strategies can be devised to generateconcrete and\\\\\\\\\\\\\\\\nactionable test cases. In the following chapter, we present the specific testing approaches\\\\\\\\\\\\\\\\napplied to the selected agents.\\\\\\\\\\\\\\\\n19\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'4. MarTest Pipeline\\\\\\\\\\\\\\\\n20\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'21\\\\\\\\\\\\\\\\nChapter 5\\\\\\\\\\\\\\\\nExperimental Design\\\\\\\\\\\\\\\\nWe evaluate our implementation on six agents selected from the publicly editable OpenAI Gym\\\\\\\\\\\\\\\\nleaderboard wiki page1. Three of these agents implement in theCartPole-v0 environment, and\\\\\\\\\\\\\\\\nthe remaining three are implement theLunarLander-v2.\\\\\\\\\\\\\\\\nFor all agents, we apply the definition ofRelaxedQ∗-irrelevance abstraction( Section 2.3.3), as\\\\\\\\\\\\\\\\nwe consider this abstraction suitable for capturing the agent’s general perception formed through\\\\\\\\\\\\\\\\nits interaction with the environment and its learning dynamics.\\\\\\\\\\\\\\\\nThe constructed abstraction assigns avalue to each experience and allows for controlling the\\\\\\\\\\\\\\\\ngranularity of the abstract classes through the abstraction-level parameter. In our experiments,\\\\\\\\\\\\\\\\nthis parameter was set tod= 4, chosen as a midpoint based on the experimental settings reported\\\\\\\\\\\\\\\\nby Zolfagharian [1, Section V, Table I].\\\\\\\\\\\\\\\\nFor each agent, we introduced a lightweight instrumentation module into the codebase to\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'record logs during both training and execution. From this point onward, we refer to this logging\\\\\\\\\\\\\\\\nprocess asinstrumentation.\\\\\\\\\\\\\\\\nThe server used to run the environments in the experiment has the following specifications:\\\\\\\\\\\\\\\\n• CPU: 3×Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz (model 79)\\\\\\\\\\\\\\\\n• RAM: 7.8GiB (MemTotal)\\\\\\\\\\\\\\\\n• Primary storage:1× 60GB (device sda)\\\\\\\\\\\\\\\\n1https://github.com/openai/gym/wiki/Leaderboard\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\n• Operating system:Ubuntu 22.04.5 LTS (Jammy)\\\\\\\\\\\\\\\\nAll experimental data produced by the MarTest-Pipeline software, as a result of each agent’s\\\\\\\\\\\\\\\\nexecution, is available athttps://zenodo.org/records/15485620. The specifications of the\\\\\\\\\\\\\\\\nmachines used to run the MarTest-Pipeline software were as follows:\\\\\\\\\\\\\\\\n• CPU:48 logical cores (2×Intel® Xeon® Silver 4310 CPU @ 2.10GHz; 12 cores per socket,\\\\\\\\\\\\\\\\n2 threads per core)\\\\\\\\\\\\\\\\n• RAM: 251.3GiB (MemTotal)\\\\\\\\\\\\\\\\n• Primary storage:1 ×7 TB (devicesda)\\\\\\\\\\\\\\\\n• OS: Ubuntu 22.04.4 LTS (Jammy)\\\\\\\\\\\\\\\\n• GPU: 4×NVIDIA A40 (46068MiB VRAM each); Driver Version: 560.35.03, CUDA\\\\\\\\\\\\\\\\nVersion: 12.6\\\\\\\\\\\\\\\\n5.1. CartPole-v0 Agents\\\\\\\\\\\\\\\\nWe begin by presenting a description of theCartPole-v0 environment provided by OpenAI Gym:\\\\\\\\\\\\\\\\n“A pole is attached by an un-actuated joint to a cart, which moves along a frictionless\\\\\\\\\\\\\\\\ntrack. The pendulum is placed upright on the cart and the goal is to balance the pole\\\\\\\\\\\\\\\\nby applying forces in the left and right direction on the cart.”2\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'Therefore, the only actions available to the agent are pushing the cart to the left or to the\\\\\\\\\\\\\\\\nright, represented by the action setA= {0,1}. The agent observes the state of the environment\\\\\\\\\\\\\\\\nas a list of four continuous values corresponding to:cart position, cart velocity, pole angle, and\\\\\\\\\\\\\\\\npole velocity at the tip(see Figure 5.1).\\\\\\\\\\\\\\\\n2https://gymnasium.farama.org/environments/classic_control/cart_pole/\\\\\\\\\\\\\\\\n22\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5.1. CartPole-v0 Agents\\\\\\\\\\\\\\\\nFigure 5.1: CartPole-v0.\\\\\\\\\\\\\\\\nFor this environment the selected agents are:\\\\\\\\\\\\\\\\nShakti Kumar (shakti)\\\\\\\\\\\\\\\\nRanked fourth on the OpenAI Gym leaderboard, this agent is reported to solve the environment\\\\\\\\\\\\\\\\nin 0 episodes. The code [8, shakti], dated from December 2019, requiring to set up a custom\\\\\\\\\\\\\\\\nenvironment based on Python 3.7.\\\\\\\\\\\\\\\\nThe necessary dependencies are listed in Appendix A.3. Due to the legacy state of the code,\\\\\\\\\\\\\\\\nwe installed PyTorch manually from pytorch.org using the following command for Ubuntu:\\\\\\\\\\\\\\\\npip install torch ==1.2.0+ cu92 torchvision ==0.4.0+ cu92 -f https ://\\\\\\\\\\\\\\\\ndownload . pytorch . org / whl / torch_stable . html\\\\\\\\\\\\\\\\nKapil Chauhan (kapil)\\\\\\\\\\\\\\\\nRanked ninth on the OpenAI Gym leaderboard, this agent is reported to solve the environment\\\\\\\\\\\\\\\\nin 4 episodes. The code [4, kapil], dated from December 2019, also requires setting up a custom\\\\\\\\\\\\\\\\nenvironment based on Python 3.7. The required dependencies are listed in Appendix A.4.\\\\\\\\\\\\\\\\nNihal T. Rao (nihal)\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'Ranked twenty-ninth on the OpenAI Gym leaderboard, this agent is reported to solve the en-\\\\\\\\\\\\\\\\nvironment in 184 episodes. The code [11, Nihal], dated from June 2020, also requires setting\\\\\\\\\\\\\\\\n23\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\nup a custom environment based on Python 3.7. The necessary dependencies are listed in Ap-\\\\\\\\\\\\\\\\npendix A.5.\\\\\\\\\\\\\\\\n5.2. LunarLander-v2 Agents\\\\\\\\\\\\\\\\nThe description of theLunarLander-v2 environment, as provided by OpenAI Gym is:\\\\\\\\\\\\\\\\n“This environment is a classic rocket trajectory optimization problem. According to\\\\\\\\\\\\\\\\nPontryagin’s maximum principle, it is optimal to fire the engine at full throttle or\\\\\\\\\\\\\\\\nturn it off. This is the reason why this environment has discrete actions: engine on\\\\\\\\\\\\\\\\nor off... The landing pad is always at coordinates (0,0). The coordinates are the first\\\\\\\\\\\\\\\\ntwo numbers in the state vector. Landing outside of the landing pad is possible. Fuel\\\\\\\\\\\\\\\\nis infinite, so an agent can learn to fly and then land on its first attempt.”3\\\\\\\\\\\\\\\\nThere are four discrete actions available:0 – do nothing,1 – fire left orientation engine,2 –\\\\\\\\\\\\\\\\nfire main engine,3 – fire right orientation engine.\\\\\\\\\\\\\\\\nThe state is represented as an 8-dimensional vector, which includes: the lander’s position\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'coordinates (x,y), its linear velocities(vx,vy), its angle and angular velocity, and two boolean\\\\\\\\\\\\\\\\nvalues indicating whether the left and right legs are in contact with the ground.4 (See Figure 5.2.)\\\\\\\\\\\\\\\\nFor this environment, the selected agents are:\\\\\\\\\\\\\\\\n5.2.1. Sanket Thakur (sanket)\\\\\\\\\\\\\\\\nRanked ninth on the OpenAI Gym leaderboard, this agent is reported to solve the environment\\\\\\\\\\\\\\\\nin 454 episodes. The code [14, sanket], dated from April 2020, requires setting up a custom\\\\\\\\\\\\\\\\nenvironment based on Python 3.7. The required dependencies are listed in Appendix A.7.\\\\\\\\\\\\\\\\n5.2.2. Mahmood Khordoo (khordoo)\\\\\\\\\\\\\\\\nRanked tenth on the OpenAI Gym leaderboard, this agent is reported to solve the environment\\\\\\\\\\\\\\\\nin 602 episodes. The code [7, khordoo], dated from April 2020, also requires setting up a custom\\\\\\\\\\\\\\\\n3https://gymnasium.farama.org/environments/box2d/lunar_lander/#description\\\\\\\\\\\\\\\\n4https://gymnasium.farama.org/environments/box2d/lunar_lander/#description\\\\\\\\\\\\\\\\n24\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5.3. Conditioned transformer configurations and metrics\\\\\\\\\\\\\\\\nFigure 5.2: LunarLander-v2.\\\\\\\\\\\\\\\\nenvironment based on Python 3.7. The required dependencies are listed in Appendix A.6.\\\\\\\\\\\\\\\\n5.2.3. Sigve Rokenes (rokenes)\\\\\\\\\\\\\\\\nRankednineteenontheOpenAIGymleaderboard, thisagentisreportedtosolvetheenvironment\\\\\\\\\\\\\\\\nin 1,590 episodes. The code [12, rokenes], dated from January 2019, requires setting up a custom\\\\\\\\\\\\\\\\nenvironment based on Python 3.7. The necessary dependencies are listed in Appendix A.8.\\\\\\\\\\\\\\\\n5.3. Conditioned transformer configurations and metrics\\\\\\\\\\\\\\\\nNext, we present the configurations and training results of the conditioned transformer for each\\\\\\\\\\\\\\\\nexperiment. We reportvocab_size, the number of tokens in the model’s vocabulary;n_embd, the\\\\\\\\\\\\\\\\ndimensionality of the token embeddings and hidden states;n_layer, the number of transformer\\\\\\\\\\\\\\\\ndecoder blocks (also called layers); and n_head, the number of attention heads in each self-\\\\\\\\\\\\\\\\nattention layer.\\\\\\\\\\\\\\\\n25\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\n5.3.1. Conditional transformer parameters for cartpole\\\\\\\\\\\\\\\\nAgent vocab_size n_embd n_layer n_head\\\\\\\\\\\\\\\\nShakti 101 1024 12 16\\\\\\\\\\\\\\\\nKapil 101 1024 12 16\\\\\\\\\\\\\\\\nNihal 101 1024 12 16\\\\\\\\\\\\\\\\nTable 5.1: Parameters of conditional transformers for cartpole.\\\\\\\\\\\\\\\\n5.3.2. Conditional transformer parameters for Lunar-Lander\\\\\\\\\\\\\\\\nAgent vocab_size n_embd n_layer n_head\\\\\\\\\\\\\\\\nSanket 840 512 6 8\\\\\\\\\\\\\\\\nKhordoo 711 512 6 8\\\\\\\\\\\\\\\\nRokenes 868 512 6 8\\\\\\\\\\\\\\\\nTable 5.2: Parameters of conditional transformer for Lunar-lander.\\\\\\\\\\\\\\\\n5.3.3. Training and validations loss\\\\\\\\\\\\\\\\nNext, we present the loss metrics for each agent in the CartPole and LunarLander environments.\\\\\\\\\\\\\\\\nAs shown, the loss for all agents decreases over time and converges to zero. This suggests that\\\\\\\\\\\\\\\\nNLP techniques are sufficient to solve these tasks, which is likely due to the relatively small\\\\\\\\\\\\\\\\nvocabulary size compared to other NLP problems, such as machine translation or large language\\\\\\\\\\\\\\\\nmodels.\\\\\\\\\\\\\\\\n26\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5.3. Conditioned transformer configurations and metrics\\\\\\\\\\\\\\\\nTraining and Evaluation Loss per Epoch, Shakti\\\\\\\\\\\\\\\\nFigure 5.3: Training and Evaluation Loss per Epoch, Shakti\\\\\\\\\\\\\\\\nTraining and Evaluation Loss per Epoch, Kapil\\\\\\\\\\\\\\\\nFigure 5.4: Training and Evaluation Loss per Epoch, Kapil\\\\\\\\\\\\\\\\n27\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n5. Experimental Design\\\\\\\\\\\\\\\\nTraining and Evaluation Loss per Epoch, Nihal\\\\\\\\\\\\\\\\nFigure 5.5: Training and Evaluation Loss per Epoch, Nihal\\\\\\\\\\\\\\\\nTraining and Evaluation Loss per Epoch, Sanket\\\\\\\\\\\\\\\\nFigure 5.6: Training and Evaluation Loss per Epoch, Sanket\\\\\\\\\\\\\\\\n28\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n5.4. Concrete Test Case Generator\\\\\\\\\\\\\\\\nTraining and Evaluation Loss per Epoch, Khordoo\\\\\\\\\\\\\\\\nFigure 5.7: Training and Evaluation Loss per Epoch, Khordoo\\\\\\\\\\\\\\\\nTraining and Evaluation Loss per Epoch, Rokenes\\\\\\\\\\\\\\\\nFigure 5.8: Training and Evaluation Loss per Epoch, Rokenes\\\\\\\\\\\\\\\\n5.4. Concrete Test Case Generator\\\\\\\\\\\\\\\\nOnce the MarTest Pipeline Regionsmodule is deployed, we obtain a list of possible regions\\\\\\\\\\\\\\\\nassociated with failure risk or low rewards. The next step is to take advantage of this output in\\\\\\\\\\\\\\\\n29\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\norder to effectively test the agents.\\\\\\\\\\\\\\\\nTo define our strategy, we refer to the Markov property, which states that the environment’s\\\\\\\\\\\\\\\\nresponse at timet+ 1 depends only on the current state and action at timet (see Section 2.1).\\\\\\\\\\\\\\\\nFollowing this principle, we consider it valid to infer possible failures or low-reward outcomes\\\\\\\\\\\\\\\\nstarting from an initial state. Our model provides a list of such high-risk initial regions, indicating\\\\\\\\\\\\\\\\nsituations where failure may occur within a limited number of steps.\\\\\\\\\\\\\\\\nFrom each initial region within an abstract sequence produced by the conditioned Trans-\\\\\\\\\\\\\\\\nformer (e.g.,w50 in ˆτ = w50 1 w12 0 ...w 223), we sampled 100 states and applied perturbations\\\\\\\\\\\\\\\\nto each point. Specifically, we added noise sampled from a uniform distribution in the range\\\\\\\\\\\\\\\\n[−0.001,0.001] to each coordinate. This perturbation step enhances the testing process by intro-\\\\\\\\\\\\\\\\nducing slight variability into the state space.\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'For each perturbed state, we initialized the environment and executed the agent starting from\\\\\\\\\\\\\\\\nthat point. We then compared the resulting rewards with those obtained by executing the agent\\\\\\\\\\\\\\\\nunder normal conditions.\\\\\\\\\\\\\\\\nDue to the nature of theLunarLander-v2 environment, an additional consideration is required.\\\\\\\\\\\\\\\\nIn this environment, the lander always starts at coordinatex= 0.0 and approximatelyy≈1.41\\\\\\\\\\\\\\\\nin normalized coordinates. In the internalBox2D coordinate system, this corresponds to a fixed\\\\\\\\\\\\\\\\nstarting point near(10.0, 13.33).\\\\\\\\\\\\\\\\nTherefore, it is necessary to validate whether a given state can be considered a valid initial\\\\\\\\\\\\\\\\nstate for the environment. Only those states satisfying this condition are used, and perturbations\\\\\\\\\\\\\\\\nare applied to them within the uniform random range described earlier. These perturbed states\\\\\\\\\\\\\\\\nare then used to initialize the environment during testing.\\\\\\\\\\\\\\\\n5.5. An Example\\\\\\\\\\\\\\\\nIn this section we show a example produced by test generator, we show you how start and all\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'taking of decisions through the sequence of states.\\\\\\\\\\\\\\\\n30\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5.6. Evaluation and Results\\\\\\\\\\\\\\\\n5.6. Evaluation and Results\\\\\\\\\\\\\\\\nIn this section, we present the evaluation and results for each agent. Two types of analyses are\\\\\\\\\\\\\\\\nincluded:\\\\\\\\\\\\\\\\nFirst, we report statistical comparisons of the average rewards. The goal is to determine\\\\\\\\\\\\\\\\nwhether, in general, the regions proposed by the test case generator result in lower rewards\\\\\\\\\\\\\\\\ncompared to normal agent execution.\\\\\\\\\\\\\\\\nSecond, we evaluate the probability of success, defined as the proportion of episodes where the\\\\\\\\\\\\\\\\nagent achieves a total reward greater than or equal to 200. It is important to clarify that using\\\\\\\\\\\\\\\\nthe mean reward alone may be misleading in some cases: an agent could obtain a few episodes\\\\\\\\\\\\\\\\nwith very high rewards, raising the average, while still failing to consistently solve the task (i.e.,\\\\\\\\\\\\\\\\nachieving fewer successful episodes). Therefore, both the average performance and the success\\\\\\\\\\\\\\\\nrate are considered in the analysis.\\\\\\\\\\\\\\\\n5.6.1. Cartpole\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'The following section provides a summary of the vocabulary size (i.e., number of abstract classes)\\\\\\\\\\\\\\\\ngenerated for each agent in cartpole environment, as well as the number of regions identified as\\\\\\\\\\\\\\\\ninitial regions with a high probability of failure or low expected reward by the transformer. We\\\\\\\\\\\\\\\\nalso report the sample size, defined as the number of test cases generated by the Concrete Test\\\\\\\\\\\\\\\\nCase Generator.\\\\\\\\\\\\\\\\nAgent Vocabulary Size Number of Initial Regions Sample Size\\\\\\\\\\\\\\\\nShakti 101 42 4000\\\\\\\\\\\\\\\\nKapil 101 42 4200\\\\\\\\\\\\\\\\nNihal 101 42 3900\\\\\\\\\\\\\\\\nTable 5.3: Summary of vocabulary size, number of initial regions and sample size used for each\\\\\\\\\\\\\\\\nagent, Cartpole.\\\\\\\\\\\\\\\\nThe sample size represents too the total number of episodes executed for both normal execution\\\\\\\\\\\\\\\\n31\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\nand simulated testing. The variation in sample size across agents arises from the design of the\\\\\\\\\\\\\\\\ntest case generator, which attempts to sample up to 100 states per risk region. However, in\\\\\\\\\\\\\\\\npractice, some abstract classes contain fewer than 100 states, limiting the number of available\\\\\\\\\\\\\\\\ntest cases.\\\\\\\\\\\\\\\\nWe now present the table of statistical comparisons based on average rewards:\\\\\\\\\\\\\\\\nAgent Mean SD SEM t-statistic P-value µtesting <µexecution\\\\\\\\\\\\\\\\nShakti\\\\\\\\\\\\\\\\nSample Testing 96.98 67.95 1.07 -80.845 p< 1 ×10−10 yes\\\\\\\\\\\\\\\\nNormal Execution 185.30 12.52 0.20\\\\\\\\\\\\\\\\nKapil\\\\\\\\\\\\\\\\nSample Testing 190.86 35.06 0.54 -76.536 p< 1 ×10−10 yes\\\\\\\\\\\\\\\\nNormal Execution 235.06 13.09 0.20\\\\\\\\\\\\\\\\nNihal\\\\\\\\\\\\\\\\nSample Testing 182.47 82.53 1.32 7.371 p= 1.000 no\\\\\\\\\\\\\\\\nNormal Execution 169.23 75.92 1.22\\\\\\\\\\\\\\\\nTable 5.4: Comparison of sample testing and normal execution for each agent in cartpole envi-\\\\\\\\\\\\\\\\nronment.\\\\\\\\\\\\\\\\nFor the agents Kapil and Shakti (CartPole), we can conclude that there is sufficient statistical\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'evidence to support the claim that the test case generator identifies regions of significantly lower\\\\\\\\\\\\\\\\nreward.\\\\\\\\\\\\\\\\nIn particular, note that for Kapil—an agent with a perfect success rate under normal conditions\\\\\\\\\\\\\\\\n(as we will discuss later)— the generator produced test cases with an average reward of 190.86,\\\\\\\\\\\\\\\\ncompared to 235.06 under normal execution.\\\\\\\\\\\\\\\\nSimilarly, for Shakti, the mean reward under generated test cases dropped to 96.98, nearly\\\\\\\\\\\\\\\\nhalf of the 185.30 average under normal conditions, indicating that the generator successfully\\\\\\\\\\\\\\\\nidentified regions associated with very low performance.\\\\\\\\\\\\\\\\nWe now present the success rate comparison for all agents, where success is defined as achieving\\\\\\\\\\\\\\\\n32\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5.6. Evaluation and Results\\\\\\\\\\\\\\\\na total reward of at least 200 per episode.\\\\\\\\\\\\\\\\nCondition Success Rate z-statistic p-value Ptesting <Pexecution\\\\\\\\\\\\\\\\nShakti\\\\\\\\\\\\\\\\nSample Testing 0.047 -14.418 p= 2.005 ×10−47 yes\\\\\\\\\\\\\\\\nNormal Execution 0.141\\\\\\\\\\\\\\\\nKapil\\\\\\\\\\\\\\\\nSample Testing 0.428 -57.982 p< 1 ×10−10 yes\\\\\\\\\\\\\\\\nNormal Execution 1.000\\\\\\\\\\\\\\\\nNihal\\\\\\\\\\\\\\\\nSample Testing 0.305 8.135 p= 1.000 no\\\\\\\\\\\\\\\\nNormal Execution 0.224\\\\\\\\\\\\\\\\nTable 5.5: Summary of success rate across agents and conditions in cartpole environment.\\\\\\\\\\\\\\\\nBased on the results, we can conclude that for the agents Kapil and Shakti (CartPole), there\\\\\\\\\\\\\\\\nis sufficient statistical evidence to affirm that the probability of success is significantly lower in\\\\\\\\\\\\\\\\nthe test case scenarios than under normal execution conditions.\\\\\\\\\\\\\\\\nNotably, even for Kapil—an agent that achieved perfect performance under normal conditions,\\\\\\\\\\\\\\\\nnever obtaining a reward below 200— the test case generator was able to identify scenarios where\\\\\\\\\\\\\\\\nfailures were possible, demonstrating its capacity to expose potential weaknesses in otherwise\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'robust policies.\\\\\\\\\\\\\\\\n5.6.2. Lunar lander\\\\\\\\\\\\\\\\nThe following section provides a summary of the vocabulary size (i.e., number of abstract classes)\\\\\\\\\\\\\\\\ngenerated for each agent in Lunar lander environment, as well the number of regions identified\\\\\\\\\\\\\\\\nas initial regions with a high probability of failure or low expected reward by the transformer.\\\\\\\\\\\\\\\\nWe also report the sample size, defined as the number of test cases generated by the Concrete\\\\\\\\\\\\\\\\nTest Case Generator.\\\\\\\\\\\\\\\\n33\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\nAgent Vocabulary Size Number of Initial Regions Sample Size\\\\\\\\\\\\\\\\nSanket 840 58 781\\\\\\\\\\\\\\\\nKhordoo 711 58 1892\\\\\\\\\\\\\\\\nRokenes 868 64 779\\\\\\\\\\\\\\\\nTable 5.6: Summary of vocabulary size, number of initial regions and sample size used for each\\\\\\\\\\\\\\\\nagent, Lunar Lander.\\\\\\\\\\\\\\\\nThe sample size represents too the total number of episodes executed for both normal execution\\\\\\\\\\\\\\\\nand simulated testing. The variation in sample size across agents arises from the design of the\\\\\\\\\\\\\\\\ntest case generator, which attempts to sample up to 100 states per risk region. However, in\\\\\\\\\\\\\\\\npractice, some abstract classes contain fewer than 100 states, limiting the number of available\\\\\\\\\\\\\\\\ntest cases.\\\\\\\\\\\\\\\\nThis effect is more pronounced in the LunarLander agents, where not all states within an\\\\\\\\\\\\\\\\nabstract class can be considered valid initial states for the environment. As a result, the effective\\\\\\\\\\\\\\\\nnumber of testable samples per region may be reduced, impacting the total sample size.\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'We now present the table of statistical comparisons based on average rewards:\\\\\\\\\\\\\\\\n34\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5.6. Evaluation and Results\\\\\\\\\\\\\\\\nAgent Mean SD SEM t-statistic P-value µtesting <µexecution\\\\\\\\\\\\\\\\nSanket\\\\\\\\\\\\\\\\nSample Testing 234.24 34.91 1.25 -2.738 p= 3.132×10−3 no\\\\\\\\\\\\\\\\nNormal Execution 239.81 44.94 1.61\\\\\\\\\\\\\\\\nKhordoo\\\\\\\\\\\\\\\\nSample Testing 200.58 71.18 1.64 -4.980 p< 3.328×10−7 yes\\\\\\\\\\\\\\\\nNormal Execution 212.07 70.77 1.63\\\\\\\\\\\\\\\\nRokenes\\\\\\\\\\\\\\\\nSample Testing 263.70 76.17 2.73 -1.487 p= 6.857×10−2 no\\\\\\\\\\\\\\\\nNormal Execution 268.92 61.64 2.21\\\\\\\\\\\\\\\\nTable 5.7: Comparison of sample testing and normal execution for each agent in Lunar Lander\\\\\\\\\\\\\\\\nenvironment.\\\\\\\\\\\\\\\\nFor the agent Khordoo (LunarLander), we can conclude that there is sufficient statistical\\\\\\\\\\\\\\\\nevidence to support the claim that the test case generator identifies regions with significantly\\\\\\\\\\\\\\\\nlower rewards.\\\\\\\\\\\\\\\\nHowever, for the agents Sanket and Rokenes, we cannot assert that the generator found regions\\\\\\\\\\\\\\\\nwith lower rewards compared to a normal execution.\\\\\\\\\\\\\\\\nWe now present the success rate comparison for all agents, where success is defined as achieving\\\\\\\\\\\\\\\\na total reward of at least 200 per episode.\\\\\\\\\\\\\\\\n35\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\nCondition Success Rate z-statistic p-value Ptesting <Pexecution\\\\\\\\\\\\\\\\nSanket\\\\\\\\\\\\\\\\nSample Testing 0.862 -4.964 p= 3.444 ×10−7 yes\\\\\\\\\\\\\\\\nNormal Execution 0.937\\\\\\\\\\\\\\\\nKhordoo\\\\\\\\\\\\\\\\nSample Testing 0.627 -7.295 p= 1.490 ×10−13 yes\\\\\\\\\\\\\\\\nNormal Execution 0.737\\\\\\\\\\\\\\\\nRokenes\\\\\\\\\\\\\\\\nSample Testing 0.910 -2.529 p= 5.713 ×10−3 yes\\\\\\\\\\\\\\\\nNormal Execution 0.944\\\\\\\\\\\\\\\\nTable 5.8: Summary of success rate across agents and conditions in Lunar Lander.\\\\\\\\\\\\\\\\nBased on the results, we can conclude that for the agents Khordoo, Sanket, and Rokenes\\\\\\\\\\\\\\\\n(LunarLander), there is sufficient statistical evidence to affirm that the probability of success is\\\\\\\\\\\\\\\\nsignificantly lower in the test case scenarios than under normal execution conditions.\\\\\\\\\\\\\\\\n5.6.3. A Small Example\\\\\\\\\\\\\\\\nIn this subsection, we present an example in which the Shakti agent fails, achieving a total reward\\\\\\\\\\\\\\\\nof 81.0. Recall that a state in the CartPole environment is represented by anndarray of four\\\\\\\\\\\\\\\\ncomponents, as described in Table 5.9:\\\\\\\\\\\\\\\\n36\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5.6. Evaluation and Results\\\\\\\\\\\\\\\\nIndex Observation Minimum Maximum\\\\\\\\\\\\\\\\n0 Cart Position −4 .8 4 .8\\\\\\\\\\\\\\\\n1 Cart Velocity −∞ ∞\\\\\\\\\\\\\\\\n2 Pole Angle −0 .418 rad (−24 ◦) 0 .418 rad (24 ◦)\\\\\\\\\\\\\\\\n3 Pole Angular Velocity −∞ ∞\\\\\\\\\\\\\\\\nTable 5.9: Observation space of the CartPole environment: anndarray of shape(4,).\\\\\\\\\\\\\\\\nWe initialize the environment to the following state, generated by our test generator:\\\\\\\\\\\\\\\\nx0 =\\\\\\\\\\\\\\\\n[\\\\\\\\\\\\\\\\n−0.73915684 −0.89601650 −0.03282847 0 .23753740\\\\\\\\\\\\\\\\n]\\\\\\\\\\\\\\\\n.\\\\\\\\\\\\\\\\nFigure 5.9 illustrates this initial configuration:\\\\\\\\\\\\\\\\nFigure 5.9: InitialstateoftheCartPoleenvironmentwithcartposition −0.73915684, cartvelocity\\\\\\\\\\\\\\\\n−0.89601650, pole angle−0.03282847, and pole angular velocity0.23753740.\\\\\\\\\\\\\\\\nFigure 5.10 shows the final state of the episode, wherein the agent has pushed the cart beyond\\\\\\\\\\\\\\\\nthe left boundary:\\\\\\\\\\\\\\\\n37\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\nFigure 5.10: Final state of the CartPole environment with cart position−2.41205427, cart veloc-\\\\\\\\\\\\\\\\nity −1.42968531, pole angle−0.06067818, and pole angular velocity−0.02501499.\\\\\\\\\\\\\\\\nConsequently, the agent performs more “left” actions (0) than “right” actions (1), demon-\\\\\\\\\\\\\\\\nstrating an inability to counteract the tendency toward negative velocity. Below, we present the\\\\\\\\\\\\\\\\ncomplete sequence of actions taken during the episode:\\\\\\\\\\\\\\\\n[−0.73915684 −0.8960165 −0.03282847 0.2375374 ] 1 1.0 False\\\\\\\\\\\\\\\\n[−0.75707717 −0.70044128 −0.02807772 −0.06531717] 1 1.0 False\\\\\\\\\\\\\\\\n[−0.771086 −0.50492826 −0.02938406 −0.36672488] 0 1.0 False\\\\\\\\\\\\\\\\n[−0.78118456 −0.6996206 −0.03671856 −0.0834501 ] 0 1.0 False\\\\\\\\\\\\\\\\n[−0.79517697 −0.89419749 −0.03838756 0.19742567] 1 1.0 False\\\\\\\\\\\\\\\\n[−0.81306092 −0.69854809 −0.03443905 −0.1071154 ] 0 1.0 False\\\\\\\\\\\\\\\\n[−0.82703189 −0.89316003 −0.03658136 0.17450633] 1 1.0 False\\\\\\\\\\\\\\\\n[−0.84489509 −0.69753415 −0.03309123 −0.12948869] 0 1.0 False\\\\\\\\\\\\\\\\n[−0.85884577 −0.89216682 −0.03568101 0.15257344] 1 1.0 False\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'[−0.87668911 −0.69655261 −0.03262954 −0.1511491 ] 0 1.0 False\\\\\\\\\\\\\\\\n[−0.89062016 −0.89119251 −0.03565252 0.13106395] 1 1.0 False\\\\\\\\\\\\\\\\n[−0.90844401 −0.69557846 −0.03303124 −0.17265027] 0 1.0 False\\\\\\\\\\\\\\\\n38\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5.6. Evaluation and Results\\\\\\\\\\\\\\\\n[−0.92235558 −0.89021247 −0.03648425 0.10943207] 1 1.0 False\\\\\\\\\\\\\\\\n[−0.94015983 −0.69458722 −0.0342956 −0.19453452] 0 1.0 False\\\\\\\\\\\\\\\\n[−0.95405157 −0.88920223 −0.03818629 0.0871354 ] 1 1.0 False\\\\\\\\\\\\\\\\n[−0.97183562 −0.69355431 −0.03644359 −0.21734658] 0 1.0 False\\\\\\\\\\\\\\\\n[−0.9857067 −0.88813685 −0.04079052 0.06362139] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.00346944 −1.08265094 −0.03951809 0.34316074] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.02512246 −0.88698972 −0.03265488 0.03828275] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.04286225 −0.69141508 −0.03188922 −0.26452163] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.05669055 −0.8860677 −0.03717965 0.01793501] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.07441191 −1.08063728 −0.03682095 0.29865938] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.09602465 −0.88501034 −0.03084776 −0.00540505] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.11372486 −0.68945988 −0.03095587 −0.30765899] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.12751406 −0.88412737 −0.03710905 −0.02489722] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.14519661 −1.07869804 −0.03760699 0.2558503 ] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.16677057 −0.8830599 −0.03248998 −0.04845328] 1 1.0 False\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'[−1.18443176 −0.68748749 −0.03345905 −0.35120743] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.19818151 −0.88211804 −0.0404832 −0.06926014] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.21582387 −1.07663691 −0.0418684 0.21038029] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.23735661 −0.8809421 −0.0376608 −0.09521039] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.25497546 −1.0755046 −0.039565 0.18535677] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.27648555 −0.87983955 −0.03585787 −0.1195402 ] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.29408234 −1.07442989 −0.03824867 0.16161772] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.31557094 −0.87878184 −0.03501632 −0.14288208] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.33314657 −1.07338526 −0.03787396 0.13855142] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.35461428 −0.87774189 −0.03510293 −0.16583546] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.37216912 −1.07234423 −0.03841964 0.11557008] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.393616 −0.87669345 −0.03610824 −0.18898212] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.41114987 −1.07128071 −0.03988788 0.092095 ] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.43257548 −0.87561041 −0.03804598 −0.21290092] 0 1.0 False\\\\\\\\\\\\\\\\n39\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\n[−1.45008769 −1.07016834 −0.042304 0.06754196] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.47149106 −0.87446621 −0.04095316 −0.23818226] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.48898038 −1.0689799 −0.0457168 0.04130677] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.51035998 −0.87323321 −0.04489067 −0.26544255] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.52782465 −1.06768666 −0.05019952 0.01275036] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.54917838 −1.26205408 −0.04994451 0.28918176] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.57441946 −1.06625682 −0.04416088 −0.01882548] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.5957446 −0.87053029 −0.04453739 −0.32510813] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.6131552 −1.06499074 −0.05103955 −0.04679636] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.63445502 −1.25934507 −0.05197548 0.22935638] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.65964192 −1.0635204 −0.04738835 −0.07925787] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.68091233 −1.25793212 −0.04897351 0.19810537] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.70607097 −1.06214514 −0.0450114 −0.10961544] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.72731387 −1.25659416 −0.04720371 0.16853378] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.75244575 −1.06082944 −0.04383303 −0.13865894] 1 1.0 False\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'[−1.77366234 −0.86510799 −0.04660621 −0.44484192] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.7909645 −1.05954062 −0.05550305 −0.16720693] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.81215532 −1.25382597 −0.05884719 0.10746281] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.83723184 −1.05791226 −0.05669793 −0.20319016] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.85839008 −1.25217946 −0.06076174 0.07108213] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.88343367 −1.05624141 −0.05934009 −0.24013553] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.9045585 −1.25046768 −0.0641428 0.03325534] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.92956785 −1.44461395 −0.0634777 0.30503082] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.95846013 −1.24864754 −0.05737708 −0.00697668] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.98343308 −1.44290169 −0.05751661 0.26706543] 1 1.0 False\\\\\\\\\\\\\\\\n[−2.01229112 −1.24700805 −0.05217531 −0.04318969] 0 1.0 False\\\\\\\\\\\\\\\\n[−2.03723128 −1.44134451 −0.0530391 0.23258573] 1 1.0 False\\\\\\\\\\\\\\\\n[−2.06605817 −1.24550639 −0.04838738 −0.07634453] 1 1.0 False\\\\\\\\\\\\\\\\n[−2.09096829 −1.04972537 −0.04991428 −0.38389269] 0 1.0 False\\\\\\\\\\\\\\\\n40\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5.6. Evaluation and Results\\\\\\\\\\\\\\\\n[−2.1119628 −1.24410443 −0.05759213 −0.10735593] 0 1.0 False\\\\\\\\\\\\\\\\n[−2.13684489 −1.43835581 −0.05973925 0.16661531] 1 1.0 False\\\\\\\\\\\\\\\\n[−2.16561201 −1.24243186 −0.05640694 −0.14429925] 0 1.0 False\\\\\\\\\\\\\\\\n[−2.19046064 −1.43670254 −0.05929293 0.13006845] 1 1.0 False\\\\\\\\\\\\\\\\n[−2.21919469 −1.24078355 −0.05669156 −0.18071551] 0 1.0 False\\\\\\\\\\\\\\\\n[−2.24401037 −1.43505037 −0.06030587 0.09355819] 1 1.0 False\\\\\\\\\\\\\\\\n[−2.27271137 −1.23911823 −0.0584347 −0.21752494] 0 1.0 False\\\\\\\\\\\\\\\\n[−2.29749374 −1.43335827 −0.0627852 0.05616779] 1 1.0 False\\\\\\\\\\\\\\\\n[−2.3261609 −1.23739489 −0.06166185 −0.25564483] 0 1.0 False\\\\\\\\\\\\\\\\n[−2.3509088 −1.43158473 −0.06677474 0.01696925] 0 1.0 False\\\\\\\\\\\\\\\\n[−2.3795405 −1.62568867 −0.06643536 0.28785909] 1 1.0 True\\\\\\\\\\\\\\\\n[−2.41205427 −1.42968531 −0.06067818 −0.02501499] total_reward : 81.0\\\\\\\\\\\\\\\\n5.6.4. Threats to Validity\\\\\\\\\\\\\\\\nThreats to external validityOur experiment aims to validate the use of the test case gener-\\\\\\\\\\\\\\\\nator for evaluating reinforcement learning agents. We selected a total of six agents from two\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'different environments—CartPole and LunarLander—ranked at different positions in the public\\\\\\\\\\\\\\\\nleaderboard. This selection strategy was designed to capture a range of agent performances\\\\\\\\\\\\\\\\nand architectures, thereby reducing the risk of bias and improving the generalizability of our\\\\\\\\\\\\\\\\nfindings.\\\\\\\\\\\\\\\\nHowever, we observed that in some cases, the agents’ behavior did not align with expectations\\\\\\\\\\\\\\\\nbased on their leaderboard rankings. The most notable example was the agent Shakti, which,\\\\\\\\\\\\\\\\ndespite being ranked highest among the CartPole agents, showed the worst performance during\\\\\\\\\\\\\\\\nnormal execution, with a success rate of only 14%.\\\\\\\\\\\\\\\\nSimilarly, although the selected LunarLander agents were distributed across different leader-\\\\\\\\\\\\\\\\nboard positions, all of them achieved average rewards above 200 in our experiments. In partic-\\\\\\\\\\\\\\\\nular, the agent Rokenes achieved both the highest average reward and the highest success rate,\\\\\\\\\\\\\\\\ndespite not being the top-ranked agent in the public leaderboard.\\\\\\\\\\\\\\\\n41\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\nThese discrepancies suggest that leaderboard rankings may not fully reflect agent robustness un-\\\\\\\\\\\\\\\\nder different testing conditions, and highlight the importance of evaluating agents using diverse\\\\\\\\\\\\\\\\nand controlled scenarios.\\\\\\\\\\\\\\\\nThreats to internal validityOne potential threat to internal validity arises from the specific\\\\\\\\\\\\\\\\nhandling required for the LunarLander environment. In this case, it was necessary to validate\\\\\\\\\\\\\\\\nwhether the sampled states could be considered valid initial states, since the environment can\\\\\\\\\\\\\\\\nonly be initialized from a limited region of the state space.\\\\\\\\\\\\\\\\nBy default, LunarLander starts from normalized coordinates(x= 0.0, y≈1.41), which corre-\\\\\\\\\\\\\\\\nspond to approximately(10.0, 13.33) in Box2D coordinates. Any perturbation or state sampled\\\\\\\\\\\\\\\\noutside of this valid initialization region would not be accepted by the environment as a true\\\\\\\\\\\\\\\\nstarting state.\\\\\\\\\\\\\\\\nAs a result, the number of valid samples per region was often reduced, leading to smaller overall\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'sample sizes for these agents. This constraint may have introduced a bias in the testing data or\\\\\\\\\\\\\\\\naffected the statistical power of the results in the LunarLander experiments.\\\\\\\\\\\\\\\\n42\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'43\\\\\\\\\\\\\\\\nChapter 6\\\\\\\\\\\\\\\\nConclusion, Limitations and Future Work\\\\\\\\\\\\\\\\n6.1. Conclusion\\\\\\\\\\\\\\\\nThis thesis explores the problem of testing reinforcement learning agents through abstract classes\\\\\\\\\\\\\\\\nand partitions.\\\\\\\\\\\\\\\\nWe proposedMarTest-Pipeline, an implementation that generates abstract representations of\\\\\\\\\\\\\\\\nagent behavior, uses a transformer conditioned on reward and failure probability to identify\\\\\\\\\\\\\\\\nfailure-prone regions, and constructs targeted test cases from those regions.\\\\\\\\\\\\\\\\nOur results demonstrated that this approach can uncover regions of reduced performance in\\\\\\\\\\\\\\\\nmultiple agents, including those with perfect scores under normal execution. These findings\\\\\\\\\\\\\\\\nsupport the idea that standard agent evaluations may overlook critical edge cases and that\\\\\\\\\\\\\\\\nsymbolic representations—when combined with generative models—can enhance the testing and\\\\\\\\\\\\\\\\nunderstanding of agent behavior.\\\\\\\\\\\\\\\\nBeyondtheimplementation, thisworkproposesabroaderconceptualview: thattheinteraction\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'between an agent and its environment can be seen as a form of emergent language. Reward,\\\\\\\\\\\\\\\\nin this context, is not only a signal of performance, but a reflection of how well the agent has\\\\\\\\\\\\\\\\ninternalized a language and this thesis constitutes the first step in a research direction.\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'6. Conclusion, Limitations and Future Work\\\\\\\\\\\\\\\\n6.2. Limitations\\\\\\\\\\\\\\\\nThis work presents a novel approach to testing reinforcement learning agents using a symbolic\\\\\\\\\\\\\\\\ngenerator based on transformer models and abstract state-action representations. While the\\\\\\\\\\\\\\\\nresults are promising and demonstrate the generator’s ability to identify failure-prone regions,\\\\\\\\\\\\\\\\nseveral limitations must be acknowledged:\\\\\\\\\\\\\\\\n• Dependence on initial state space constraints:In the LunarLander environment,\\\\\\\\\\\\\\\\nonly a limited subset of states can be used as valid initial states. This required an explicit\\\\\\\\\\\\\\\\nvalidation step to ensure sampled states could be used for environment initialization. As\\\\\\\\\\\\\\\\na consequence, the effective sample size for LunarLander agents was reduced, potentially\\\\\\\\\\\\\\\\nlimiting the statistical power of the results.\\\\\\\\\\\\\\\\n• Uneven sample sizes across agents:The test case generator attempts to sample up to\\\\\\\\\\\\\\\\n100 states per high-risk region. However, some abstract classes contained fewer than 100\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'states in both CartPole and LunarLander environments, resulting in variation in sample\\\\\\\\\\\\\\\\nsizes across agents and environments. This may have introduced some imbalance in the\\\\\\\\\\\\\\\\nstatistical comparisons.\\\\\\\\\\\\\\\\n• Fixed abstraction level:All experiments were conducted using a fixed abstraction level\\\\\\\\\\\\\\\\nof d = 4 . While this value provided a reasonable trade-off between expressiveness and\\\\\\\\\\\\\\\\ngenerality, it is possible that a different abstraction depth might yield better or more\\\\\\\\\\\\\\\\ninterpretable results depending on the agent and environment.\\\\\\\\\\\\\\\\n• Limited evaluation scope: The approach was tested only on agents from two envi-\\\\\\\\\\\\\\\\nronments: CartPole and LunarLander. Although these environments differ in complexity\\\\\\\\\\\\\\\\nand dynamics, further evaluation on more diverse tasks—particularly continuous control\\\\\\\\\\\\\\\\nand high-dimensional environments—is needed to assess the general applicability of the\\\\\\\\\\\\\\\\nmethod.\\\\\\\\\\\\\\\\n• Transformer conditioning is limited:The transformer model was trained conditioned\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'only on the average reward and failure probability. While this was sufficient for the current\\\\\\\\\\\\\\\\nexperiments, future versions could incorporate richer contextual information (e.g., policy\\\\\\\\\\\\\\\\n44\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'6.3. Future Work\\\\\\\\\\\\\\\\nentropy, time horizon, or agent architecture) to improve the diversity and precision of\\\\\\\\\\\\\\\\ngenerated test cases.\\\\\\\\\\\\\\\\n• Mismatchwithleaderboardexpectations: Insomecases, theperformanceobservedin\\\\\\\\\\\\\\\\nnormal execution did not match the agent’s leaderboard ranking. For example, the Shakti\\\\\\\\\\\\\\\\nagent, which held the top rank among CartPole submissions, showed the lowest success\\\\\\\\\\\\\\\\nrate in our experiments. This highlights the need to interpret leaderboard positions with\\\\\\\\\\\\\\\\ncaution and emphasizes the value of independent and reproducible testing frameworks.\\\\\\\\\\\\\\\\n6.3. Future Work\\\\\\\\\\\\\\\\nOne potential extension is to design abstraction mechanisms that are adaptive—i.e., where the\\\\\\\\\\\\\\\\nabstraction depth is learned based on agent behavior or performance metrics. Another important\\\\\\\\\\\\\\\\ndirection for future work is to implement alternative definitions of abstract classes. This is\\\\\\\\\\\\\\\\nespecially relevant, as it may provide better insight into the interaction between the agent and the\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'environment. Lastly, incorporating richer conditioning signals into the Transformer, or exploring\\\\\\\\\\\\\\\\narchitectures specifically tailored for causal reasoning in RL environments, may improve the\\\\\\\\\\\\\\\\nquality of the generated test cases and help uncover more nuanced weaknesses in agent policies.\\\\\\\\\\\\\\\\nThis work was guided by the intuition thatreward may be a consequence of a shared language\\\\\\\\\\\\\\\\nbetween the agent and the environment. While this idea is still in an early and exploratory stage,\\\\\\\\\\\\\\\\nit suggests several possible directions for future research.\\\\\\\\\\\\\\\\nOne such direction is to investigate the relationship between the level of abstraction used\\\\\\\\\\\\\\\\nto represent agent-environment interactions and the rewards obtained. It may be possible to\\\\\\\\\\\\\\\\nformulate an optimization process—either analytical or learned—that selects the abstraction\\\\\\\\\\\\\\\\nlevel most appropriate for a given task or agent architecture.\\\\\\\\\\\\\\\\nAnother line of inquiry involves a closer integration between abstract symbolic representations\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'and transformer-based models. Rather than conditioning the transformer on simple reward\\\\\\\\\\\\\\\\nstatistics, future models could be trained to evaluate or even generate abstract behavior trajec-\\\\\\\\\\\\\\\\ntories as structured linguistic sequences—potentially assigning meaning and structure to patterns\\\\\\\\\\\\\\\\nof interaction.\\\\\\\\\\\\\\\\n45\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'6. Conclusion, Limitations and Future Work\\\\\\\\\\\\\\\\nAlthough preliminary, these ideas point to a broader aspiration: to better understand the role\\\\\\\\\\\\\\\\nof representation, communication, and interpretation in reinforcement learning. By approaching\\\\\\\\\\\\\\\\nthe agent-environment loop as a form of emergent communication, we hope to take small steps\\\\\\\\\\\\\\\\ntoward a more unified view of learning, abstraction, and meaning.\\\\\\\\\\\\\\\\n46\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n47\\\\\\\\\\\\\\\\nChapter A\\\\\\\\\\\\\\\\nAppendix\\\\\\\\\\\\\\\\nA.1. MartTest Pipeline Requirements\\\\\\\\\\\\\\\\nabsl -py ==1.4.0\\\\\\\\\\\\\\\\naccelerate ==1.6.0\\\\\\\\\\\\\\\\nanyio ==3.6.2\\\\\\\\\\\\\\\\nargon2 - cffi ==21.3.0\\\\\\\\\\\\\\\\nargon2 -cffi - bindings ==21.2.0\\\\\\\\\\\\\\\\narrow ==1.2.3\\\\\\\\\\\\\\\\nasttokens ==2.2.1\\\\\\\\\\\\\\\\nastunparse ==1.6.3\\\\\\\\\\\\\\\\nattrs ==22.2.0\\\\\\\\\\\\\\\\nBabel ==2.11.0\\\\\\\\\\\\\\\\nbackcall ==0.2.0\\\\\\\\\\\\\\\\nbeautifulsoup4 ==4.11.1\\\\\\\\\\\\\\\\nbleach ==5.0.1\\\\\\\\\\\\\\\\nbox2d -py ==2.3.5\\\\\\\\\\\\\\\\ncachetools ==5.2.1\\\\\\\\\\\\\\\\ncertifi ==2022.12.7\\\\\\\\\\\\\\\\ncffi ==1.15.1\\\\\\\\\\\\\\\\ncharset - normalizer ==3.0.1\\\\\\\\\\\\\\\\ncloudpickle ==3.1.1\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'A. Appendix\\\\\\\\\\\\\\\\ncomm ==0.1.2\\\\\\\\\\\\\\\\ncontourpy ==1.0.7\\\\\\\\\\\\\\\\ncycler ==0.11.0\\\\\\\\\\\\\\\\ndebugpy ==1.6.5\\\\\\\\\\\\\\\\ndecorator ==5.1.1\\\\\\\\\\\\\\\\ndefusedxml ==0.7.1\\\\\\\\\\\\\\\\nentrypoints ==0.4\\\\\\\\\\\\\\\\nexecuting ==1.2.0\\\\\\\\\\\\\\\\nfastjsonschema ==2.16.2\\\\\\\\\\\\\\\\nfilelock ==3.18.0\\\\\\\\\\\\\\\\nflatbuffers ==23.1.4\\\\\\\\\\\\\\\\nfonttools ==4.38.0\\\\\\\\\\\\\\\\nfqdn ==1.5.1\\\\\\\\\\\\\\\\nfsspec ==2025.3.2\\\\\\\\\\\\\\\\ngast ==0.4.0\\\\\\\\\\\\\\\\ngoogle - auth ==2.16.0\\\\\\\\\\\\\\\\ngoogle -auth - oauthlib ==0.4.6\\\\\\\\\\\\\\\\ngoogle - pasta ==0.2.0\\\\\\\\\\\\\\\\ngrpcio ==1.51.1\\\\\\\\\\\\\\\\ngym ==0.26.2\\\\\\\\\\\\\\\\ngym - notices ==0.0.8\\\\\\\\\\\\\\\\nh5py ==3.7.0\\\\\\\\\\\\\\\\nhuggingface - hub ==0.30.2\\\\\\\\\\\\\\\\nidna ==3.4\\\\\\\\\\\\\\\\nipykernel ==6.20.2\\\\\\\\\\\\\\\\nipython ==8.8.0\\\\\\\\\\\\\\\\nipython - genutils ==0.2.0\\\\\\\\\\\\\\\\nisoduration ==20.11.0\\\\\\\\\\\\\\\\njedi ==0.18.2\\\\\\\\\\\\\\\\nJinja2 ==3.1.2\\\\\\\\\\\\\\\\njoblib ==1.4.2\\\\\\\\\\\\\\\\njson5 ==0.9.11\\\\\\\\\\\\\\\\n48\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'A.1. MartTest Pipeline Requirements\\\\\\\\\\\\\\\\njsonpointer ==2.3\\\\\\\\\\\\\\\\njsonschema ==4.17.3\\\\\\\\\\\\\\\\njupyter - events ==0.6.3\\\\\\\\\\\\\\\\njupyter_client ==7.4.9\\\\\\\\\\\\\\\\njupyter_core ==5.1.3\\\\\\\\\\\\\\\\njupyter_server ==2.1.0\\\\\\\\\\\\\\\\njupyter_server_terminals ==0.4.4\\\\\\\\\\\\\\\\njupyterlab ==3.5.2\\\\\\\\\\\\\\\\njupyterlab - pygments ==0.2.2\\\\\\\\\\\\\\\\njupyterlab_server ==2.19.0\\\\\\\\\\\\\\\\nkeras ==2.11.0\\\\\\\\\\\\\\\\nkiwisolver ==1.4.4\\\\\\\\\\\\\\\\nlibclang ==15.0.6.1\\\\\\\\\\\\\\\\nMarkdown ==3.4.1\\\\\\\\\\\\\\\\nMarkupSafe ==2.1.1\\\\\\\\\\\\\\\\nmatplotlib ==3.6.3\\\\\\\\\\\\\\\\nmatplotlib - inline ==0.1.6\\\\\\\\\\\\\\\\nmistune ==2.0.4\\\\\\\\\\\\\\\\nmpmath ==1.3.0\\\\\\\\\\\\\\\\nnbclassic ==0.4.8\\\\\\\\\\\\\\\\nnbclient ==0.7.2\\\\\\\\\\\\\\\\nnbconvert ==7.2.8\\\\\\\\\\\\\\\\nnbformat ==5.7.3\\\\\\\\\\\\\\\\nnest - asyncio ==1.5.6\\\\\\\\\\\\\\\\nnetworkx ==3.3\\\\\\\\\\\\\\\\nnotebook ==6.5.2\\\\\\\\\\\\\\\\nnotebook_shim ==0.2.2\\\\\\\\\\\\\\\\nnumpy ==1.24.1\\\\\\\\\\\\\\\\nnvidia - cublas - cu11 ==11.10.3.66\\\\\\\\\\\\\\\\nnvidia -cuda - nvrtc - cu11 ==11.7.99\\\\\\\\\\\\\\\\nnvidia -cuda - runtime - cu11 ==11.7.99\\\\\\\\\\\\\\\\nnvidia - cudnn - cu11 ==8.5.0.96\\\\\\\\\\\\\\\\n49\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'A. Appendix\\\\\\\\\\\\\\\\noauthlib ==3.2.2\\\\\\\\\\\\\\\\nopt - einsum ==3.3.0\\\\\\\\\\\\\\\\npackaging ==23.0\\\\\\\\\\\\\\\\npandas ==2.2.3\\\\\\\\\\\\\\\\npandocfilters ==1.5.0\\\\\\\\\\\\\\\\nparso ==0.8.3\\\\\\\\\\\\\\\\npexpect ==4.8.0\\\\\\\\\\\\\\\\npickleshare ==0.7.5\\\\\\\\\\\\\\\\nPillow ==9.4.0\\\\\\\\\\\\\\\\nplatformdirs ==2.6.2\\\\\\\\\\\\\\\\nprometheus - client ==0.15.0\\\\\\\\\\\\\\\\nprompt - toolkit ==3.0.36\\\\\\\\\\\\\\\\nprotobuf ==3.19.6\\\\\\\\\\\\\\\\npsutil ==5.9.4\\\\\\\\\\\\\\\\nptyprocess ==0.7.0\\\\\\\\\\\\\\\\npure -eval ==0.2.2\\\\\\\\\\\\\\\\npyasn1 ==0.4.8\\\\\\\\\\\\\\\\npyasn1 - modules ==0.2.8\\\\\\\\\\\\\\\\npycparser ==2.21\\\\\\\\\\\\\\\\npygame ==2.1.0\\\\\\\\\\\\\\\\nPygments ==2.14.0\\\\\\\\\\\\\\\\npyparsing ==3.0.9\\\\\\\\\\\\\\\\npyrsistent ==0.19.3\\\\\\\\\\\\\\\\npython - dateutil ==2.8.2\\\\\\\\\\\\\\\\npython -json - logger ==2.0.4\\\\\\\\\\\\\\\\npytz ==2022.7.1\\\\\\\\\\\\\\\\nPyYAML ==6.0\\\\\\\\\\\\\\\\npyzmq ==25.0.0\\\\\\\\\\\\\\\\nregex ==2024.11.6\\\\\\\\\\\\\\\\nrequests ==2.28.2\\\\\\\\\\\\\\\\nrequests - oauthlib ==1.3.1\\\\\\\\\\\\\\\\nrfc3339 - validator ==0.1.4\\\\\\\\\\\\\\\\n50\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'A.1. MartTest Pipeline Requirements\\\\\\\\\\\\\\\\nrfc3986 - validator ==0.1.1\\\\\\\\\\\\\\\\nrsa ==4.9\\\\\\\\\\\\\\\\nsafetensors ==0.5.3\\\\\\\\\\\\\\\\nscikit - learn ==1.6.1\\\\\\\\\\\\\\\\nscipy ==1.15.2\\\\\\\\\\\\\\\\nSend2Trash ==1.8.0\\\\\\\\\\\\\\\\nsix ==1.16.0\\\\\\\\\\\\\\\\nsniffio ==1.3.0\\\\\\\\\\\\\\\\nsoupsieve ==2.3.2. post1\\\\\\\\\\\\\\\\nstack - data ==0.6.2\\\\\\\\\\\\\\\\nswig ==4.3.0\\\\\\\\\\\\\\\\nsympy ==1.13.3\\\\\\\\\\\\\\\\ntensorboard ==2.11.2\\\\\\\\\\\\\\\\ntensorboard -data - server ==0.6.1\\\\\\\\\\\\\\\\ntensorboard - plugin - wit ==1.8.1\\\\\\\\\\\\\\\\ntensorflow ==2.11.0\\\\\\\\\\\\\\\\ntensorflow - estimator ==2.11.0\\\\\\\\\\\\\\\\ntensorflow -io -gcs - filesystem ==0.29.0\\\\\\\\\\\\\\\\ntermcolor ==2.2.0\\\\\\\\\\\\\\\\nterminado ==0.17.1\\\\\\\\\\\\\\\\nthreadpoolctl ==3.6.0\\\\\\\\\\\\\\\\ntinycss2 ==1.2.1\\\\\\\\\\\\\\\\ntokenizers ==0.21.1\\\\\\\\\\\\\\\\ntomli ==2.0.1\\\\\\\\\\\\\\\\ntorch ==2.1.2+ cu118\\\\\\\\\\\\\\\\ntorchaudio ==2.1.2+ cu118\\\\\\\\\\\\\\\\ntorchvision ==0.16.2+ cu118\\\\\\\\\\\\\\\\ntornado ==6.2\\\\\\\\\\\\\\\\ntqdm ==4.67.1\\\\\\\\\\\\\\\\ntraitlets ==5.8.1\\\\\\\\\\\\\\\\ntransformers ==4.51.3\\\\\\\\\\\\\\\\ntriton ==2.1.0\\\\\\\\\\\\\\\\n51\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'A. Appendix\\\\\\\\\\\\\\\\ntyping_extensions ==4.4.0\\\\\\\\\\\\\\\\ntzdata ==2025.1\\\\\\\\\\\\\\\\nuri - template ==1.2.0\\\\\\\\\\\\\\\\nurllib3 ==1.26.14\\\\\\\\\\\\\\\\nwcwidth ==0.2.6\\\\\\\\\\\\\\\\nwebcolors ==1.12\\\\\\\\\\\\\\\\nwebencodings ==0.5.1\\\\\\\\\\\\\\\\nwebsocket - client ==1.4.2\\\\\\\\\\\\\\\\nWerkzeug ==2.2.2\\\\\\\\\\\\\\\\nwrapt ==1.14.1\\\\\\\\\\\\\\\\nSnippet A.1: Paquetes y versiones\\\\\\\\\\\\\\\\nA.2. Conditioned Transformer Model CondGPT2\\\\\\\\\\\\\\\\nCondGPT2 (\\\\\\\\\\\\\\\\n( transformer ): GPT2Model (\\\\\\\\\\\\\\\\n( wte ): Embedding (996 , 1024)\\\\\\\\\\\\\\\\n( wpe ): Embedding (1024 , 1024)\\\\\\\\\\\\\\\\n( drop ): Dropout (p =0.1 , inplace = False )\\\\\\\\\\\\\\\\n(h): ModuleList (\\\\\\\\\\\\\\\\n(0 -11) : 12 x GPT2Block (\\\\\\\\\\\\\\\\n( ln_1 ): LayerNorm ((1024 ,) , eps =1e -05 , elementwise_affine = True )\\\\\\\\\\\\\\\\n( attn ): GPT2Attention (\\\\\\\\\\\\\\\\n( c_attn ): Conv1D (nf =3072 , nx =1024)\\\\\\\\\\\\\\\\n( c_proj ): Conv1D (nf =1024 , nx =1024)\\\\\\\\\\\\\\\\n( attn_dropout ): Dropout (p =0.1 , inplace = False )\\\\\\\\\\\\\\\\n( resid_dropout ): Dropout (p =0.1 , inplace = False )\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\n( ln_2 ): LayerNorm ((1024 ,) , eps =1e -05 , elementwise_affine = True )\\\\\\\\\\\\\\\\n( mlp ): GPT2MLP (\\\\\\\\\\\\\\\\n( c_fc ): Conv1D (nf =4096 , nx =1024)\\\\\\\\\\\\\\\\n52\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'A.3. Requirements Shakti\\\\\\\\\\\\\\\\n( c_proj ): Conv1D (nf =1024 , nx =4096)\\\\\\\\\\\\\\\\n( act ): NewGELUActivation ()\\\\\\\\\\\\\\\\n( dropout ): Dropout (p =0.1 , inplace = False )\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\n( ln_f ): LayerNorm ((1024 ,) , eps =1e -05 , elementwise_affine = True )\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\n( lm_head ): Linear ( in_features =1024 , out_features =996 , bias = False )\\\\\\\\\\\\\\\\n( cond_proj ): Sequential (\\\\\\\\\\\\\\\\n(0) : Linear ( in_features =2 , out_features =1024 , bias = True )\\\\\\\\\\\\\\\\n(1) : ReLU ()\\\\\\\\\\\\\\\\n(2) : Linear ( in_features =1024 , out_features =1024 , bias = True )\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\nA.3. Requirements Shakti\\\\\\\\\\\\\\\\nabsl -py ==2.1.0\\\\\\\\\\\\\\\\nastor ==0.8.1\\\\\\\\\\\\\\\\ncachetools ==4.2.4\\\\\\\\\\\\\\\\ncertifi ==2025.1.31\\\\\\\\\\\\\\\\ncharset - normalizer ==3.4.1\\\\\\\\\\\\\\\\ncloudpickle ==1.2.2\\\\\\\\\\\\\\\\ncycler ==0.11.0\\\\\\\\\\\\\\\\nfuture ==1.0.0\\\\\\\\\\\\\\\\ngast ==0.2.2\\\\\\\\\\\\\\\\ngoogle - auth ==1.35.0\\\\\\\\\\\\\\\\ngoogle -auth - oauthlib ==0.4.6\\\\\\\\\\\\\\\\ngoogle - pasta ==0.2.0\\\\\\\\\\\\\\\\ngrpcio ==1.62.3\\\\\\\\\\\\\\\\n53\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'A. Appendix\\\\\\\\\\\\\\\\ngym ==0.15.4\\\\\\\\\\\\\\\\nh5py ==3.8.0\\\\\\\\\\\\\\\\nidna ==3.10\\\\\\\\\\\\\\\\nimportlib - metadata ==6.7.0\\\\\\\\\\\\\\\\nKeras ==2.3.1\\\\\\\\\\\\\\\\nKeras - Applications ==1.0.8\\\\\\\\\\\\\\\\nKeras - Preprocessing ==1.1.2\\\\\\\\\\\\\\\\nkiwisolver ==1.4.5\\\\\\\\\\\\\\\\nMarkdown ==3.4.4\\\\\\\\\\\\\\\\nMarkupSafe ==2.1.5\\\\\\\\\\\\\\\\nmatplotlib ==3.1.2\\\\\\\\\\\\\\\\nnumpy ==1.18.1\\\\\\\\\\\\\\\\noauthlib ==3.2.2\\\\\\\\\\\\\\\\nopencv - python ==4.11.0.86\\\\\\\\\\\\\\\\nopt - einsum ==3.3.0\\\\\\\\\\\\\\\\nPillow ==9.5.0\\\\\\\\\\\\\\\\nprotobuf ==3.20.3\\\\\\\\\\\\\\\\npyasn1 ==0.5.1\\\\\\\\\\\\\\\\npyasn1 - modules ==0.3.0\\\\\\\\\\\\\\\\npyglet ==1.3.2\\\\\\\\\\\\\\\\npyparsing ==3.1.4\\\\\\\\\\\\\\\\npython - dateutil ==2.9.0. post0\\\\\\\\\\\\\\\\nPyYAML ==6.0.1\\\\\\\\\\\\\\\\nrequests ==2.31.0\\\\\\\\\\\\\\\\nrequests - oauthlib ==2.0.0\\\\\\\\\\\\\\\\nrsa ==4.9\\\\\\\\\\\\\\\\nscipy ==1.4.1\\\\\\\\\\\\\\\\nsix ==1.17.0\\\\\\\\\\\\\\\\ntensorboard ==2.1.1\\\\\\\\\\\\\\\\ntensorflow - estimator ==2.1.0\\\\\\\\\\\\\\\\ntensorflow - gpu ==2.1.0\\\\\\\\\\\\\\\\ntermcolor ==2.3.0\\\\\\\\\\\\\\\\n54\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'A.4. Requirements Kapil\\\\\\\\\\\\\\\\ntorch ==1.2.0+ cu92\\\\\\\\\\\\\\\\ntorchvision ==0.4.0+ cu92\\\\\\\\\\\\\\\\ntyping_extensions ==4.7.1\\\\\\\\\\\\\\\\nurllib3 ==2.0.7\\\\\\\\\\\\\\\\nWerkzeug ==2.2.3\\\\\\\\\\\\\\\\nwrapt ==1.16.0\\\\\\\\\\\\\\\\nzipp ==3.15.0\\\\\\\\\\\\\\\\nA.4. Requirements Kapil\\\\\\\\\\\\\\\\nabsl -py ==2.1.0\\\\\\\\\\\\\\\\nastor ==0.8.1\\\\\\\\\\\\\\\\nbackcall ==0.2.0\\\\\\\\\\\\\\\\ncloudpickle ==1.2.2\\\\\\\\\\\\\\\\ncycler ==0.11.0\\\\\\\\\\\\\\\\ndebugpy ==1.7.0\\\\\\\\\\\\\\\\ndecorator ==5.1.1\\\\\\\\\\\\\\\\nentrypoints ==0.4\\\\\\\\\\\\\\\\nfuture ==1.0.0\\\\\\\\\\\\\\\\ngast ==0.2.2\\\\\\\\\\\\\\\\ngoogle - pasta ==0.2.0\\\\\\\\\\\\\\\\ngrpcio ==1.62.3\\\\\\\\\\\\\\\\ngym ==0.14.0\\\\\\\\\\\\\\\\nh5py ==3.8.0\\\\\\\\\\\\\\\\nimportlib - metadata ==6.7.0\\\\\\\\\\\\\\\\nipykernel ==6.16.2\\\\\\\\\\\\\\\\nipython ==7.34.0\\\\\\\\\\\\\\\\njedi ==0.19.2\\\\\\\\\\\\\\\\njupyter_client ==7.4.9\\\\\\\\\\\\\\\\njupyter_core ==4.12.0\\\\\\\\\\\\\\\\nKeras ==2.3.1\\\\\\\\\\\\\\\\n55\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'A. Appendix\\\\\\\\\\\\\\\\nKeras - Applications ==1.0.8\\\\\\\\\\\\\\\\nKeras - Preprocessing ==1.1.2\\\\\\\\\\\\\\\\nkiwisolver ==1.4.5\\\\\\\\\\\\\\\\nMarkdown ==3.4.4\\\\\\\\\\\\\\\\nMarkupSafe ==2.1.5\\\\\\\\\\\\\\\\nmatplotlib ==2.2.4\\\\\\\\\\\\\\\\nmatplotlib - inline ==0.1.6\\\\\\\\\\\\\\\\nnest - asyncio ==1.6.0\\\\\\\\\\\\\\\\nnumpy ==1.21.6\\\\\\\\\\\\\\\\nopt - einsum ==3.3.0\\\\\\\\\\\\\\\\npackaging ==24.0\\\\\\\\\\\\\\\\nparso ==0.8.4\\\\\\\\\\\\\\\\npexpect ==4.9.0\\\\\\\\\\\\\\\\npickleshare ==0.7.5\\\\\\\\\\\\\\\\nprompt_toolkit ==3.0.48\\\\\\\\\\\\\\\\nprotobuf ==3.20.3\\\\\\\\\\\\\\\\npsutil ==7.0.0\\\\\\\\\\\\\\\\nptyprocess ==0.7.0\\\\\\\\\\\\\\\\npyglet ==1.3.2\\\\\\\\\\\\\\\\nPygments ==2.17.2\\\\\\\\\\\\\\\\npyparsing ==3.1.4\\\\\\\\\\\\\\\\npython - dateutil ==2.9.0. post0\\\\\\\\\\\\\\\\npytz ==2025.1\\\\\\\\\\\\\\\\nPyYAML ==6.0.1\\\\\\\\\\\\\\\\npyzmq ==26.2.1\\\\\\\\\\\\\\\\nscipy ==1.7.3\\\\\\\\\\\\\\\\nsix ==1.17.0\\\\\\\\\\\\\\\\ntensorboard ==1.15.0\\\\\\\\\\\\\\\\ntensorflow ==1.15.0\\\\\\\\\\\\\\\\ntensorflow - estimator ==1.15.1\\\\\\\\\\\\\\\\ntermcolor ==2.3.0\\\\\\\\\\\\\\\\ntornado ==6.2\\\\\\\\\\\\\\\\n56\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'A.5. Requirements Nihal\\\\\\\\\\\\\\\\ntraitlets ==5.9.0\\\\\\\\\\\\\\\\ntyping_extensions ==4.7.1\\\\\\\\\\\\\\\\nwcwidth ==0.2.13\\\\\\\\\\\\\\\\nWerkzeug ==2.2.3\\\\\\\\\\\\\\\\nwrapt ==1.16.0\\\\\\\\\\\\\\\\nzipp ==3.15.0\\\\\\\\\\\\\\\\nA.5. Requirements Nihal\\\\\\\\\\\\\\\\nabsl -py ==2.1.0\\\\\\\\\\\\\\\\nastor ==0.8.1\\\\\\\\\\\\\\\\nbackcall ==0.2.0\\\\\\\\\\\\\\\\ncachetools ==4.2.4\\\\\\\\\\\\\\\\ncertifi ==2025.1.31\\\\\\\\\\\\\\\\ncharset - normalizer ==3.4.1\\\\\\\\\\\\\\\\ncloudpickle ==1.2.2\\\\\\\\\\\\\\\\ncycler ==0.11.0\\\\\\\\\\\\\\\\ndebugpy ==1.7.0\\\\\\\\\\\\\\\\ndecorator ==5.1.1\\\\\\\\\\\\\\\\nentrypoints ==0.4\\\\\\\\\\\\\\\\nfuture ==1.0.0\\\\\\\\\\\\\\\\ngast ==0.2.2\\\\\\\\\\\\\\\\ngoogle - auth ==1.35.0\\\\\\\\\\\\\\\\ngoogle -auth - oauthlib ==0.4.6\\\\\\\\\\\\\\\\ngoogle - pasta ==0.2.0\\\\\\\\\\\\\\\\ngrpcio ==1.62.3\\\\\\\\\\\\\\\\ngym ==0.15.4\\\\\\\\\\\\\\\\nh5py ==3.8.0\\\\\\\\\\\\\\\\nidna ==3.10\\\\\\\\\\\\\\\\nimportlib - metadata ==6.7.0\\\\\\\\\\\\\\\\nipykernel ==6.16.2\\\\\\\\\\\\\\\\n57\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'A. Appendix\\\\\\\\\\\\\\\\nipython ==7.34.0\\\\\\\\\\\\\\\\njedi ==0.19.2\\\\\\\\\\\\\\\\njupyter_client ==7.4.9\\\\\\\\\\\\\\\\njupyter_core ==4.12.0\\\\\\\\\\\\\\\\nKeras - Applications ==1.0.8\\\\\\\\\\\\\\\\nKeras - Preprocessing ==1.1.2\\\\\\\\\\\\\\\\nkiwisolver ==1.4.5\\\\\\\\\\\\\\\\nMarkdown ==3.4.4\\\\\\\\\\\\\\\\nMarkupSafe ==2.1.5\\\\\\\\\\\\\\\\nmatplotlib ==3.1.2\\\\\\\\\\\\\\\\nmatplotlib - inline ==0.1.6\\\\\\\\\\\\\\\\nnest - asyncio ==1.6.0\\\\\\\\\\\\\\\\nnumpy ==1.21.6\\\\\\\\\\\\\\\\noauthlib ==3.2.2\\\\\\\\\\\\\\\\nopencv - python ==4.11.0.86\\\\\\\\\\\\\\\\nopt - einsum ==3.3.0\\\\\\\\\\\\\\\\npackaging ==24.0\\\\\\\\\\\\\\\\nparso ==0.8.4\\\\\\\\\\\\\\\\npexpect ==4.9.0\\\\\\\\\\\\\\\\npickleshare ==0.7.5\\\\\\\\\\\\\\\\nprompt_toolkit ==3.0.48\\\\\\\\\\\\\\\\nprotobuf ==3.20.3\\\\\\\\\\\\\\\\npsutil ==7.0.0\\\\\\\\\\\\\\\\nptyprocess ==0.7.0\\\\\\\\\\\\\\\\npyasn1 ==0.5.1\\\\\\\\\\\\\\\\npyasn1 - modules ==0.3.0\\\\\\\\\\\\\\\\npyglet ==1.3.2\\\\\\\\\\\\\\\\nPygments ==2.17.2\\\\\\\\\\\\\\\\npyparsing ==3.1.4\\\\\\\\\\\\\\\\npython - dateutil ==2.9.0. post0\\\\\\\\\\\\\\\\npyzmq ==26.2.1\\\\\\\\\\\\\\\\nrequests ==2.31.0\\\\\\\\\\\\\\\\n58\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'A.6. Requirements Khordoo\\\\\\\\\\\\\\\\nrequests - oauthlib ==2.0.0\\\\\\\\\\\\\\\\nrsa ==4.9\\\\\\\\\\\\\\\\nscipy ==1.4.1\\\\\\\\\\\\\\\\nsix ==1.17.0\\\\\\\\\\\\\\\\ntensorboard ==2.1.1\\\\\\\\\\\\\\\\ntensorflow - estimator ==2.1.0\\\\\\\\\\\\\\\\ntensorflow - gpu ==2.1.0\\\\\\\\\\\\\\\\ntermcolor ==2.3.0\\\\\\\\\\\\\\\\ntornado ==6.2\\\\\\\\\\\\\\\\ntraitlets ==5.9.0\\\\\\\\\\\\\\\\ntyping_extensions ==4.7.1\\\\\\\\\\\\\\\\nurllib3 ==2.0.7\\\\\\\\\\\\\\\\nwcwidth ==0.2.13\\\\\\\\\\\\\\\\nWerkzeug ==2.2.3\\\\\\\\\\\\\\\\nwrapt ==1.16.0\\\\\\\\\\\\\\\\nzipp ==3.15.0\\\\\\\\\\\\\\\\nA.6. Requirements Khordoo\\\\\\\\\\\\\\\\nbox2d -py ==2.3.8\\\\\\\\\\\\\\\\ncloudpickle ==1.2.2\\\\\\\\\\\\\\\\nfuture ==1.0.0\\\\\\\\\\\\\\\\ngym ==0.15.4\\\\\\\\\\\\\\\\nnumpy ==1.18.1\\\\\\\\\\\\\\\\nopencv - python ==4.11.0.86\\\\\\\\\\\\\\\\nPillow ==9.5.0\\\\\\\\\\\\\\\\npyglet ==1.3.2\\\\\\\\\\\\\\\\nscipy ==1.7.3\\\\\\\\\\\\\\\\nsix ==1.17.0\\\\\\\\\\\\\\\\ntorch ==1.2.0+ cu92\\\\\\\\\\\\\\\\ntorchvision ==0.4.0+ cu92\\\\\\\\\\\\\\\\n59\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'A. Appendix\\\\\\\\\\\\\\\\nA.7. Requirements Sanket\\\\\\\\\\\\\\\\nbox2d -py ==2.3.4\\\\\\\\\\\\\\\\ncloudpickle ==1.2.2\\\\\\\\\\\\\\\\nfuture ==1.0.0\\\\\\\\\\\\\\\\ngym ==0.15.4\\\\\\\\\\\\\\\\nnumpy ==1.21.6\\\\\\\\\\\\\\\\nopencv - python ==4.11.0.86\\\\\\\\\\\\\\\\nPillow ==9.5.0\\\\\\\\\\\\\\\\npyglet ==1.3.2\\\\\\\\\\\\\\\\nscipy ==1.7.3\\\\\\\\\\\\\\\\nsix ==1.17.0\\\\\\\\\\\\\\\\ntorch ==1.2.0+ cu92\\\\\\\\\\\\\\\\ntorchvision ==0.4.0+ cu92\\\\\\\\\\\\\\\\nA.8. Requirements Rokenes\\\\\\\\\\\\\\\\nabsl -py ==2.1.0\\\\\\\\\\\\\\\\nastor ==0.8.1\\\\\\\\\\\\\\\\nbox2d -py ==2.3.4\\\\\\\\\\\\\\\\ncloudpickle ==1.2.2\\\\\\\\\\\\\\\\nfuture ==1.0.0\\\\\\\\\\\\\\\\ngast ==0.6.0\\\\\\\\\\\\\\\\ngoogle - pasta ==0.2.0\\\\\\\\\\\\\\\\ngrpcio ==1.62.3\\\\\\\\\\\\\\\\ngym ==0.15.4\\\\\\\\\\\\\\\\nh5py ==3.8.0\\\\\\\\\\\\\\\\nimportlib - metadata ==6.7.0\\\\\\\\\\\\\\\\nKeras - Applications ==1.0.8\\\\\\\\\\\\\\\\nKeras - Preprocessing ==1.1.2\\\\\\\\\\\\\\\\nMarkdown ==3.4.4\\\\\\\\\\\\\\\\nMarkupSafe ==2.1.5\\\\\\\\\\\\\\\\n60\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nA.8. Requirements Rokenes\\\\\\\\\\\\\\\\nnumpy ==1.21.6\\\\\\\\\\\\\\\\nopencv - python ==4.11.0.86\\\\\\\\\\\\\\\\nprotobuf ==3.20.3\\\\\\\\\\\\\\\\npyglet ==1.3.2\\\\\\\\\\\\\\\\nscipy ==1.7.3\\\\\\\\\\\\\\\\nsix ==1.17.0\\\\\\\\\\\\\\\\ntensorboard ==1.14.0\\\\\\\\\\\\\\\\ntensorflow ==1.14.0\\\\\\\\\\\\\\\\ntensorflow - estimator ==1.14.0\\\\\\\\\\\\\\\\ntermcolor ==2.3.0\\\\\\\\\\\\\\\\ntyping_extensions ==4.7.1\\\\\\\\\\\\\\\\nWerkzeug ==2.2.3\\\\\\\\\\\\\\\\nwrapt ==1.16.0\\\\\\\\\\\\\\\\nzipp ==3.15.0\\\\\\\\\\\\\\\\n61\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'63\\\\\\\\\\\\\\\\nBibliography\\\\\\\\\\\\\\\\nThe references are sorted alphabetically by first author.\\\\\\\\\\\\\\\\n[1] Lionel C. Briand Fellow IEEE Mojtaba Bagherzadeh Amirhossein Zolfagharian, Manel Ab-\\\\\\\\\\\\\\\\ndellatif and Ramesh S.A Search-Based Testing Approach for Deep Reinforcement Learning\\\\\\\\\\\\\\\\nAgents. IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, vol. 49, no. 7 edition,\\\\\\\\\\\\\\\\n2023. ISBN 0-201-37921-X.\\\\\\\\\\\\\\\\n[2] Matteo Biagiola and Paolo Tonella.Testing the Plasticity of Reinforcement Learning Based\\\\\\\\\\\\\\\\nSystems. Università della Svizzera italiana, Switzerland, revised edition, 2022. ISBN 0-201-\\\\\\\\\\\\\\\\n37921-X.\\\\\\\\\\\\\\\\n[3] Matteo Biagiola and Paolo Tonella.Testing of Deep Reinforcement Learning Agents with\\\\\\\\\\\\\\\\nSurrogate Models. Università della Svizzera italiana, Switzerland, revised edition, 2023.\\\\\\\\\\\\\\\\nISBN 0-201-37921-X.\\\\\\\\\\\\\\\\n[4] Kapil Chauhan. CartPole_DQN: CartPole_v0 Jupyter Notebook. https://github.\\\\\\\\\\\\\\\\ncom/kapilnchauhan77/CartPole_DQN/blob/master/CartPole_v0.ipynb, 2019. Accessed:\\\\\\\\\\\\\\\\nMay 15, 2025.\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'[5] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.Deep Learning. MIT Press, 2016.\\\\\\\\\\\\\\\\nhttp://www.deeplearningbook.org.\\\\\\\\\\\\\\\\n[6] IBM. What is natural language processing? https://www.ibm.com/think/topics/\\\\\\\\\\\\\\\\nnatural-language-processing, n.d. Accessed: May 23, 2025.\\\\\\\\\\\\\\\\n[7] Mahmood Khordoo. Deep-Reinforcement-Learning-with-PyTorch: n-\\\\\\\\\\\\\\\\nstep DQN for LunarLander-v2. https://github.com/khordoo/\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'Bibliography\\\\\\\\\\\\\\\\nDeep-Reinforcement-Learning-with-PyTorch/blob/example/examples/DQN/\\\\\\\\\\\\\\\\nlunarlander_v2-dqn-n-step.py , 2020. Accessed: May 15,2025.\\\\\\\\\\\\\\\\n[8] Shakti Kumar. adaptiveSystems: RL_Benchmarks README. https://github.\\\\\\\\\\\\\\\\ncom/shaktikshri/adaptiveSystems/blob/master/RL_Benchmarks/README.md, 2019. Ac-\\\\\\\\\\\\\\\\ncessed: May 15, 2025.\\\\\\\\\\\\\\\\n[9] Emmanouil D. Oikonomou, Petros Karvelis, Nikolaos Giannakeas, Aristidis Vrachatis, Evri-\\\\\\\\\\\\\\\\npidis Glavas, and Alexandros T. Tzallas. How natural language processing derived tech-\\\\\\\\\\\\\\\\nniques are used on biological data: a systematic review. Network Modeling Analysis in\\\\\\\\\\\\\\\\nHealth Informatics and Bioinformatics, 13(23), 2024. DOI 10.1007/s13721-024-00458-1.\\\\\\\\\\\\\\\\n[10] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\\\\\\\\\\\\\\\nunderstanding by generative pre-training. https://cdn.openai.com/research-covers/\\\\\\\\\\\\\\\\nlanguage-unsupervised/language_understanding_paper.pdf, 2018. OpenAI Report.\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'[11] Nihal T. Rao. RL-Double-DQN: Double DQN Implementation for CartPole-v0. https:\\\\\\\\\\\\\\\\n//github.com/nihal-rao/RL-Double-DQN , 2020. Accessed: May 15, 2025.\\\\\\\\\\\\\\\\n[12] SigveRokenes. learning-rl/gym/lunarlander-v2: DQNExampleforLunarLander-v2. https:\\\\\\\\\\\\\\\\n//github.com/evgiz/learning-rl/tree/master/gym/lunarlander-v2, 2019. Accessed:\\\\\\\\\\\\\\\\nMay 15, 2025.\\\\\\\\\\\\\\\\n[13] Richard S. Sutton and Andrew G. Barto.Reinforcement Learning: An Introduction. The\\\\\\\\\\\\\\\\nMIT Press Cambridge, Massachusetts,London, England, revised edition, 2015. ISBN 0-201-\\\\\\\\\\\\\\\\n37921-X.\\\\\\\\\\\\\\\\n[14] Sanket Thakur. LunarLander_DQN: DQN Implementation for LunarLander-\\\\\\\\\\\\\\\\nv2. https://github.com/sanketsans/openAIenv/blob/master/DQN/LunarLander/\\\\\\\\\\\\\\\\nLunarLander_DQN.ipynb, 2020. Accessed: May 15, 2025.\\\\\\\\\\\\\\\\n[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\\\\\\\\\\\\\\\\nGomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In\\\\\\\\\\\\\\\\nAdvances in Neural Information Processing Systems , volume 30. Curran Asso-\\\\\\\\\\\\\\\\n64\\\\\\'\"\\'',\n",
       "   '\\'\"\\\\\\'Bibliography\\\\\\\\\\\\\\\\nciates, Inc., 2017. URL https://papers.nips.cc/paper_files/paper/2017/file/\\\\\\\\\\\\\\\\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\\\\\\\\\\\\\\\n65\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nBibliography\\\\\\\\\\\\\\\\n66\\\\\\'\"\\''],\n",
       "  'description': 'tokens with recursive tokenization, without cleaning or another technique'}}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_token_file = os.path.join(TOKENS_DIRECTORY,\"tokens1.json\")\n",
    "tokens_file= open(path_token_file,\"r\",encoding=\"utf-8\")\n",
    "tokens_json = json.load(tokens_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "542d5261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\l'\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_5916\\2082917411.py:1: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  path_pdf_thesis = os.path.join(DATA_DIRECTORY,\"investigations\\landing\\Constructing a language for testing RL.pdf\")\n"
     ]
    }
   ],
   "source": [
    "path_pdf_thesis = os.path.join(DATA_DIRECTORY,\"investigations\\landing\\Constructing a language for testing RL.pdf\")\n",
    "reader_thesis = PdfReader(path_pdf_thesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada467ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_1(reader_pdf: PdfReader,token_json:dict,description:str) -> dict:\n",
    "    pdf_texts = [p.extract_text().strip() for p in reader_pdf.pages]\n",
    "    character_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators = [\"\\n\\n\",\"\\n\",\". \", \" \", \"\"],\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap=0\n",
    "    )\n",
    "\n",
    "    character_split_texts = character_splitter.split_text(\n",
    "        \"\\n\\n\".join(pdf_texts)\n",
    "    )\n",
    "\n",
    "    token_json[\"first_tokenization\"] = { \n",
    "        \"values\" : character_split_texts, \n",
    "        \"description\":description\n",
    "    }\n",
    "\n",
    "\n",
    "    return token_json\n",
    "\n",
    "\n",
    "tokens = tokenizer_1(\n",
    "    reader_pdf=reader_thesis,\n",
    "    token_json=tokens_json,\n",
    "    description=\"tokens with recursive tokenization, without cleaning or another technique\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c67c3f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROL_CHARS = ''.join(map(chr, list(range(0,32)) + [127]))\n",
    "CONTROL_RE = re.compile(f\"[{re.escape(CONTROL_CHARS.replace('\\\\n','').replace('\\\\t',''))}]\")\n",
    "\n",
    "def strip_control_chars(text:str) -> str:\n",
    "    return CONTROL_RE.sub(\"\", text)\n",
    "\n",
    "tokens['first_tokenization']['values'] = [strip_control_chars(text)  for text in tokens['first_tokenization']['values']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "99830930",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(path_token_file,'w',encoding=\"utf-8\")\n",
    "json.dump(tokens,file,indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e58874d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\'\"\\\\\\'Maestría en Ingeniería \\\\\\\\\\\\\\\\nde Sistemas \\\\\\\\\\\\\\\\ny Computación\\\\\\\\\\\\\\\\nDissertation\\\\\\\\\\\\\\\\nConstructing a language for testing\\\\\\\\\\\\\\\\nReinforcement Learning programs using\\\\\\\\\\\\\\\\nNLP techniques\\\\\\\\\\\\\\\\nLUIS ALEJANDRO MEDINA\\\\\\\\\\\\\\\\nJuly 19, 2025\\\\\\\\\\\\\\\\nThis thesis is submitted in partial fulfillment of the requirements\\\\\\\\\\\\\\\\nfor a degree of Master in Systems and Computing Engineering\\\\\\\\\\\\\\\\n(MISIS).\\\\\\\\\\\\\\\\nThesis Committee:\\\\\\\\\\\\\\\\nProf. Nicolás Cardozo (Promotor) Universidad de los Andes, Colombia\\\\\\\\\\\\\\\\nProf. Reviewer 1Ivana Dusparic Trinity College Dublin, Ireland\\\\\\\\\\\\\\\\nProf. Reviewer 2Ruben Manrique Universidad de los Andes, Colombia\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nConstructing a language for testing Reinforcement Learning pro-\\\\\\\\\\\\\\\\ngrams using NLP techniques\\\\\\\\\\\\\\\\n© YEAR LUIS ALEJANDRO MEDINA\\\\\\\\\\\\\\\\nSystems and Computing Engineering Department\\\\\\\\\\\\\\\\nFLAG lab\\\\\\\\\\\\\\\\nFaculty of Engineering\\\\\\\\\\\\\\\\nUniversidad de los Andes\\\\\\\\\\\\\\\\nBogotá, Colombia\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'With all my heart: To my dear dad, Luis Hernando Medina,\\\\\\\\\\\\\\\\neverything I have achieved is thanks to you. To all my family,\\\\\\\\\\\\\\\\nthank you for never abandoning me. And to my beloved Martín, I\\\\\\\\\\\\\\\\nwill always love you.\\\\\\\\\\\\\\\\n... And I will always be two steps behind you. “Two Steps Behind”, Def Leppard.\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'iv\\\\\\\\\\\\\\\\nAbstract\\\\\\\\\\\\\\\\nReinforcementLearning(RL)hasgarneredsignificantattentionfromtheresearchcommunitydue\\\\\\\\\\\\\\\\nto its expanding applications across various fields, including recommendation systems, robotics,\\\\\\\\\\\\\\\\nautonomous vehicles, and more. Consequently, there has been a growing interest in testing Deep\\\\\\\\\\\\\\\\nReinforcement Learning (DRL) agents and identifying potential faults that could lead to critical\\\\\\\\\\\\\\\\nfailures during their execution.\\\\\\\\\\\\\\\\nVarious techniques exist for testing systems, including white-box, black-box, and data-quality\\\\\\\\\\\\\\\\ntesting methods. White-box techniques rely on the assumption that complete information about\\\\\\\\\\\\\\\\nthe system (e.g., code, data, and architecture) is available. Black-box techniques, on the other\\\\\\\\\\\\\\\\nhand, operate under the premise that no internal details of the system are accessible, focusing\\\\\\\\\\\\\\\\ninstead on assessing the system’s performance and correctness based solely on specified require-\\\\\\\\\\\\\\\\nments. Due to the potentially vast amount of data generated by these systems, data-quality\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'techniques leverage this information to infer system behavior and detect possible issues.\\\\\\\\\\\\\\\\nIn (RL), various methods exist for the testing and evaluation of agents. Adversarial attacks,\\\\\\\\\\\\\\\\nfocusing on perturbing the raw input of the RL agent, Mutation testing and debuggers focus\\\\\\\\\\\\\\\\non assessing the code, statistical methods for building performance metrics and Search-based\\\\\\\\\\\\\\\\ntesting focusing on the data produced by the agent as genetics algorithms, Plasticity maps, and\\\\\\\\\\\\\\\\nConstructing Language and Natural Language Process (NLP) techniques.\\\\\\\\\\\\\\\\nConstructing language models using NLP techniques for testing in Reinforcement Learning is\\\\\\\\\\\\\\\\nbased on the idea of communication between the agent and the environment, thereby establishing\\\\\\\\\\\\\\\\na shared language. An inference language is key to finding ways to detect possible agent failures,\\\\\\\\\\\\\\\\nincorrect behaviors, and low-reward outcomes.\\\\\\\\\\\\\\\\nOur evaluation uses different implementations from two RL application benchmarks—CartPole\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'and LunarLander—to generate testing scenarios for each. We analyze two key metrics: the\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'average rewardand theprobability of fault. This is important because agents can achieve\\\\\\\\\\\\\\\\nhigh average rewards that may overcompensate for cases with lower performance. However, our\\\\\\\\\\\\\\\\nresults suggest that the proposed technique is capable of identifyingregions of lower reward.\\\\\\\\\\\\\\\\nFor example, in the case of theShakti agent, the average reward during normal execution\\\\\\\\\\\\\\\\nis 185.3, whereas in our testing scenarios, it drops to 96.98. This indicates that our test case\\\\\\\\\\\\\\\\ngenerator can uncover regions of reduced performance.\\\\\\\\\\\\\\\\nOn the other hand, even for theKapil agent, which consistently shows a perfect success rate\\\\\\\\\\\\\\\\nduring normal execution, we were able to identify testing regions where the success rate drops\\\\\\\\\\\\\\\\nsignificantly—to 42.8%.\\\\\\\\\\\\\\\\nThe results indicate that the proposed test case generator was able to identify failure-prone\\\\\\\\\\\\\\\\nregions and lower rewards regions in multiple agents— even in those with perfect performance\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'under normal conditions. Statistical analysis confirmed that, for most agents, the generated test\\\\\\\\\\\\\\\\ncases led to significantly lower success rates or reduced average rewards compared to standard\\\\\\\\\\\\\\\\nexecution.\\\\\\\\\\\\\\\\nv\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'vi\\\\\\\\\\\\\\\\nContents\\\\\\\\\\\\\\\\nAbstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv\\\\\\\\\\\\\\\\nList of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii\\\\\\\\\\\\\\\\n1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\\\\\\\\\\\\\\\n2. Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\\\\\\\\\\\\\\\n2.1. Reinforment Learning, Makov process desicion (MDP) . . . . . . . . . . . . . . . 5\\\\\\\\\\\\\\\\n2.2. Natural Language Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\\\\\\\\\\\\\\\n2.2.1. Early Neural Network-Based Models . . . . . . . . . . . . . . . . . . . . . 6\\\\\\\\\\\\\\\\n2.3. Abstract Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\\\\\\\\\\\\\\\n2.3.1. Definition π∗-irrelevance abstraction . . . . . . . . . . . . . . . . . . . . . 10\\\\\\\\\\\\\\\\n2.3.2. Definition Q∗-irrelevance abstraction . . . . . . . . . . . . . . . . . . . . . 10\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'2.3.3. Relax Q∗-irrelevance abstraction . . . . . . . . . . . . . . . . . . . . . . . 10\\\\\\\\\\\\\\\\n3. MarTest: An NLP Approach for test sequence generation . . . . . . . . . . . . . . . 11\\\\\\\\\\\\\\\\n3.1. Vocabulary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\\\\\\\\\\\\\\\n3.2. Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\\\\\\\\\\\\\\\n3.3. Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\\\\\\\\\\\\\\\n3.3.1. Average reward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\\\\\\\\\\\\\\\n3.3.2. Failure probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\\\\\\\\\\\\\\\n3.3.3. Random Forest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\\\\\\\\\\\\\\\n3.4. Model-Conditioned Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\\\\\\\\\\\\\\\n3.4.1. Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'4. MarTest Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\\\\\\\\\\\\\\\n4.1. Pipeline Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\\\\\\\\\\\\\\\n4.2. Module 1: Data acquisition and preparation . . . . . . . . . . . . . . . . . . . . . 16\\\\\\\\\\\\\\\\n4.2.1. Inputs training and execution logs . . . . . . . . . . . . . . . . . . . . . . 16\\\\\\\\\\\\\\\\n4.2.2. Data Balacing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\\\\\\\\\\\\\\\n4.2.3. Q-Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\\\\\\\\\\\\\\\n4.3. Module 2: Abstract Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\\\\\\\\\\\\\\\n4.3.1. Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\\\\\\\\\\\\\\\n4.4. Module 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\\\\\\\\\\\\\\\n4.4.1. Abstract Episodes generation . . . . . . . . . . . . . . . . . . . . . . . . . 17\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'4.5. Module 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\\\\\\\\\\\\\\\n4.5.1. Random Forest Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\\\\\\\\\\\\\\\n4.6. Module 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\\\\\\\\\\\\\\\n4.6.1. Conditioned Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\\\\\\\\\\\\\\\n4.6.2. Test Regions Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\\\\\\\\\\\\\\\n5. Experimental Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\\\\\\\\\\\\\\\n5.1. CartPole-v0 Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'CONTENTS\\\\\\\\\\\\\\\\n5.2. LunarLander-v2 Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\\\\\\\\\\\\\\\n5.2.1. Sanket Thakur (sanket) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\\\\\\\\\\\\\\\n5.2.2. Mahmood Khordoo (khordoo) . . . . . . . . . . . . . . . . . . . . . . . . 24\\\\\\\\\\\\\\\\n5.2.3. Sigve Rokenes (rokenes) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\\\\\\\\\\\\\\\n5.3. Conditioned transformer configurations and metrics . . . . . . . . . . . . . . . . 25\\\\\\\\\\\\\\\\n5.3.1. Conditional transformer parameters for cartpole . . . . . . . . . . . . . . 26\\\\\\\\\\\\\\\\n5.3.2. Conditional transformer parameters for Lunar-Lander . . . . . . . . . . . 26\\\\\\\\\\\\\\\\n5.3.3. Training and validations loss . . . . . . . . . . . . . . . . . . . . . . . . . 26\\\\\\\\\\\\\\\\n5.4. Concrete Test Case Generator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\\\\\\\\\\\\\\\n5.5. An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5.6. Evaluation and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\\\\\\\\\\\\\\\n5.6.1. Cartpole . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\\\\\\\\\\\\\\\n5.6.2. Lunar lander . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\\\\\\\\\\\\\\\n5.6.3. A Small Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\\\\\\\\\\\\\\\n5.6.4. Threats to Validity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\\\\\\\\\\\\\\\n6. Conclusion, Limitations and Future Work . . . . . . . . . . . . . . . . . . . . . . . . 43\\\\\\\\\\\\\\\\n6.1. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\\\\\\\\\\\\\\\n6.2. Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\\\\\\\\\\\\\\\n6.3. Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\\\\\\\\\\\\\\\nA. Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'A.1. MartTest Pipeline Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\\\\\\\\\\\\\\\nA.2. Conditioned Transformer Model CondGPT2 . . . . . . . . . . . . . . . . . . . . . 52\\\\\\\\\\\\\\\\nA.3. Requirements Shakti . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\\\\\\\\\\\\\\\nA.4. Requirements Kapil . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\\\\\\\\\\\\\\\\nA.5. Requirements Nihal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\\\\\\\\\\\\\\\nA.6. Requirements Khordoo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\\\\\\\\\\\\\\\nA.7. Requirements Sanket . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\\\\\\\\\\\\\\\nA.8. Requirements Rokenes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\\\\\\\\\\\\\\\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\\\\\\\\\\\\\\\nvii\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'viii\\\\\\\\\\\\\\\\nList of Figures\\\\\\\\\\\\\\\\n2.1. The transformer architecture, Attention Is All You Need, 2017. . . . . . . . . . . 8\\\\\\\\\\\\\\\\n2.2. Transformer achitecture, Improving Language Understanding by Generative Pre-\\\\\\\\\\\\\\\\nTraining. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\\\\\\\\\\\\\\\n2.3. ConditionedTransformer, rewardˆrandfailureprobability Pf arefirstprocessedby\\\\\\\\\\\\\\\\nan auxiliary neural network, then added element-wise to the word-token embeddings. 10\\\\\\\\\\\\\\\\n4.1. Martest pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\\\\\\\\\\\\\\\n5.1. CartPole-v0. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\\\\\\\\\\\\\\\n5.2. LunarLander-v2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\\\\\\\\\\\\\\\n5.3. Training and Evaluation Loss per Epoch, Shakti . . . . . . . . . . . . . . . . . . 27\\\\\\\\\\\\\\\\n5.4. Training and Evaluation Loss per Epoch, Kapil . . . . . . . . . . . . . . . . . . . 27\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5.5. Training and Evaluation Loss per Epoch, Nihal . . . . . . . . . . . . . . . . . . . 28\\\\\\\\\\\\\\\\n5.6. Training and Evaluation Loss per Epoch, Sanket . . . . . . . . . . . . . . . . . . 28\\\\\\\\\\\\\\\\n5.7. Training and Evaluation Loss per Epoch, Khordoo . . . . . . . . . . . . . . . . . 29\\\\\\\\\\\\\\\\n5.8. Training and Evaluation Loss per Epoch, Rokenes . . . . . . . . . . . . . . . . . . 29\\\\\\\\\\\\\\\\n5.9. Initial state of the CartPole environment with cart position−0.73915684, cart\\\\\\\\\\\\\\\\nvelocity −0.89601650, pole angle−0.03282847, and pole angular velocity0.23753740. 37\\\\\\\\\\\\\\\\n5.10.Final state of the CartPole environment with cart position−2.41205427, cart ve-\\\\\\\\\\\\\\\\nlocity −1.42968531, pole angle−0.06067818, and pole angular velocity−0.02501499. 38\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'1\\\\\\\\\\\\\\\\nChapter 1\\\\\\\\\\\\\\\\nIntroduction\\\\\\\\\\\\\\\\nReinforment Learning (RL) and Natural Language Processing (NLP) have seen increasing inter-\\\\\\\\\\\\\\\\nest and research advances in recent years. The combination of deep learning and RL, referred to\\\\\\\\\\\\\\\\nas Deep Reinforcement Learning (DRL), has achieved success to solve various decision-making\\\\\\\\\\\\\\\\nproblems such as autonomous driving and robotics, while NLP has experienced a notable suc-\\\\\\\\\\\\\\\\ncess with the introduction of transformer architecture, particularly the decoder-only transformer\\\\\\\\\\\\\\\\nmodel and other architectures, such as Recurrent Neural Networks (RNNs) and Long Short-Term\\\\\\\\\\\\\\\\nMemory (LSTMs) with their primary distinction lying in their capacity to handle memory with\\\\\\\\\\\\\\\\nlong sequences and parallel processing.\\\\\\\\\\\\\\\\nDRL has recently been applied in many practical contexts. For instance, Netflix uses it to\\\\\\\\\\\\\\\\nrecommend which movie to show to a user in order to maximize engagement (System of recom-\\\\\\\\\\\\\\\\nmendation) [3], Microsoft developed Personalizer, a service developer that can be use for content\\\\\\'\"\\'',\n",
       " '\\'\\\\\\'\\\\\\\\\\\\\\'recomendation [3]. Other applications include the mastery of strategy games such as Dota 2 and\\\\\\\\\\\\\\\\nStarCraft 2 in which it has defeated human players [2].\\\\\\\\\\\\\\\\nWith the introduction of the paper \"Attention is All you need\" Vaswani et al. [15] published\\\\\\\\\\\\\\\\nin 2017 where they introduce the transformer model laying down the foundation for modern\\\\\\\\\\\\\\\\nNLP and sequence modeling, such techniques have influenced the development of architectures\\\\\\\\\\\\\\\\nlike BERT, GPT, and other state-of-the-art NLP models, gaining huge popularity in all fields.\\\\\\\\\\\\\\\\nThe utility and use of NLP-based tools has been a great advantage for society and probably a\\\\\\\\\\\\\\\\nprincipal engine for progress.\\\\\\\\\\\\\\\\nFor the constantly growing use of RL models in many fields, there has been a growing need\\\\\\\\\\\\\\'\\\\\\'\\'',\n",
       " '\\'\"\\\\\\'1. Introduction\\\\\\\\\\\\\\\\nof testing such models. Therefore, we have built a novel way for testing RL agents by bridging\\\\\\\\\\\\\\\\ntwo fields. To reconcile both fields, we need to reconcile the infinite state spaces in the case of\\\\\\\\\\\\\\\\n(continuous) RL agents, and the finite vocabulary/alphabet used in NLP models.\\\\\\\\\\\\\\\\nWe put forward a language as a means to reconcile RL and NLP. Through the execution of an\\\\\\\\\\\\\\\\nagent, this is in continuous interaction with the environment establishing a language the proposed\\\\\\\\\\\\\\\\napproach build such language is based on the mathematical concept of abstraction, often called\\\\\\\\\\\\\\\\n’equivalences classes’ or ’abstracts classes’. The vocabulary for testing an RL program consist of\\\\\\\\\\\\\\\\nabstract classes inputs for NLP models, wich then can produce scenarios (i.e., paths of abstract\\\\\\\\\\\\\\\\nstates - abstract paths) with possible fails or lower rewards.\\\\\\\\\\\\\\\\nNLP models model/generate abstracts paths, that is, sequences of state representative ab-\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'stracts classes and the actions. Afterwards we validate that the generated abstract paths indeed\\\\\\\\\\\\\\\\nare possible and lead to a diminished agent performance. To do this, we developed a test gen-\\\\\\\\\\\\\\\\nerator cases program extracts regions from abstract paths and disrupt representative states in\\\\\\\\\\\\\\\\neach class to observe the agent’s behavior.\\\\\\\\\\\\\\\\nWe evaluate our approach using agents selected from the GitHub OpenAI Gym leaderboard.\\\\\\\\\\\\\\\\nFor each agent, we first generate logs from both training and execution, which serve as inputs to\\\\\\\\\\\\\\\\nour implementation,MarTest-Pipeline. Based on the forecasted regions produced by the pipeline,\\\\\\\\\\\\\\\\nwe then construct concrete test cases to evaluate the agent’s performance.\\\\\\\\\\\\\\\\nOur results show that the implementation is able to identify regions associated with lower\\\\\\\\\\\\\\\\naverage rewards and reduced success rates. Moreover, we demonstrate that the performance on\\\\\\\\\\\\\\\\nthe testing samples is statistically significantly lower than that observed under normal execution,\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'highlighting that high performance of an agent trained in a normal execution does not guarantee\\\\\\\\\\\\\\\\nrobustness under targeted testing conditions.\\\\\\\\\\\\\\\\nThesis Outline\\\\\\\\\\\\\\\\nThe remainder of this thesis is organized as follows:\\\\\\\\\\\\\\\\n• Chapter 2 – Background:Introduces the theoretical foundations relevant to this work,\\\\\\\\\\\\\\\\nincluding reinforcement learning and Markov decision processes (MDPs), natural language\\\\\\\\\\\\\\\\nprocessing models, and various forms of abstraction used to represent agent-environment\\\\\\\\\\\\\\\\n2\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'interactions symbolically.\\\\\\\\\\\\\\\\n• Chapter 3 – Approach:Describes the conceptual framework of the proposed method,\\\\\\\\\\\\\\\\nincluding the symbolic vocabulary, abstract corpus construction, and the output model\\\\\\\\\\\\\\\\nbased on a conditioned transformer. Assumptions and design decisions are also discussed.\\\\\\\\\\\\\\\\n• Chapter 4 – Implementation:Presents the detailed structure of theMarTest-Pipeline,\\\\\\\\\\\\\\\\nexplaining each module in the pipeline, from the processing of logs and Q-values to the\\\\\\\\\\\\\\\\ngeneration of abstract episodes, classification models, and the conditioned transformer used\\\\\\\\\\\\\\\\nto produce test case regions.\\\\\\\\\\\\\\\\n• Chapter 5 – Experimental Design: Details the experimental setup, including the\\\\\\\\\\\\\\\\nselected agents from theCartPole and LunarLander environments. It also describes how\\\\\\\\\\\\\\\\nconcrete test cases are generated from the abstract representations and evaluates the agents\\\\\\\\\\\\\\\\nusing statistical analysis. The chapter concludes with a discussion on threats to validity.\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'• Chapter 6 – Limitations and Future Work:Discusses the current limitations of the\\\\\\\\\\\\\\\\napproach and outlines possible future directions, including the refinement of abstraction\\\\\\\\\\\\\\\\nlevels, integrationoflearning-basedrepresentations, anddeeperunificationofreinforcement\\\\\\\\\\\\\\\\nlearning and natural language processing techniques.\\\\\\\\\\\\\\\\n3\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'1. Introduction\\\\\\\\\\\\\\\\n4\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5\\\\\\\\\\\\\\\\nChapter 2\\\\\\\\\\\\\\\\nBackground\\\\\\\\\\\\\\\\nTo motivate the problem and to provide an overview of our approach we give the definitions of\\\\\\\\\\\\\\\\nthe concepts required to understand this work, regarding RL, NLP and Abstract classes.\\\\\\\\\\\\\\\\n2.1. Reinforment Learning, Makov process desicion (MDP)\\\\\\\\\\\\\\\\nReinforcement Learning trains an agent that interacts with an environmento maximize the ob-\\\\\\\\\\\\\\\\nserved reward obtained from the environment after action execution, RL uses a trial and error\\\\\\\\\\\\\\\\nstrategy to explore the environment. At every state, the agent chooses an action to execute from\\\\\\\\\\\\\\\\nthe set of all possible actions for the state, and receive a corresponding reward for the executed\\\\\\\\\\\\\\\\naction.\\\\\\\\\\\\\\\\nA Markov Decision Process (MDP) is defined as a 4-tuple< S,A,T,R > where S is the set\\\\\\\\\\\\\\\\nof continuous non-enumerable states (|S|≥| N|) ,A is the set of (continuous) actions,T is the\\\\\\\\\\\\\\\\ntransition function whereT : SXAXS →[0,1] such thatT(s,a,s ′) determines the probability\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'of reaching a states′ by performing an actiona in state s, R : SXA →R a reward function\\\\\\\\\\\\\\\\nthat determines a reward for a pair of an action and a state. At each time step, the agent uses\\\\\\\\\\\\\\\\na mapping from state to probabilities of selecting each possible action. This mapping is called\\\\\\\\\\\\\\\\nthe agent’s policyπ: SXA →[0,1] considered the solution of the MDP.\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'2. Background\\\\\\\\\\\\\\\\nMarkov Property\\\\\\\\\\\\\\\\nMarkov Propertyis defined as the environment’s response att+ 1 depends only on the state and\\\\\\\\\\\\\\\\naction representations att:\\\\\\\\\\\\\\\\nPr{Rt+1 = r,St+1 = s′|S0,A0,R1,....Rt,St,At}= Pr{Rt+1 = r,St+1|St,At}\\\\\\\\\\\\\\\\nfor allr,s′,St,At. [13].\\\\\\\\\\\\\\\\n2.2. Natural Language Processing\\\\\\\\\\\\\\\\nNatural Language Processing (NLP)[6] is a subfield of artificial intelligence (AI) and linguistics\\\\\\\\\\\\\\\\nfocused on enabling computers to understand, interpret, and generate human language in a\\\\\\\\\\\\\\\\nway that is both meaningful and useful. The key goals of NLP are understanding language,\\\\\\\\\\\\\\\\ngenerating language and interacting with humans, the applications of NLP techniques are on\\\\\\\\\\\\\\\\ntext classification, machine translation, speech recognition, text generation, among others. NLP\\\\\\\\\\\\\\\\ntechniques has been used in other fields as DNA (or RNA) sequences, the field is often referred to\\\\\\\\\\\\\\\\nas Computational Biology or Bioinformatics, specically in the subfield of Genomic Data Analysis\\\\\\'\"\\'',\n",
       " '\\'\\\\\\'\\\\\\\\\\\\\\'where DNA and RNA sequences can be thought of as \"biological texts\"[9].\\\\\\\\\\\\\\\\nThere are a considerably amount of models in NLP, we will mention those are in the beginning\\\\\\\\\\\\\\\\na good start for the purpose of our approach.\\\\\\\\\\\\\\\\n2.2.1. Early Neural Network-Based Models\\\\\\\\\\\\\\\\nEarly Neural Network-Based Models refer to the first wave of neural network architectures and\\\\\\\\\\\\\\\\nmethods applied NLP tasks before the dominance of Transformer-based models. These models\\\\\\\\\\\\\\\\nprimarily relied on feedforward neural networks, recurrent architectures, and word embeddings,\\\\\\\\\\\\\\\\noffering improvements over traditional statistical and rule-based methods.\\\\\\\\\\\\\\\\nRNNs\\\\\\\\\\\\\\\\nRecurrent Neural Networks (RNNs) belong to a family of neural networks designed for processing\\\\\\\\\\\\\\\\nsequential data. They can process inputs of variable size, such as images, and can scale to much\\\\\\\\\\\\\\\\nlonger sequences than networks without sequence-based specialization [5].\\\\\\\\\\\\\\\\n6\\\\\\\\\\\\\\'\\\\\\'\\'',\n",
       " '\\'\\\\\\'\\\\\\\\\\\\\\'2.2. Natural Language Processing\\\\\\\\\\\\\\\\nRNNs process data sequentially, meaning the output at a given step depends not only on the\\\\\\\\\\\\\\\\ncurrent input but also on information from previous steps, they have a hidden state that serves\\\\\\\\\\\\\\\\nas a \"memory,\" capturing information from previous time steps.\\\\\\\\\\\\\\\\nThe architecture for RNNs given a input sequenceX = [x1,x2,...,x T] where T is the sequence\\\\\\\\\\\\\\\\nlength, we have:\\\\\\\\\\\\\\\\nht = f(Wtht−1 + Wxxt + b)\\\\\\\\\\\\\\\\nyt = g(Wyht + c)\\\\\\\\\\\\\\\\nWhere Wt,Wx,Wy,b,c are parameters of the model andg is the output activation function\\\\\\\\\\\\\\\\n(e.g, softmax for classification)\\\\\\\\\\\\\\\\nLSTMs\\\\\\\\\\\\\\\\nLong Short-Term Memory (LSTMs) are a type of Recurrent Neural Network (RNN) known\\\\\\\\\\\\\\\\nas gated RNNs. They are based on the idea of introducing self-loops to create paths through\\\\\\\\\\\\\\\\nwhich the gradient can flow for long durations. LSTMs have been found to be extremely suc-\\\\\\\\\\\\\\\\ncessful in many applications, such as unconstrained handwriting recognition, speech recognition,\\\\\\\\\\\\\\'\\\\\\'\\'',\n",
       " '\\'\\\\\\'\\\\\\\\\\\\\\'handwriting generation, machine translation, image captioning, and parsing [5].\\\\\\\\\\\\\\\\nTransformer\\\\\\\\\\\\\\\\nThe Transformer architecture (Vaswani et al., 2017), introduced in the seminal paper \"Attention\\\\\\\\\\\\\\\\nIs All You Need\" (2017), revolutionized the field of NLP and later impacted other domains like\\\\\\\\\\\\\\\\ncomputer vision. It replaced traditional sequential processing models, such as RNNs and LSTMs,\\\\\\\\\\\\\\\\nby introducing self-attention mechanisms, which enable efficient parallelization and long-range\\\\\\\\\\\\\\\\ndependency modeling see Figure 2.1.\\\\\\\\\\\\\\\\n7\\\\\\\\\\\\\\'\\\\\\'\\'',\n",
       " '\\'\\\\\\'\\\\\\\\\\\\\\'2. Background\\\\\\\\\\\\\\\\nFigure 2.1: The transformer architecture, Attention Is All You Need, 2017.\\\\\\\\\\\\\\\\nTransformer decoder only\\\\\\\\\\\\\\\\nTheTransformerDecoder-OnlyArchitecture, popularizedbyGPT(GenerativePre-trainedTrans-\\\\\\\\\\\\\\\\nformer), is a variant of the original Transformer architecture introduced by Vaswani et al. in\\\\\\\\\\\\\\\\n\"Attention is All You Need\" (2017). GPT uses only the decoder part of the Transformer for its\\\\\\\\\\\\\\\\ndesign, focusing on autoregressive text generation tasks[10]. For the achitecture see Figure 2.2\\\\\\\\\\\\\\\\n8\\\\\\\\\\\\\\'\\\\\\'\\'',\n",
       " '\\'\"\\\\\\'2.2. Natural Language Processing\\\\\\\\\\\\\\\\nFigure 2.2: Transformer achitecture, Improving Language Understanding by Generative Pre-\\\\\\\\\\\\\\\\nTraining.\\\\\\\\\\\\\\\\nModel-Conditioned Transformer\\\\\\\\\\\\\\\\nIn this work, we employ the Model-Conditioned Transformer, a GPT-based architecture that\\\\\\\\\\\\\\\\nprocesses no only conventional word token but also additional conditioning variables. Analogous\\\\\\\\\\\\\\\\nto music-generation models that incorporate parameters such as tempo and velocity, our model\\\\\\\\\\\\\\\\nreceives two supplementary numerical inputs:\\\\\\\\\\\\\\\\n1. Performance reward.\\\\\\\\\\\\\\\\n2. Failure probability.\\\\\\\\\\\\\\\\nBoth values are generated during agent execution: the performance reward quantifies the\\\\\\\\\\\\\\\\nagent’s effectiveness, while the failure probability estimates the likelihood of erroneous actions.\\\\\\\\\\\\\\\\nSpecifically, both the performance reward and the failure probability are first processed by an\\\\\\\\\\\\\\\\nauxiliary neural network to produce conditioning vectors. These vectors are then added element-\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'wise to the word-token embeddings, and the resulting combined representations are passed into\\\\\\\\\\\\\\\\nthe transformer decoder, see Figure 2.3 .\\\\\\\\\\\\\\\\n9\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'2. Background\\\\\\\\\\\\\\\\nFigure 2.3: Conditioned Transformer, rewardˆr and failure probabilityPf are first processed by\\\\\\\\\\\\\\\\nan auxiliary neural network, then added element-wise to the word-token embeddings.\\\\\\\\\\\\\\\\n2.3. Abstract Classes\\\\\\\\\\\\\\\\nThe definition of Abstracts Classes relies in the definition of equivalences class, a partition of a\\\\\\\\\\\\\\\\nset. A State Abstraction is defined as a mapping from an original states ∈S to an abstract\\\\\\\\\\\\\\\\nstate sϕ ∈Sϕ\\\\\\\\\\\\\\\\nϕ: S →Sϕ\\\\\\\\\\\\\\\\nwhere Sϕ ∈P(S) and fulfills equivalence conditions (reflexive, symmetric, and transitive) [1].\\\\\\\\\\\\\\\\n2.3.1. Definition π∗-irrelevance abstraction\\\\\\\\\\\\\\\\ns1 and s2 are in the same abstract classϕ(s1) = ϕ(s2) if π∗(s1) = π(s2) where π∗is the optimal\\\\\\\\\\\\\\\\npolicy.\\\\\\\\\\\\\\\\n2.3.2. Definition Q∗-irrelevance abstraction\\\\\\\\\\\\\\\\ns1 ands2 are in the same abstract classϕ(s1) = ϕ(s2) if for all actiona∈A, Q∗(s1,a) = Q∗(s2,a)\\\\\\\\\\\\\\\\nwhere Q∗(s,a) is the optimalQ-value function the maximum expected reward.\\\\\\\\\\\\\\\\n2.3.3. Relax Q∗-irrelevance abstraction\\\\\\\\\\\\\\\\ns1 and s2 are in the same abstract classϕ(s1) = ϕ(s2) if:\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'∀a∈A: ⌈Q∗(s1,a)/d⌉= ⌈Q∗(s2,a)/d⌉\\\\\\\\\\\\\\\\nwhere d is a control parameter (abstraction level) [1].\\\\\\\\\\\\\\\\n10\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'11\\\\\\\\\\\\\\\\nChapter 3\\\\\\\\\\\\\\\\nMarTest: An NLP Approach for test sequence generation\\\\\\\\\\\\\\\\nWe aim to identify the principal potential failures or “vulnerabilities” that an agent may en-\\\\\\\\\\\\\\\\ncounter during execution of its policy within an environment. To this end, we leverage all\\\\\\\\\\\\\\\\navailable execution data—hereinafter referred to as logs (in the context of data testing). These\\\\\\\\\\\\\\\\nlogs consist of records of episodes, i.e. sequences of states, actions and rewards, from which we\\\\\\\\\\\\\\\\nseek to predict possible failures.\\\\\\\\\\\\\\\\nAs noted above, if the state spaceS is uncountable then the set of all possible episodes is\\\\\\\\\\\\\\\\nlikewise uncountable|Sn|>|N|for n= 1,2,3,4....\\\\\\\\\\\\\\\\nHence, the task of enumerating all potential failures is unfeasible. As a first step toward\\\\\\\\\\\\\\\\ntractability, we introduce a reduction via abstract classes, mappingSinto a countable abstraction\\\\\\\\\\\\\\\\nSϕ:\\\\\\\\\\\\\\\\nϕ: S →Sϕ and |Sϕ|<|N|\\\\\\\\\\\\\\\\nOncethisreductionisimplemented, theproblemcanbeusingtheresultingcountablespaceand\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'addressed from a language-processing perspective by regarding each abstract class as a distinct\\\\\\\\\\\\\\\\nlexical token.\\\\\\\\\\\\\\\\n3.1. Vocabulary\\\\\\\\\\\\\\\\nWe define a vocabulary for getting a finite set of sequences:\\\\\\\\\\\\\\\\nlet V be a vocabulary, ifw∈V then w= ˆs or w= a for ˆs∈ˆS and ˆS is a set of partitions ofS\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'3. MarTest: An NLP Approach for test sequence generation\\\\\\\\\\\\\\\\nNote that defining the vocabulary is the first step in framing our problem as an NLP problem.\\\\\\\\\\\\\\\\nThen, we define the collection of all texts, known as the corpus.\\\\\\\\\\\\\\\\n3.2. Corpus\\\\\\\\\\\\\\\\nLet τ be a concrete sequence as s0a0s1a1...snan of state-action tuples and ϕ an abstraction\\\\\\\\\\\\\\\\nfunction, we sayˆτ is an abstraction sequence or abstract path for a sequenceτ under function\\\\\\\\\\\\\\\\nϕ(τ) = ˆτ if:\\\\\\\\\\\\\\\\nfor ˆτ = ˆS0a0 ˆS1a1... ˆSnan we have thatϕ(si) = ˆSi for i= 1,2,3...n\\\\\\\\\\\\\\\\nLet ˆDϕ be a corpus or abstract dataset such thatˆDϕ is composed of abstract pathsˆτ.\\\\\\\\\\\\\\\\nIf we were to take the setˆDϕ as the training set for an NLP model, the model would most\\\\\\\\\\\\\\\\nlikely imitate the agent’s behavior in an abstract way, since we would be training it on behavior\\\\\\\\\\\\\\\\npatterns associated with each abstract class. However, our goal is not to replicate the agent’s\\\\\\\\\\\\\\\\nbehavior, but rather to identify low-reward regions and potential failures. For this purpose, we\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'extend the definition of our dataset.\\\\\\\\\\\\\\\\n3.3. Dataset\\\\\\\\\\\\\\\\nLet D be the training dataset for the conditioned Transformer model, such that(ˆτ,¯r,Pf ) ∈D\\\\\\\\\\\\\\\\nif ˆτ ∈ ˆDϕ, ¯r is the average reward ofˆτ, andPf is the failure probability ofˆτ.\\\\\\\\\\\\\\\\n3.3.1. Average reward\\\\\\\\\\\\\\\\nSince ˆτ may represent multiple concrete sequences—that is, it may happen thatϕ(τ1) = ϕ(τ2) =\\\\\\\\\\\\\\\\n··· = ϕ(τn) = ˆτ for sequences τ1,τ2,...,τ n of state-action tuples—we take the total rewards\\\\\\\\\\\\\\\\nassociated with eachτi and compute their average to obtain the mean reward¯r.\\\\\\\\\\\\\\\\n3.3.2. Failure probability\\\\\\\\\\\\\\\\nWe can associate eachˆτ ∈ ˆDϕ with a failure probability. Given a sequence of concrete trajectories\\\\\\\\\\\\\\\\nτ1,τ2,...,τ n executed by an agent, we know that each trajectory either resulted in failure or not.\\\\\\\\\\\\\\\\n12\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'3.3. Dataset\\\\\\\\\\\\\\\\nThat is, we can construct a list of pairs(τ1,j1),(τ2,j2),..., (τn,jn), wherejk = 1 if the execution\\\\\\\\\\\\\\\\nof τk was successful, andjk = 0 otherwise.\\\\\\\\\\\\\\\\nTherefore, wecanderiveacorrespondinglistofpairs (ˆτ1,j1),(ˆτ2,j2),..., (ˆτn,jn), whereϕ(τk) =\\\\\\\\\\\\\\\\nˆτk for k= 1,2,...,n . In this way, we obtain a dataset of abstract trajectories labeled as success\\\\\\\\\\\\\\\\nor failure. With the appropriate model, we can then predict the failure probability for eachˆτ.\\\\\\\\\\\\\\\\n3.3.3. Random Forest\\\\\\\\\\\\\\\\nTo predict the failure probability of an abstract trajectoryˆτ, we follow the approach proposed by\\\\\\\\\\\\\\\\nZolfagharian [1, Section IV, Subsection 7], which consists of representingˆτ as a binary sequence\\\\\\\\\\\\\\\\ndetermined by the presence of abstract states withinˆτ. This representation, together with the\\\\\\\\\\\\\\\\ncorresponding failure label, serves as input for aRandom Forestmodel.\\\\\\\\\\\\\\\\nAssume that ˆS0, ˆS1,..., ˆSm are all the abstract classes produced by a mappingϕ. Then, for a\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'given abstract trajectoryˆτ, we can construct the following binary representation:\\\\\\\\\\\\\\\\nAbstract Sequence ˆS0 ˆS1 ˆS2 ... ˆSm\\\\\\\\\\\\\\\\nˆτ 0 1 1 ... 0\\\\\\\\\\\\\\\\nTable 3.1: Binary representation of an abstract trajectory\\\\\\\\\\\\\\\\nTherefore, by constructing a binary representation for each sequence followed by the agent, and\\\\\\\\\\\\\\\\nlabeling the corresponding performance outcome as either failure (1) or success (0), we obtain a\\\\\\\\\\\\\\\\ndataset suitable for training a classification algorithm:\\\\\\\\\\\\\\\\nIndex ˆS0 ˆS1 ˆS2 ... ˆSm Failure\\\\\\\\\\\\\\\\ni 0 1 1 ... 0 1\\\\\\\\\\\\\\\\ni+ 1 1 0 0 ... 0 0\\\\\\\\\\\\\\\\nTable 3.2: Training data representation for a classification model\\\\\\\\\\\\\\\\n13\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'3. MarTest: An NLP Approach for test sequence generation\\\\\\\\\\\\\\\\n3.4. Model-Conditioned Transformer\\\\\\\\\\\\\\\\nIn this approach, any natural language processing (NLP) technique can be employed to generate\\\\\\\\\\\\\\\\nabstract sequences associated with a certain probability of failure. However, as previously stated,\\\\\\\\\\\\\\\\nthis work proposes the use of a conditioned transformer to identify risk regions.\\\\\\\\\\\\\\\\nGiven the dataset D, composed of 3-tuples (ˆτ,¯r,Pf ), we have a training set for a Model-\\\\\\\\\\\\\\\\nConditioned Transformer, as described in Section 2.2.1. Once the conditioned transformer is\\\\\\\\\\\\\\\\ntrained, it can be used to generate abstract sequences given a rewardr and a failure probability\\\\\\\\\\\\\\\\nPf.\\\\\\\\\\\\\\\\nThese sequences, composed of abstract statesˆS interpreted by the transformer as tokensw,\\\\\\\\\\\\\\\\ncan also be seen as regions associated with a certain likelihood of failure and low reward. The\\\\\\\\\\\\\\\\nselection and handling of these regions may vary depending on the objective. In Section 5.4, we\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'introduce a method for generating test cases based on these sequences as part of our experimental\\\\\\\\\\\\\\\\nsetup.\\\\\\\\\\\\\\\\n3.4.1. Assumptions\\\\\\\\\\\\\\\\nIn this work, we focus on RL agents with discrete actions we could extend the theory for infinity\\\\\\\\\\\\\\\\nof set of actions, but for simplicity we will do in a future work.\\\\\\\\\\\\\\\\n14\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'15\\\\\\\\\\\\\\\\nChapter 4\\\\\\\\\\\\\\\\nMarTest Pipeline\\\\\\\\\\\\\\\\nIn this chapter, we present the implementation of our approach, called MarTest-Pipeline, a\\\\\\\\\\\\\\\\nsoftware tool designed to produce risk regions for testing reinforcement learning agents. The\\\\\\\\\\\\\\\\nimplementation is available athttps://github.com/lamedinaauniandes/Martest-pipeline.\\\\\\\\\\\\\\\\nWe describe the purpose and functionality of each module at a high level, emphasizing that all\\\\\\\\\\\\\\\\ncomponents are configurable and support module-specific arguments. This overview is intended\\\\\\\\\\\\\\\\ntohelpthereadernavigatetheimplementationwithgreaterclarity. Fordetailsregardingsoftware\\\\\\\\\\\\\\\\nrequirements and environment setup, please refer to Appendix A.1.\\\\\\\\\\\\\\\\n4.1. Pipeline Implementation\\\\\\\\\\\\\\\\nOur implementation comprises five principal modules, each corresponding to a distinct stage in\\\\\\\\\\\\\\\\nthe log-processing pipeline for any agent (shown in Figure 4.1).\\\\\\\\\\\\\\\\nEach module performs its designated processing tasks and produces the artifacts required by\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'the subsequent module. This modular design offers us considerable flexibility for future work:\\\\\\\\\\\\\\\\nfor instance, researchers wishing to focus on the abstract-class graph can directly leverage the\\\\\\\\\\\\\\\\noutputs generated by the second module,2. Abstract Classes, as the basis for their analyses.\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'4. MarTest Pipeline\\\\\\\\\\\\\\\\nFigure 4.1: Martest pipeline .\\\\\\\\\\\\\\\\n4.2. Module 1: Data acquisition and preparation\\\\\\\\\\\\\\\\n4.2.1. Inputs training and execution logs\\\\\\\\\\\\\\\\nThe pipeline begins with two types of logs: training logs, which record the agent’s behavior\\\\\\\\\\\\\\\\nduring the training phase, and execution logs, which capture its behavior after training (i.e.,\\\\\\\\\\\\\\\\nduring real-world deployment). Although it is not strictly necessary to provide both types of\\\\\\\\\\\\\\\\nlogs, combining them can help balance the dataset. In particular, if the execution logs contain an\\\\\\\\\\\\\\\\ninsufficient number of failure episodes —as may happen in some cases where the agent performs\\\\\\\\\\\\\\\\nperfectly with no observed failures— an effective strategy is to augment them with records from\\\\\\\\\\\\\\\\nthe training phase.\\\\\\\\\\\\\\\\n4.2.2. Data Balacing\\\\\\\\\\\\\\\\nIn this stage of the pipeline, we specify a mixture ratiop∈[0,1] and sample from the training\\\\\\\\\\\\\\\\nlogs accordingly. Since early training episodes may not accurately represent the agent’s behavior,\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'we drawm episodes from training data wherem = p·Ntrain and are sample according to the\\\\\\\\\\\\\\\\nprobability distribution over episodes introduced by Zolfagharian [1, Sección IV,Ec. (6)]:\\\\\\\\\\\\\\\\nP(ei) = i\\\\\\\\\\\\\\\\nNtrain∑\\\\\\\\\\\\\\\\nj=1\\\\\\\\\\\\\\\\nj\\\\\\\\\\\\\\\\n, i = 1,2,...,m,\\\\\\\\\\\\\\\\nwhere Ntrain denotes the total number of training episodes. This weighting favors latter (more\\\\\\\\\\\\\\\\nrepresentative) episodes while still incorporating early-stage data with more state exploration.\\\\\\\\\\\\\\\\nThe output of this stage is an execution log combining episodes of both training and execution.\\\\\\\\\\\\\\\\n16\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'4.3. Module 2: Abstract Classes\\\\\\\\\\\\\\\\n4.2.3. Q-Values\\\\\\\\\\\\\\\\nDuring this step in the process, we calculate the total reward obtain for each state and action\\\\\\\\\\\\\\\\nfor all episodes in the logs, based in the definition of theaction-value for a states, actiona, and\\\\\\\\\\\\\\\\npolicy π [13, Sección 3.7, Ec. (3.11)] :\\\\\\\\\\\\\\\\nqπ(s,a) = Eπ\\\\\\\\\\\\\\\\n[\\\\\\\\\\\\\\\\nGt |St = s, At = a\\\\\\\\\\\\\\\\n]\\\\\\\\\\\\\\\\n= Eπ\\\\\\\\\\\\\\\\n[∞∑\\\\\\\\\\\\\\\\nk=0\\\\\\\\\\\\\\\\nγkRt+k+1\\\\\\\\\\\\\\\\n⏐⏐⏐St = s, At = a\\\\\\\\\\\\\\\\n]\\\\\\\\\\\\\\\\nThis equation calculates the value expected to take an actiona in state s, accounting for\\\\\\\\\\\\\\\\nfuture rewards, in our case we do not use expected values because the policy has already been\\\\\\\\\\\\\\\\nimplemented. Furthermore, as we have already obtained the rewards, we do not discount the\\\\\\\\\\\\\\\\nrewards. As a consequence we calculate the q-values as:γ = 1,\\\\\\\\\\\\\\\\nqπ(s,a) =\\\\\\\\\\\\\\\\n∞∑\\\\\\\\\\\\\\\\nk=0\\\\\\\\\\\\\\\\nRt+k+1\\\\\\\\\\\\\\\\nThe end result of this is the q-table with the q-value for each state-action pair.\\\\\\\\\\\\\\\\n4.3. Module 2: Abstract Classes\\\\\\\\\\\\\\\\n4.3.1. Construction\\\\\\\\\\\\\\\\nOnce we have the Q-table, we proceed to build the abstract classes using the definition of Relaxed\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'Q∗-irrelevance abstraction (see Section 2.3.3). This module generates a JSON file containing a\\\\\\\\\\\\\\\\ndictionary, where the keys are the representatives of the abstract classes and the values are lists\\\\\\\\\\\\\\\\nof the states grouped within each class.\\\\\\\\\\\\\\\\nSince the definition ofQ∗-irrelevance abstraction depends on the abstraction level—a config-\\\\\\\\\\\\\\\\nurable parameter—the user can set this value according to the desired granularity.\\\\\\\\\\\\\\\\nNote that at this stage, we are constructing the vocabulary referenced in Section 3.1.\\\\\\\\\\\\\\\\n4.4. Module 3\\\\\\\\\\\\\\\\n4.4.1. Abstract Episodes generation\\\\\\\\\\\\\\\\nIn this module, we proceed to build the corpusˆDϕ, as defined in Section 3.2, and compute the\\\\\\\\\\\\\\\\naverage reward of each abstract episode. The output consists of CSV files containing abstract\\\\\\\\\\\\\\\\n17\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'4. MarTest Pipeline\\\\\\\\\\\\\\\\nsequences along with their corresponding average rewards.\\\\\\\\\\\\\\\\n4.5. Module 4\\\\\\\\\\\\\\\\n4.5.1. Random Forest Model\\\\\\\\\\\\\\\\nIn this module, we compute the failure probability for each abstract sequence obtained in the\\\\\\\\\\\\\\\\nprevious module. To achieve this, we generate CSV files containing the binary representations of\\\\\\\\\\\\\\\\neach abstract sequence along with their corresponding failure labels, as described in Section 3.3.3,\\\\\\\\\\\\\\\\nin order to estimate the failure probability of each abstract sequence.\\\\\\\\\\\\\\\\nFinally, we generate CSV files containing the datasetD, as described in Section 3.3, which\\\\\\\\\\\\\\\\nserves as input for the Transformer model.\\\\\\\\\\\\\\\\nInthismoduleweusethelibrary sklearnbyscikit-learn, asimpleandefficienttoolforpredictive\\\\\\\\\\\\\\\\nanalysis, we use the classsklearn.ensemble.RandomForestClassifierprecisely and show us a great\\\\\\\\\\\\\\\\nprecision and performance.\\\\\\\\\\\\\\\\n4.6. Module 5\\\\\\\\\\\\\\\\n4.6.1. Conditioned Transformer\\\\\\\\\\\\\\\\nIn this module, we train the Conditioned Transformer using the datasetD produced in the\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'previous module. Once the model is trained, we can also generate abstract sequences given a\\\\\\\\\\\\\\\\nreward valuer and a failure probabilityPf, thereby identifying regions with potential risk of\\\\\\\\\\\\\\\\nfailure or low reward, as described in Section 3.4.\\\\\\\\\\\\\\\\nWe use thetransformers library fromHugging Face, which integrates with PyTorch. Specifi-\\\\\\\\\\\\\\\\ncally, we base our implementation on thetransformers.GPT2LMHeadModel class, which follows\\\\\\\\\\\\\\\\nthe GPT-2 architecture—a decoder-only Transformer commonly used for text generation. We\\\\\\\\\\\\\\\\nmodified this architecture to also receive the average reward and failure probability as inputs in\\\\\\\\\\\\\\\\norder to predict tokens (i.e., abstract states or regions) associated with specific levels of risk or\\\\\\\\\\\\\\\\nfailure likelihood. We refer to this modified model as theConditioned Transformer.\\\\\\\\\\\\\\\\nThe Conditioned Transformer is implemented by extending theGPT2LMHeadModel class to\\\\\\\\\\\\\\\\n18\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'4.6. Module 5\\\\\\\\\\\\\\\\na new class namedCondGPT2. The core idea is to preprocess the average reward and failure\\\\\\\\\\\\\\\\nprobability through a small feedforward network, producing a vector representation that is then\\\\\\\\\\\\\\\\nadded to the token embeddings of the abstract sequence before being passed to the GPT-2 model.\\\\\\\\\\\\\\\\nFor further details, see Section A.2.\\\\\\\\\\\\\\\\n4.6.2. Test Regions Cases\\\\\\\\\\\\\\\\nAt the end of the pipeline, we obtain an output in the form of a list ofpossible abstract\\\\\\\\\\\\\\\\nepisodes—that is, sequences of abstract states and actions (e.g.,’w2 1 w1 2 w3 1 w6 ... w5\\\\\\\\\\\\\\\\n1 True’) interpreted by the transformer astokens or words. However, since these tokens\\\\\\\\\\\\\\\\nwi represent abstract classes, they can also be understood assequences of regions and\\\\\\\\\\\\\\\\nactions.\\\\\\\\\\\\\\\\nGiven that the transformer has been designed to receive the average reward and fail-\\\\\\\\\\\\\\\\nure probabilityas input parameters, we interpret these region–action sequences aspossible\\\\\\\\\\\\\\\\nepisodes conditioned on those parameters.\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'Based on this representation, multiple strategies can be devised to generateconcrete and\\\\\\\\\\\\\\\\nactionable test cases. In the following chapter, we present the specific testing approaches\\\\\\\\\\\\\\\\napplied to the selected agents.\\\\\\\\\\\\\\\\n19\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'4. MarTest Pipeline\\\\\\\\\\\\\\\\n20\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'21\\\\\\\\\\\\\\\\nChapter 5\\\\\\\\\\\\\\\\nExperimental Design\\\\\\\\\\\\\\\\nWe evaluate our implementation on six agents selected from the publicly editable OpenAI Gym\\\\\\\\\\\\\\\\nleaderboard wiki page1. Three of these agents implement in theCartPole-v0 environment, and\\\\\\\\\\\\\\\\nthe remaining three are implement theLunarLander-v2.\\\\\\\\\\\\\\\\nFor all agents, we apply the definition ofRelaxedQ∗-irrelevance abstraction( Section 2.3.3), as\\\\\\\\\\\\\\\\nwe consider this abstraction suitable for capturing the agent’s general perception formed through\\\\\\\\\\\\\\\\nits interaction with the environment and its learning dynamics.\\\\\\\\\\\\\\\\nThe constructed abstraction assigns avalue to each experience and allows for controlling the\\\\\\\\\\\\\\\\ngranularity of the abstract classes through the abstraction-level parameter. In our experiments,\\\\\\\\\\\\\\\\nthis parameter was set tod= 4, chosen as a midpoint based on the experimental settings reported\\\\\\\\\\\\\\\\nby Zolfagharian [1, Section V, Table I].\\\\\\\\\\\\\\\\nFor each agent, we introduced a lightweight instrumentation module into the codebase to\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'record logs during both training and execution. From this point onward, we refer to this logging\\\\\\\\\\\\\\\\nprocess asinstrumentation.\\\\\\\\\\\\\\\\nThe server used to run the environments in the experiment has the following specifications:\\\\\\\\\\\\\\\\n• CPU: 3×Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz (model 79)\\\\\\\\\\\\\\\\n• RAM: 7.8GiB (MemTotal)\\\\\\\\\\\\\\\\n• Primary storage:1× 60GB (device sda)\\\\\\\\\\\\\\\\n1https://github.com/openai/gym/wiki/Leaderboard\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\n• Operating system:Ubuntu 22.04.5 LTS (Jammy)\\\\\\\\\\\\\\\\nAll experimental data produced by the MarTest-Pipeline software, as a result of each agent’s\\\\\\\\\\\\\\\\nexecution, is available athttps://zenodo.org/records/15485620. The specifications of the\\\\\\\\\\\\\\\\nmachines used to run the MarTest-Pipeline software were as follows:\\\\\\\\\\\\\\\\n• CPU:48 logical cores (2×Intel® Xeon® Silver 4310 CPU @ 2.10GHz; 12 cores per socket,\\\\\\\\\\\\\\\\n2 threads per core)\\\\\\\\\\\\\\\\n• RAM: 251.3GiB (MemTotal)\\\\\\\\\\\\\\\\n• Primary storage:1 ×7 TB (devicesda)\\\\\\\\\\\\\\\\n• OS: Ubuntu 22.04.4 LTS (Jammy)\\\\\\\\\\\\\\\\n• GPU: 4×NVIDIA A40 (46068MiB VRAM each); Driver Version: 560.35.03, CUDA\\\\\\\\\\\\\\\\nVersion: 12.6\\\\\\\\\\\\\\\\n5.1. CartPole-v0 Agents\\\\\\\\\\\\\\\\nWe begin by presenting a description of theCartPole-v0 environment provided by OpenAI Gym:\\\\\\\\\\\\\\\\n“A pole is attached by an un-actuated joint to a cart, which moves along a frictionless\\\\\\\\\\\\\\\\ntrack. The pendulum is placed upright on the cart and the goal is to balance the pole\\\\\\\\\\\\\\\\nby applying forces in the left and right direction on the cart.”2\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'Therefore, the only actions available to the agent are pushing the cart to the left or to the\\\\\\\\\\\\\\\\nright, represented by the action setA= {0,1}. The agent observes the state of the environment\\\\\\\\\\\\\\\\nas a list of four continuous values corresponding to:cart position, cart velocity, pole angle, and\\\\\\\\\\\\\\\\npole velocity at the tip(see Figure 5.1).\\\\\\\\\\\\\\\\n2https://gymnasium.farama.org/environments/classic_control/cart_pole/\\\\\\\\\\\\\\\\n22\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5.1. CartPole-v0 Agents\\\\\\\\\\\\\\\\nFigure 5.1: CartPole-v0.\\\\\\\\\\\\\\\\nFor this environment the selected agents are:\\\\\\\\\\\\\\\\nShakti Kumar (shakti)\\\\\\\\\\\\\\\\nRanked fourth on the OpenAI Gym leaderboard, this agent is reported to solve the environment\\\\\\\\\\\\\\\\nin 0 episodes. The code [8, shakti], dated from December 2019, requiring to set up a custom\\\\\\\\\\\\\\\\nenvironment based on Python 3.7.\\\\\\\\\\\\\\\\nThe necessary dependencies are listed in Appendix A.3. Due to the legacy state of the code,\\\\\\\\\\\\\\\\nwe installed PyTorch manually from pytorch.org using the following command for Ubuntu:\\\\\\\\\\\\\\\\npip install torch ==1.2.0+ cu92 torchvision ==0.4.0+ cu92 -f https ://\\\\\\\\\\\\\\\\ndownload . pytorch . org / whl / torch_stable . html\\\\\\\\\\\\\\\\nKapil Chauhan (kapil)\\\\\\\\\\\\\\\\nRanked ninth on the OpenAI Gym leaderboard, this agent is reported to solve the environment\\\\\\\\\\\\\\\\nin 4 episodes. The code [4, kapil], dated from December 2019, also requires setting up a custom\\\\\\\\\\\\\\\\nenvironment based on Python 3.7. The required dependencies are listed in Appendix A.4.\\\\\\\\\\\\\\\\nNihal T. Rao (nihal)\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'Ranked twenty-ninth on the OpenAI Gym leaderboard, this agent is reported to solve the en-\\\\\\\\\\\\\\\\nvironment in 184 episodes. The code [11, Nihal], dated from June 2020, also requires setting\\\\\\\\\\\\\\\\n23\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\nup a custom environment based on Python 3.7. The necessary dependencies are listed in Ap-\\\\\\\\\\\\\\\\npendix A.5.\\\\\\\\\\\\\\\\n5.2. LunarLander-v2 Agents\\\\\\\\\\\\\\\\nThe description of theLunarLander-v2 environment, as provided by OpenAI Gym is:\\\\\\\\\\\\\\\\n“This environment is a classic rocket trajectory optimization problem. According to\\\\\\\\\\\\\\\\nPontryagin’s maximum principle, it is optimal to fire the engine at full throttle or\\\\\\\\\\\\\\\\nturn it off. This is the reason why this environment has discrete actions: engine on\\\\\\\\\\\\\\\\nor off... The landing pad is always at coordinates (0,0). The coordinates are the first\\\\\\\\\\\\\\\\ntwo numbers in the state vector. Landing outside of the landing pad is possible. Fuel\\\\\\\\\\\\\\\\nis infinite, so an agent can learn to fly and then land on its first attempt.”3\\\\\\\\\\\\\\\\nThere are four discrete actions available:0 – do nothing,1 – fire left orientation engine,2 –\\\\\\\\\\\\\\\\nfire main engine,3 – fire right orientation engine.\\\\\\\\\\\\\\\\nThe state is represented as an 8-dimensional vector, which includes: the lander’s position\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'coordinates (x,y), its linear velocities(vx,vy), its angle and angular velocity, and two boolean\\\\\\\\\\\\\\\\nvalues indicating whether the left and right legs are in contact with the ground.4 (See Figure 5.2.)\\\\\\\\\\\\\\\\nFor this environment, the selected agents are:\\\\\\\\\\\\\\\\n5.2.1. Sanket Thakur (sanket)\\\\\\\\\\\\\\\\nRanked ninth on the OpenAI Gym leaderboard, this agent is reported to solve the environment\\\\\\\\\\\\\\\\nin 454 episodes. The code [14, sanket], dated from April 2020, requires setting up a custom\\\\\\\\\\\\\\\\nenvironment based on Python 3.7. The required dependencies are listed in Appendix A.7.\\\\\\\\\\\\\\\\n5.2.2. Mahmood Khordoo (khordoo)\\\\\\\\\\\\\\\\nRanked tenth on the OpenAI Gym leaderboard, this agent is reported to solve the environment\\\\\\\\\\\\\\\\nin 602 episodes. The code [7, khordoo], dated from April 2020, also requires setting up a custom\\\\\\\\\\\\\\\\n3https://gymnasium.farama.org/environments/box2d/lunar_lander/#description\\\\\\\\\\\\\\\\n4https://gymnasium.farama.org/environments/box2d/lunar_lander/#description\\\\\\\\\\\\\\\\n24\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5.3. Conditioned transformer configurations and metrics\\\\\\\\\\\\\\\\nFigure 5.2: LunarLander-v2.\\\\\\\\\\\\\\\\nenvironment based on Python 3.7. The required dependencies are listed in Appendix A.6.\\\\\\\\\\\\\\\\n5.2.3. Sigve Rokenes (rokenes)\\\\\\\\\\\\\\\\nRankednineteenontheOpenAIGymleaderboard, thisagentisreportedtosolvetheenvironment\\\\\\\\\\\\\\\\nin 1,590 episodes. The code [12, rokenes], dated from January 2019, requires setting up a custom\\\\\\\\\\\\\\\\nenvironment based on Python 3.7. The necessary dependencies are listed in Appendix A.8.\\\\\\\\\\\\\\\\n5.3. Conditioned transformer configurations and metrics\\\\\\\\\\\\\\\\nNext, we present the configurations and training results of the conditioned transformer for each\\\\\\\\\\\\\\\\nexperiment. We reportvocab_size, the number of tokens in the model’s vocabulary;n_embd, the\\\\\\\\\\\\\\\\ndimensionality of the token embeddings and hidden states;n_layer, the number of transformer\\\\\\\\\\\\\\\\ndecoder blocks (also called layers); and n_head, the number of attention heads in each self-\\\\\\\\\\\\\\\\nattention layer.\\\\\\\\\\\\\\\\n25\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\n5.3.1. Conditional transformer parameters for cartpole\\\\\\\\\\\\\\\\nAgent vocab_size n_embd n_layer n_head\\\\\\\\\\\\\\\\nShakti 101 1024 12 16\\\\\\\\\\\\\\\\nKapil 101 1024 12 16\\\\\\\\\\\\\\\\nNihal 101 1024 12 16\\\\\\\\\\\\\\\\nTable 5.1: Parameters of conditional transformers for cartpole.\\\\\\\\\\\\\\\\n5.3.2. Conditional transformer parameters for Lunar-Lander\\\\\\\\\\\\\\\\nAgent vocab_size n_embd n_layer n_head\\\\\\\\\\\\\\\\nSanket 840 512 6 8\\\\\\\\\\\\\\\\nKhordoo 711 512 6 8\\\\\\\\\\\\\\\\nRokenes 868 512 6 8\\\\\\\\\\\\\\\\nTable 5.2: Parameters of conditional transformer for Lunar-lander.\\\\\\\\\\\\\\\\n5.3.3. Training and validations loss\\\\\\\\\\\\\\\\nNext, we present the loss metrics for each agent in the CartPole and LunarLander environments.\\\\\\\\\\\\\\\\nAs shown, the loss for all agents decreases over time and converges to zero. This suggests that\\\\\\\\\\\\\\\\nNLP techniques are sufficient to solve these tasks, which is likely due to the relatively small\\\\\\\\\\\\\\\\nvocabulary size compared to other NLP problems, such as machine translation or large language\\\\\\\\\\\\\\\\nmodels.\\\\\\\\\\\\\\\\n26\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5.3. Conditioned transformer configurations and metrics\\\\\\\\\\\\\\\\nTraining and Evaluation Loss per Epoch, Shakti\\\\\\\\\\\\\\\\nFigure 5.3: Training and Evaluation Loss per Epoch, Shakti\\\\\\\\\\\\\\\\nTraining and Evaluation Loss per Epoch, Kapil\\\\\\\\\\\\\\\\nFigure 5.4: Training and Evaluation Loss per Epoch, Kapil\\\\\\\\\\\\\\\\n27\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n5. Experimental Design\\\\\\\\\\\\\\\\nTraining and Evaluation Loss per Epoch, Nihal\\\\\\\\\\\\\\\\nFigure 5.5: Training and Evaluation Loss per Epoch, Nihal\\\\\\\\\\\\\\\\nTraining and Evaluation Loss per Epoch, Sanket\\\\\\\\\\\\\\\\nFigure 5.6: Training and Evaluation Loss per Epoch, Sanket\\\\\\\\\\\\\\\\n28\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n5.4. Concrete Test Case Generator\\\\\\\\\\\\\\\\nTraining and Evaluation Loss per Epoch, Khordoo\\\\\\\\\\\\\\\\nFigure 5.7: Training and Evaluation Loss per Epoch, Khordoo\\\\\\\\\\\\\\\\nTraining and Evaluation Loss per Epoch, Rokenes\\\\\\\\\\\\\\\\nFigure 5.8: Training and Evaluation Loss per Epoch, Rokenes\\\\\\\\\\\\\\\\n5.4. Concrete Test Case Generator\\\\\\\\\\\\\\\\nOnce the MarTest Pipeline Regionsmodule is deployed, we obtain a list of possible regions\\\\\\\\\\\\\\\\nassociated with failure risk or low rewards. The next step is to take advantage of this output in\\\\\\\\\\\\\\\\n29\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\norder to effectively test the agents.\\\\\\\\\\\\\\\\nTo define our strategy, we refer to the Markov property, which states that the environment’s\\\\\\\\\\\\\\\\nresponse at timet+ 1 depends only on the current state and action at timet (see Section 2.1).\\\\\\\\\\\\\\\\nFollowing this principle, we consider it valid to infer possible failures or low-reward outcomes\\\\\\\\\\\\\\\\nstarting from an initial state. Our model provides a list of such high-risk initial regions, indicating\\\\\\\\\\\\\\\\nsituations where failure may occur within a limited number of steps.\\\\\\\\\\\\\\\\nFrom each initial region within an abstract sequence produced by the conditioned Trans-\\\\\\\\\\\\\\\\nformer (e.g.,w50 in ˆτ = w50 1 w12 0 ...w 223), we sampled 100 states and applied perturbations\\\\\\\\\\\\\\\\nto each point. Specifically, we added noise sampled from a uniform distribution in the range\\\\\\\\\\\\\\\\n[−0.001,0.001] to each coordinate. This perturbation step enhances the testing process by intro-\\\\\\\\\\\\\\\\nducing slight variability into the state space.\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'For each perturbed state, we initialized the environment and executed the agent starting from\\\\\\\\\\\\\\\\nthat point. We then compared the resulting rewards with those obtained by executing the agent\\\\\\\\\\\\\\\\nunder normal conditions.\\\\\\\\\\\\\\\\nDue to the nature of theLunarLander-v2 environment, an additional consideration is required.\\\\\\\\\\\\\\\\nIn this environment, the lander always starts at coordinatex= 0.0 and approximatelyy≈1.41\\\\\\\\\\\\\\\\nin normalized coordinates. In the internalBox2D coordinate system, this corresponds to a fixed\\\\\\\\\\\\\\\\nstarting point near(10.0, 13.33).\\\\\\\\\\\\\\\\nTherefore, it is necessary to validate whether a given state can be considered a valid initial\\\\\\\\\\\\\\\\nstate for the environment. Only those states satisfying this condition are used, and perturbations\\\\\\\\\\\\\\\\nare applied to them within the uniform random range described earlier. These perturbed states\\\\\\\\\\\\\\\\nare then used to initialize the environment during testing.\\\\\\\\\\\\\\\\n5.5. An Example\\\\\\\\\\\\\\\\nIn this section we show a example produced by test generator, we show you how start and all\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'taking of decisions through the sequence of states.\\\\\\\\\\\\\\\\n30\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5.6. Evaluation and Results\\\\\\\\\\\\\\\\n5.6. Evaluation and Results\\\\\\\\\\\\\\\\nIn this section, we present the evaluation and results for each agent. Two types of analyses are\\\\\\\\\\\\\\\\nincluded:\\\\\\\\\\\\\\\\nFirst, we report statistical comparisons of the average rewards. The goal is to determine\\\\\\\\\\\\\\\\nwhether, in general, the regions proposed by the test case generator result in lower rewards\\\\\\\\\\\\\\\\ncompared to normal agent execution.\\\\\\\\\\\\\\\\nSecond, we evaluate the probability of success, defined as the proportion of episodes where the\\\\\\\\\\\\\\\\nagent achieves a total reward greater than or equal to 200. It is important to clarify that using\\\\\\\\\\\\\\\\nthe mean reward alone may be misleading in some cases: an agent could obtain a few episodes\\\\\\\\\\\\\\\\nwith very high rewards, raising the average, while still failing to consistently solve the task (i.e.,\\\\\\\\\\\\\\\\nachieving fewer successful episodes). Therefore, both the average performance and the success\\\\\\\\\\\\\\\\nrate are considered in the analysis.\\\\\\\\\\\\\\\\n5.6.1. Cartpole\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'The following section provides a summary of the vocabulary size (i.e., number of abstract classes)\\\\\\\\\\\\\\\\ngenerated for each agent in cartpole environment, as well as the number of regions identified as\\\\\\\\\\\\\\\\ninitial regions with a high probability of failure or low expected reward by the transformer. We\\\\\\\\\\\\\\\\nalso report the sample size, defined as the number of test cases generated by the Concrete Test\\\\\\\\\\\\\\\\nCase Generator.\\\\\\\\\\\\\\\\nAgent Vocabulary Size Number of Initial Regions Sample Size\\\\\\\\\\\\\\\\nShakti 101 42 4000\\\\\\\\\\\\\\\\nKapil 101 42 4200\\\\\\\\\\\\\\\\nNihal 101 42 3900\\\\\\\\\\\\\\\\nTable 5.3: Summary of vocabulary size, number of initial regions and sample size used for each\\\\\\\\\\\\\\\\nagent, Cartpole.\\\\\\\\\\\\\\\\nThe sample size represents too the total number of episodes executed for both normal execution\\\\\\\\\\\\\\\\n31\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\nand simulated testing. The variation in sample size across agents arises from the design of the\\\\\\\\\\\\\\\\ntest case generator, which attempts to sample up to 100 states per risk region. However, in\\\\\\\\\\\\\\\\npractice, some abstract classes contain fewer than 100 states, limiting the number of available\\\\\\\\\\\\\\\\ntest cases.\\\\\\\\\\\\\\\\nWe now present the table of statistical comparisons based on average rewards:\\\\\\\\\\\\\\\\nAgent Mean SD SEM t-statistic P-value µtesting <µexecution\\\\\\\\\\\\\\\\nShakti\\\\\\\\\\\\\\\\nSample Testing 96.98 67.95 1.07 -80.845 p< 1 ×10−10 yes\\\\\\\\\\\\\\\\nNormal Execution 185.30 12.52 0.20\\\\\\\\\\\\\\\\nKapil\\\\\\\\\\\\\\\\nSample Testing 190.86 35.06 0.54 -76.536 p< 1 ×10−10 yes\\\\\\\\\\\\\\\\nNormal Execution 235.06 13.09 0.20\\\\\\\\\\\\\\\\nNihal\\\\\\\\\\\\\\\\nSample Testing 182.47 82.53 1.32 7.371 p= 1.000 no\\\\\\\\\\\\\\\\nNormal Execution 169.23 75.92 1.22\\\\\\\\\\\\\\\\nTable 5.4: Comparison of sample testing and normal execution for each agent in cartpole envi-\\\\\\\\\\\\\\\\nronment.\\\\\\\\\\\\\\\\nFor the agents Kapil and Shakti (CartPole), we can conclude that there is sufficient statistical\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'evidence to support the claim that the test case generator identifies regions of significantly lower\\\\\\\\\\\\\\\\nreward.\\\\\\\\\\\\\\\\nIn particular, note that for Kapil—an agent with a perfect success rate under normal conditions\\\\\\\\\\\\\\\\n(as we will discuss later)— the generator produced test cases with an average reward of 190.86,\\\\\\\\\\\\\\\\ncompared to 235.06 under normal execution.\\\\\\\\\\\\\\\\nSimilarly, for Shakti, the mean reward under generated test cases dropped to 96.98, nearly\\\\\\\\\\\\\\\\nhalf of the 185.30 average under normal conditions, indicating that the generator successfully\\\\\\\\\\\\\\\\nidentified regions associated with very low performance.\\\\\\\\\\\\\\\\nWe now present the success rate comparison for all agents, where success is defined as achieving\\\\\\\\\\\\\\\\n32\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5.6. Evaluation and Results\\\\\\\\\\\\\\\\na total reward of at least 200 per episode.\\\\\\\\\\\\\\\\nCondition Success Rate z-statistic p-value Ptesting <Pexecution\\\\\\\\\\\\\\\\nShakti\\\\\\\\\\\\\\\\nSample Testing 0.047 -14.418 p= 2.005 ×10−47 yes\\\\\\\\\\\\\\\\nNormal Execution 0.141\\\\\\\\\\\\\\\\nKapil\\\\\\\\\\\\\\\\nSample Testing 0.428 -57.982 p< 1 ×10−10 yes\\\\\\\\\\\\\\\\nNormal Execution 1.000\\\\\\\\\\\\\\\\nNihal\\\\\\\\\\\\\\\\nSample Testing 0.305 8.135 p= 1.000 no\\\\\\\\\\\\\\\\nNormal Execution 0.224\\\\\\\\\\\\\\\\nTable 5.5: Summary of success rate across agents and conditions in cartpole environment.\\\\\\\\\\\\\\\\nBased on the results, we can conclude that for the agents Kapil and Shakti (CartPole), there\\\\\\\\\\\\\\\\nis sufficient statistical evidence to affirm that the probability of success is significantly lower in\\\\\\\\\\\\\\\\nthe test case scenarios than under normal execution conditions.\\\\\\\\\\\\\\\\nNotably, even for Kapil—an agent that achieved perfect performance under normal conditions,\\\\\\\\\\\\\\\\nnever obtaining a reward below 200— the test case generator was able to identify scenarios where\\\\\\\\\\\\\\\\nfailures were possible, demonstrating its capacity to expose potential weaknesses in otherwise\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'robust policies.\\\\\\\\\\\\\\\\n5.6.2. Lunar lander\\\\\\\\\\\\\\\\nThe following section provides a summary of the vocabulary size (i.e., number of abstract classes)\\\\\\\\\\\\\\\\ngenerated for each agent in Lunar lander environment, as well the number of regions identified\\\\\\\\\\\\\\\\nas initial regions with a high probability of failure or low expected reward by the transformer.\\\\\\\\\\\\\\\\nWe also report the sample size, defined as the number of test cases generated by the Concrete\\\\\\\\\\\\\\\\nTest Case Generator.\\\\\\\\\\\\\\\\n33\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\nAgent Vocabulary Size Number of Initial Regions Sample Size\\\\\\\\\\\\\\\\nSanket 840 58 781\\\\\\\\\\\\\\\\nKhordoo 711 58 1892\\\\\\\\\\\\\\\\nRokenes 868 64 779\\\\\\\\\\\\\\\\nTable 5.6: Summary of vocabulary size, number of initial regions and sample size used for each\\\\\\\\\\\\\\\\nagent, Lunar Lander.\\\\\\\\\\\\\\\\nThe sample size represents too the total number of episodes executed for both normal execution\\\\\\\\\\\\\\\\nand simulated testing. The variation in sample size across agents arises from the design of the\\\\\\\\\\\\\\\\ntest case generator, which attempts to sample up to 100 states per risk region. However, in\\\\\\\\\\\\\\\\npractice, some abstract classes contain fewer than 100 states, limiting the number of available\\\\\\\\\\\\\\\\ntest cases.\\\\\\\\\\\\\\\\nThis effect is more pronounced in the LunarLander agents, where not all states within an\\\\\\\\\\\\\\\\nabstract class can be considered valid initial states for the environment. As a result, the effective\\\\\\\\\\\\\\\\nnumber of testable samples per region may be reduced, impacting the total sample size.\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'We now present the table of statistical comparisons based on average rewards:\\\\\\\\\\\\\\\\n34\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5.6. Evaluation and Results\\\\\\\\\\\\\\\\nAgent Mean SD SEM t-statistic P-value µtesting <µexecution\\\\\\\\\\\\\\\\nSanket\\\\\\\\\\\\\\\\nSample Testing 234.24 34.91 1.25 -2.738 p= 3.132×10−3 no\\\\\\\\\\\\\\\\nNormal Execution 239.81 44.94 1.61\\\\\\\\\\\\\\\\nKhordoo\\\\\\\\\\\\\\\\nSample Testing 200.58 71.18 1.64 -4.980 p< 3.328×10−7 yes\\\\\\\\\\\\\\\\nNormal Execution 212.07 70.77 1.63\\\\\\\\\\\\\\\\nRokenes\\\\\\\\\\\\\\\\nSample Testing 263.70 76.17 2.73 -1.487 p= 6.857×10−2 no\\\\\\\\\\\\\\\\nNormal Execution 268.92 61.64 2.21\\\\\\\\\\\\\\\\nTable 5.7: Comparison of sample testing and normal execution for each agent in Lunar Lander\\\\\\\\\\\\\\\\nenvironment.\\\\\\\\\\\\\\\\nFor the agent Khordoo (LunarLander), we can conclude that there is sufficient statistical\\\\\\\\\\\\\\\\nevidence to support the claim that the test case generator identifies regions with significantly\\\\\\\\\\\\\\\\nlower rewards.\\\\\\\\\\\\\\\\nHowever, for the agents Sanket and Rokenes, we cannot assert that the generator found regions\\\\\\\\\\\\\\\\nwith lower rewards compared to a normal execution.\\\\\\\\\\\\\\\\nWe now present the success rate comparison for all agents, where success is defined as achieving\\\\\\\\\\\\\\\\na total reward of at least 200 per episode.\\\\\\\\\\\\\\\\n35\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\nCondition Success Rate z-statistic p-value Ptesting <Pexecution\\\\\\\\\\\\\\\\nSanket\\\\\\\\\\\\\\\\nSample Testing 0.862 -4.964 p= 3.444 ×10−7 yes\\\\\\\\\\\\\\\\nNormal Execution 0.937\\\\\\\\\\\\\\\\nKhordoo\\\\\\\\\\\\\\\\nSample Testing 0.627 -7.295 p= 1.490 ×10−13 yes\\\\\\\\\\\\\\\\nNormal Execution 0.737\\\\\\\\\\\\\\\\nRokenes\\\\\\\\\\\\\\\\nSample Testing 0.910 -2.529 p= 5.713 ×10−3 yes\\\\\\\\\\\\\\\\nNormal Execution 0.944\\\\\\\\\\\\\\\\nTable 5.8: Summary of success rate across agents and conditions in Lunar Lander.\\\\\\\\\\\\\\\\nBased on the results, we can conclude that for the agents Khordoo, Sanket, and Rokenes\\\\\\\\\\\\\\\\n(LunarLander), there is sufficient statistical evidence to affirm that the probability of success is\\\\\\\\\\\\\\\\nsignificantly lower in the test case scenarios than under normal execution conditions.\\\\\\\\\\\\\\\\n5.6.3. A Small Example\\\\\\\\\\\\\\\\nIn this subsection, we present an example in which the Shakti agent fails, achieving a total reward\\\\\\\\\\\\\\\\nof 81.0. Recall that a state in the CartPole environment is represented by anndarray of four\\\\\\\\\\\\\\\\ncomponents, as described in Table 5.9:\\\\\\\\\\\\\\\\n36\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5.6. Evaluation and Results\\\\\\\\\\\\\\\\nIndex Observation Minimum Maximum\\\\\\\\\\\\\\\\n0 Cart Position −4 .8 4 .8\\\\\\\\\\\\\\\\n1 Cart Velocity −∞ ∞\\\\\\\\\\\\\\\\n2 Pole Angle −0 .418 rad (−24 ◦) 0 .418 rad (24 ◦)\\\\\\\\\\\\\\\\n3 Pole Angular Velocity −∞ ∞\\\\\\\\\\\\\\\\nTable 5.9: Observation space of the CartPole environment: anndarray of shape(4,).\\\\\\\\\\\\\\\\nWe initialize the environment to the following state, generated by our test generator:\\\\\\\\\\\\\\\\nx0 =\\\\\\\\\\\\\\\\n[\\\\\\\\\\\\\\\\n−0.73915684 −0.89601650 −0.03282847 0 .23753740\\\\\\\\\\\\\\\\n]\\\\\\\\\\\\\\\\n.\\\\\\\\\\\\\\\\nFigure 5.9 illustrates this initial configuration:\\\\\\\\\\\\\\\\nFigure 5.9: InitialstateoftheCartPoleenvironmentwithcartposition −0.73915684, cartvelocity\\\\\\\\\\\\\\\\n−0.89601650, pole angle−0.03282847, and pole angular velocity0.23753740.\\\\\\\\\\\\\\\\nFigure 5.10 shows the final state of the episode, wherein the agent has pushed the cart beyond\\\\\\\\\\\\\\\\nthe left boundary:\\\\\\\\\\\\\\\\n37\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\nFigure 5.10: Final state of the CartPole environment with cart position−2.41205427, cart veloc-\\\\\\\\\\\\\\\\nity −1.42968531, pole angle−0.06067818, and pole angular velocity−0.02501499.\\\\\\\\\\\\\\\\nConsequently, the agent performs more “left” actions (0) than “right” actions (1), demon-\\\\\\\\\\\\\\\\nstrating an inability to counteract the tendency toward negative velocity. Below, we present the\\\\\\\\\\\\\\\\ncomplete sequence of actions taken during the episode:\\\\\\\\\\\\\\\\n[−0.73915684 −0.8960165 −0.03282847 0.2375374 ] 1 1.0 False\\\\\\\\\\\\\\\\n[−0.75707717 −0.70044128 −0.02807772 −0.06531717] 1 1.0 False\\\\\\\\\\\\\\\\n[−0.771086 −0.50492826 −0.02938406 −0.36672488] 0 1.0 False\\\\\\\\\\\\\\\\n[−0.78118456 −0.6996206 −0.03671856 −0.0834501 ] 0 1.0 False\\\\\\\\\\\\\\\\n[−0.79517697 −0.89419749 −0.03838756 0.19742567] 1 1.0 False\\\\\\\\\\\\\\\\n[−0.81306092 −0.69854809 −0.03443905 −0.1071154 ] 0 1.0 False\\\\\\\\\\\\\\\\n[−0.82703189 −0.89316003 −0.03658136 0.17450633] 1 1.0 False\\\\\\\\\\\\\\\\n[−0.84489509 −0.69753415 −0.03309123 −0.12948869] 0 1.0 False\\\\\\\\\\\\\\\\n[−0.85884577 −0.89216682 −0.03568101 0.15257344] 1 1.0 False\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'[−0.87668911 −0.69655261 −0.03262954 −0.1511491 ] 0 1.0 False\\\\\\\\\\\\\\\\n[−0.89062016 −0.89119251 −0.03565252 0.13106395] 1 1.0 False\\\\\\\\\\\\\\\\n[−0.90844401 −0.69557846 −0.03303124 −0.17265027] 0 1.0 False\\\\\\\\\\\\\\\\n38\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5.6. Evaluation and Results\\\\\\\\\\\\\\\\n[−0.92235558 −0.89021247 −0.03648425 0.10943207] 1 1.0 False\\\\\\\\\\\\\\\\n[−0.94015983 −0.69458722 −0.0342956 −0.19453452] 0 1.0 False\\\\\\\\\\\\\\\\n[−0.95405157 −0.88920223 −0.03818629 0.0871354 ] 1 1.0 False\\\\\\\\\\\\\\\\n[−0.97183562 −0.69355431 −0.03644359 −0.21734658] 0 1.0 False\\\\\\\\\\\\\\\\n[−0.9857067 −0.88813685 −0.04079052 0.06362139] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.00346944 −1.08265094 −0.03951809 0.34316074] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.02512246 −0.88698972 −0.03265488 0.03828275] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.04286225 −0.69141508 −0.03188922 −0.26452163] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.05669055 −0.8860677 −0.03717965 0.01793501] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.07441191 −1.08063728 −0.03682095 0.29865938] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.09602465 −0.88501034 −0.03084776 −0.00540505] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.11372486 −0.68945988 −0.03095587 −0.30765899] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.12751406 −0.88412737 −0.03710905 −0.02489722] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.14519661 −1.07869804 −0.03760699 0.2558503 ] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.16677057 −0.8830599 −0.03248998 −0.04845328] 1 1.0 False\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'[−1.18443176 −0.68748749 −0.03345905 −0.35120743] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.19818151 −0.88211804 −0.0404832 −0.06926014] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.21582387 −1.07663691 −0.0418684 0.21038029] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.23735661 −0.8809421 −0.0376608 −0.09521039] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.25497546 −1.0755046 −0.039565 0.18535677] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.27648555 −0.87983955 −0.03585787 −0.1195402 ] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.29408234 −1.07442989 −0.03824867 0.16161772] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.31557094 −0.87878184 −0.03501632 −0.14288208] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.33314657 −1.07338526 −0.03787396 0.13855142] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.35461428 −0.87774189 −0.03510293 −0.16583546] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.37216912 −1.07234423 −0.03841964 0.11557008] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.393616 −0.87669345 −0.03610824 −0.18898212] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.41114987 −1.07128071 −0.03988788 0.092095 ] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.43257548 −0.87561041 −0.03804598 −0.21290092] 0 1.0 False\\\\\\\\\\\\\\\\n39\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\n[−1.45008769 −1.07016834 −0.042304 0.06754196] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.47149106 −0.87446621 −0.04095316 −0.23818226] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.48898038 −1.0689799 −0.0457168 0.04130677] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.51035998 −0.87323321 −0.04489067 −0.26544255] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.52782465 −1.06768666 −0.05019952 0.01275036] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.54917838 −1.26205408 −0.04994451 0.28918176] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.57441946 −1.06625682 −0.04416088 −0.01882548] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.5957446 −0.87053029 −0.04453739 −0.32510813] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.6131552 −1.06499074 −0.05103955 −0.04679636] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.63445502 −1.25934507 −0.05197548 0.22935638] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.65964192 −1.0635204 −0.04738835 −0.07925787] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.68091233 −1.25793212 −0.04897351 0.19810537] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.70607097 −1.06214514 −0.0450114 −0.10961544] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.72731387 −1.25659416 −0.04720371 0.16853378] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.75244575 −1.06082944 −0.04383303 −0.13865894] 1 1.0 False\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'[−1.77366234 −0.86510799 −0.04660621 −0.44484192] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.7909645 −1.05954062 −0.05550305 −0.16720693] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.81215532 −1.25382597 −0.05884719 0.10746281] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.83723184 −1.05791226 −0.05669793 −0.20319016] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.85839008 −1.25217946 −0.06076174 0.07108213] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.88343367 −1.05624141 −0.05934009 −0.24013553] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.9045585 −1.25046768 −0.0641428 0.03325534] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.92956785 −1.44461395 −0.0634777 0.30503082] 1 1.0 False\\\\\\\\\\\\\\\\n[−1.95846013 −1.24864754 −0.05737708 −0.00697668] 0 1.0 False\\\\\\\\\\\\\\\\n[−1.98343308 −1.44290169 −0.05751661 0.26706543] 1 1.0 False\\\\\\\\\\\\\\\\n[−2.01229112 −1.24700805 −0.05217531 −0.04318969] 0 1.0 False\\\\\\\\\\\\\\\\n[−2.03723128 −1.44134451 −0.0530391 0.23258573] 1 1.0 False\\\\\\\\\\\\\\\\n[−2.06605817 −1.24550639 −0.04838738 −0.07634453] 1 1.0 False\\\\\\\\\\\\\\\\n[−2.09096829 −1.04972537 −0.04991428 −0.38389269] 0 1.0 False\\\\\\\\\\\\\\\\n40\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5.6. Evaluation and Results\\\\\\\\\\\\\\\\n[−2.1119628 −1.24410443 −0.05759213 −0.10735593] 0 1.0 False\\\\\\\\\\\\\\\\n[−2.13684489 −1.43835581 −0.05973925 0.16661531] 1 1.0 False\\\\\\\\\\\\\\\\n[−2.16561201 −1.24243186 −0.05640694 −0.14429925] 0 1.0 False\\\\\\\\\\\\\\\\n[−2.19046064 −1.43670254 −0.05929293 0.13006845] 1 1.0 False\\\\\\\\\\\\\\\\n[−2.21919469 −1.24078355 −0.05669156 −0.18071551] 0 1.0 False\\\\\\\\\\\\\\\\n[−2.24401037 −1.43505037 −0.06030587 0.09355819] 1 1.0 False\\\\\\\\\\\\\\\\n[−2.27271137 −1.23911823 −0.0584347 −0.21752494] 0 1.0 False\\\\\\\\\\\\\\\\n[−2.29749374 −1.43335827 −0.0627852 0.05616779] 1 1.0 False\\\\\\\\\\\\\\\\n[−2.3261609 −1.23739489 −0.06166185 −0.25564483] 0 1.0 False\\\\\\\\\\\\\\\\n[−2.3509088 −1.43158473 −0.06677474 0.01696925] 0 1.0 False\\\\\\\\\\\\\\\\n[−2.3795405 −1.62568867 −0.06643536 0.28785909] 1 1.0 True\\\\\\\\\\\\\\\\n[−2.41205427 −1.42968531 −0.06067818 −0.02501499] total_reward : 81.0\\\\\\\\\\\\\\\\n5.6.4. Threats to Validity\\\\\\\\\\\\\\\\nThreats to external validityOur experiment aims to validate the use of the test case gener-\\\\\\\\\\\\\\\\nator for evaluating reinforcement learning agents. We selected a total of six agents from two\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'different environments—CartPole and LunarLander—ranked at different positions in the public\\\\\\\\\\\\\\\\nleaderboard. This selection strategy was designed to capture a range of agent performances\\\\\\\\\\\\\\\\nand architectures, thereby reducing the risk of bias and improving the generalizability of our\\\\\\\\\\\\\\\\nfindings.\\\\\\\\\\\\\\\\nHowever, we observed that in some cases, the agents’ behavior did not align with expectations\\\\\\\\\\\\\\\\nbased on their leaderboard rankings. The most notable example was the agent Shakti, which,\\\\\\\\\\\\\\\\ndespite being ranked highest among the CartPole agents, showed the worst performance during\\\\\\\\\\\\\\\\nnormal execution, with a success rate of only 14%.\\\\\\\\\\\\\\\\nSimilarly, although the selected LunarLander agents were distributed across different leader-\\\\\\\\\\\\\\\\nboard positions, all of them achieved average rewards above 200 in our experiments. In partic-\\\\\\\\\\\\\\\\nular, the agent Rokenes achieved both the highest average reward and the highest success rate,\\\\\\\\\\\\\\\\ndespite not being the top-ranked agent in the public leaderboard.\\\\\\\\\\\\\\\\n41\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'5. Experimental Design\\\\\\\\\\\\\\\\nThese discrepancies suggest that leaderboard rankings may not fully reflect agent robustness un-\\\\\\\\\\\\\\\\nder different testing conditions, and highlight the importance of evaluating agents using diverse\\\\\\\\\\\\\\\\nand controlled scenarios.\\\\\\\\\\\\\\\\nThreats to internal validityOne potential threat to internal validity arises from the specific\\\\\\\\\\\\\\\\nhandling required for the LunarLander environment. In this case, it was necessary to validate\\\\\\\\\\\\\\\\nwhether the sampled states could be considered valid initial states, since the environment can\\\\\\\\\\\\\\\\nonly be initialized from a limited region of the state space.\\\\\\\\\\\\\\\\nBy default, LunarLander starts from normalized coordinates(x= 0.0, y≈1.41), which corre-\\\\\\\\\\\\\\\\nspond to approximately(10.0, 13.33) in Box2D coordinates. Any perturbation or state sampled\\\\\\\\\\\\\\\\noutside of this valid initialization region would not be accepted by the environment as a true\\\\\\\\\\\\\\\\nstarting state.\\\\\\\\\\\\\\\\nAs a result, the number of valid samples per region was often reduced, leading to smaller overall\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'sample sizes for these agents. This constraint may have introduced a bias in the testing data or\\\\\\\\\\\\\\\\naffected the statistical power of the results in the LunarLander experiments.\\\\\\\\\\\\\\\\n42\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'43\\\\\\\\\\\\\\\\nChapter 6\\\\\\\\\\\\\\\\nConclusion, Limitations and Future Work\\\\\\\\\\\\\\\\n6.1. Conclusion\\\\\\\\\\\\\\\\nThis thesis explores the problem of testing reinforcement learning agents through abstract classes\\\\\\\\\\\\\\\\nand partitions.\\\\\\\\\\\\\\\\nWe proposedMarTest-Pipeline, an implementation that generates abstract representations of\\\\\\\\\\\\\\\\nagent behavior, uses a transformer conditioned on reward and failure probability to identify\\\\\\\\\\\\\\\\nfailure-prone regions, and constructs targeted test cases from those regions.\\\\\\\\\\\\\\\\nOur results demonstrated that this approach can uncover regions of reduced performance in\\\\\\\\\\\\\\\\nmultiple agents, including those with perfect scores under normal execution. These findings\\\\\\\\\\\\\\\\nsupport the idea that standard agent evaluations may overlook critical edge cases and that\\\\\\\\\\\\\\\\nsymbolic representations—when combined with generative models—can enhance the testing and\\\\\\\\\\\\\\\\nunderstanding of agent behavior.\\\\\\\\\\\\\\\\nBeyondtheimplementation, thisworkproposesabroaderconceptualview: thattheinteraction\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'between an agent and its environment can be seen as a form of emergent language. Reward,\\\\\\\\\\\\\\\\nin this context, is not only a signal of performance, but a reflection of how well the agent has\\\\\\\\\\\\\\\\ninternalized a language and this thesis constitutes the first step in a research direction.\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'6. Conclusion, Limitations and Future Work\\\\\\\\\\\\\\\\n6.2. Limitations\\\\\\\\\\\\\\\\nThis work presents a novel approach to testing reinforcement learning agents using a symbolic\\\\\\\\\\\\\\\\ngenerator based on transformer models and abstract state-action representations. While the\\\\\\\\\\\\\\\\nresults are promising and demonstrate the generator’s ability to identify failure-prone regions,\\\\\\\\\\\\\\\\nseveral limitations must be acknowledged:\\\\\\\\\\\\\\\\n• Dependence on initial state space constraints:In the LunarLander environment,\\\\\\\\\\\\\\\\nonly a limited subset of states can be used as valid initial states. This required an explicit\\\\\\\\\\\\\\\\nvalidation step to ensure sampled states could be used for environment initialization. As\\\\\\\\\\\\\\\\na consequence, the effective sample size for LunarLander agents was reduced, potentially\\\\\\\\\\\\\\\\nlimiting the statistical power of the results.\\\\\\\\\\\\\\\\n• Uneven sample sizes across agents:The test case generator attempts to sample up to\\\\\\\\\\\\\\\\n100 states per high-risk region. However, some abstract classes contained fewer than 100\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'states in both CartPole and LunarLander environments, resulting in variation in sample\\\\\\\\\\\\\\\\nsizes across agents and environments. This may have introduced some imbalance in the\\\\\\\\\\\\\\\\nstatistical comparisons.\\\\\\\\\\\\\\\\n• Fixed abstraction level:All experiments were conducted using a fixed abstraction level\\\\\\\\\\\\\\\\nof d = 4 . While this value provided a reasonable trade-off between expressiveness and\\\\\\\\\\\\\\\\ngenerality, it is possible that a different abstraction depth might yield better or more\\\\\\\\\\\\\\\\ninterpretable results depending on the agent and environment.\\\\\\\\\\\\\\\\n• Limited evaluation scope: The approach was tested only on agents from two envi-\\\\\\\\\\\\\\\\nronments: CartPole and LunarLander. Although these environments differ in complexity\\\\\\\\\\\\\\\\nand dynamics, further evaluation on more diverse tasks—particularly continuous control\\\\\\\\\\\\\\\\nand high-dimensional environments—is needed to assess the general applicability of the\\\\\\\\\\\\\\\\nmethod.\\\\\\\\\\\\\\\\n• Transformer conditioning is limited:The transformer model was trained conditioned\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'only on the average reward and failure probability. While this was sufficient for the current\\\\\\\\\\\\\\\\nexperiments, future versions could incorporate richer contextual information (e.g., policy\\\\\\\\\\\\\\\\n44\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'6.3. Future Work\\\\\\\\\\\\\\\\nentropy, time horizon, or agent architecture) to improve the diversity and precision of\\\\\\\\\\\\\\\\ngenerated test cases.\\\\\\\\\\\\\\\\n• Mismatchwithleaderboardexpectations: Insomecases, theperformanceobservedin\\\\\\\\\\\\\\\\nnormal execution did not match the agent’s leaderboard ranking. For example, the Shakti\\\\\\\\\\\\\\\\nagent, which held the top rank among CartPole submissions, showed the lowest success\\\\\\\\\\\\\\\\nrate in our experiments. This highlights the need to interpret leaderboard positions with\\\\\\\\\\\\\\\\ncaution and emphasizes the value of independent and reproducible testing frameworks.\\\\\\\\\\\\\\\\n6.3. Future Work\\\\\\\\\\\\\\\\nOne potential extension is to design abstraction mechanisms that are adaptive—i.e., where the\\\\\\\\\\\\\\\\nabstraction depth is learned based on agent behavior or performance metrics. Another important\\\\\\\\\\\\\\\\ndirection for future work is to implement alternative definitions of abstract classes. This is\\\\\\\\\\\\\\\\nespecially relevant, as it may provide better insight into the interaction between the agent and the\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'environment. Lastly, incorporating richer conditioning signals into the Transformer, or exploring\\\\\\\\\\\\\\\\narchitectures specifically tailored for causal reasoning in RL environments, may improve the\\\\\\\\\\\\\\\\nquality of the generated test cases and help uncover more nuanced weaknesses in agent policies.\\\\\\\\\\\\\\\\nThis work was guided by the intuition thatreward may be a consequence of a shared language\\\\\\\\\\\\\\\\nbetween the agent and the environment. While this idea is still in an early and exploratory stage,\\\\\\\\\\\\\\\\nit suggests several possible directions for future research.\\\\\\\\\\\\\\\\nOne such direction is to investigate the relationship between the level of abstraction used\\\\\\\\\\\\\\\\nto represent agent-environment interactions and the rewards obtained. It may be possible to\\\\\\\\\\\\\\\\nformulate an optimization process—either analytical or learned—that selects the abstraction\\\\\\\\\\\\\\\\nlevel most appropriate for a given task or agent architecture.\\\\\\\\\\\\\\\\nAnother line of inquiry involves a closer integration between abstract symbolic representations\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'and transformer-based models. Rather than conditioning the transformer on simple reward\\\\\\\\\\\\\\\\nstatistics, future models could be trained to evaluate or even generate abstract behavior trajec-\\\\\\\\\\\\\\\\ntories as structured linguistic sequences—potentially assigning meaning and structure to patterns\\\\\\\\\\\\\\\\nof interaction.\\\\\\\\\\\\\\\\n45\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'6. Conclusion, Limitations and Future Work\\\\\\\\\\\\\\\\nAlthough preliminary, these ideas point to a broader aspiration: to better understand the role\\\\\\\\\\\\\\\\nof representation, communication, and interpretation in reinforcement learning. By approaching\\\\\\\\\\\\\\\\nthe agent-environment loop as a form of emergent communication, we hope to take small steps\\\\\\\\\\\\\\\\ntoward a more unified view of learning, abstraction, and meaning.\\\\\\\\\\\\\\\\n46\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n47\\\\\\\\\\\\\\\\nChapter A\\\\\\\\\\\\\\\\nAppendix\\\\\\\\\\\\\\\\nA.1. MartTest Pipeline Requirements\\\\\\\\\\\\\\\\nabsl -py ==1.4.0\\\\\\\\\\\\\\\\naccelerate ==1.6.0\\\\\\\\\\\\\\\\nanyio ==3.6.2\\\\\\\\\\\\\\\\nargon2 - cffi ==21.3.0\\\\\\\\\\\\\\\\nargon2 -cffi - bindings ==21.2.0\\\\\\\\\\\\\\\\narrow ==1.2.3\\\\\\\\\\\\\\\\nasttokens ==2.2.1\\\\\\\\\\\\\\\\nastunparse ==1.6.3\\\\\\\\\\\\\\\\nattrs ==22.2.0\\\\\\\\\\\\\\\\nBabel ==2.11.0\\\\\\\\\\\\\\\\nbackcall ==0.2.0\\\\\\\\\\\\\\\\nbeautifulsoup4 ==4.11.1\\\\\\\\\\\\\\\\nbleach ==5.0.1\\\\\\\\\\\\\\\\nbox2d -py ==2.3.5\\\\\\\\\\\\\\\\ncachetools ==5.2.1\\\\\\\\\\\\\\\\ncertifi ==2022.12.7\\\\\\\\\\\\\\\\ncffi ==1.15.1\\\\\\\\\\\\\\\\ncharset - normalizer ==3.0.1\\\\\\\\\\\\\\\\ncloudpickle ==3.1.1\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'A. Appendix\\\\\\\\\\\\\\\\ncomm ==0.1.2\\\\\\\\\\\\\\\\ncontourpy ==1.0.7\\\\\\\\\\\\\\\\ncycler ==0.11.0\\\\\\\\\\\\\\\\ndebugpy ==1.6.5\\\\\\\\\\\\\\\\ndecorator ==5.1.1\\\\\\\\\\\\\\\\ndefusedxml ==0.7.1\\\\\\\\\\\\\\\\nentrypoints ==0.4\\\\\\\\\\\\\\\\nexecuting ==1.2.0\\\\\\\\\\\\\\\\nfastjsonschema ==2.16.2\\\\\\\\\\\\\\\\nfilelock ==3.18.0\\\\\\\\\\\\\\\\nflatbuffers ==23.1.4\\\\\\\\\\\\\\\\nfonttools ==4.38.0\\\\\\\\\\\\\\\\nfqdn ==1.5.1\\\\\\\\\\\\\\\\nfsspec ==2025.3.2\\\\\\\\\\\\\\\\ngast ==0.4.0\\\\\\\\\\\\\\\\ngoogle - auth ==2.16.0\\\\\\\\\\\\\\\\ngoogle -auth - oauthlib ==0.4.6\\\\\\\\\\\\\\\\ngoogle - pasta ==0.2.0\\\\\\\\\\\\\\\\ngrpcio ==1.51.1\\\\\\\\\\\\\\\\ngym ==0.26.2\\\\\\\\\\\\\\\\ngym - notices ==0.0.8\\\\\\\\\\\\\\\\nh5py ==3.7.0\\\\\\\\\\\\\\\\nhuggingface - hub ==0.30.2\\\\\\\\\\\\\\\\nidna ==3.4\\\\\\\\\\\\\\\\nipykernel ==6.20.2\\\\\\\\\\\\\\\\nipython ==8.8.0\\\\\\\\\\\\\\\\nipython - genutils ==0.2.0\\\\\\\\\\\\\\\\nisoduration ==20.11.0\\\\\\\\\\\\\\\\njedi ==0.18.2\\\\\\\\\\\\\\\\nJinja2 ==3.1.2\\\\\\\\\\\\\\\\njoblib ==1.4.2\\\\\\\\\\\\\\\\njson5 ==0.9.11\\\\\\\\\\\\\\\\n48\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'A.1. MartTest Pipeline Requirements\\\\\\\\\\\\\\\\njsonpointer ==2.3\\\\\\\\\\\\\\\\njsonschema ==4.17.3\\\\\\\\\\\\\\\\njupyter - events ==0.6.3\\\\\\\\\\\\\\\\njupyter_client ==7.4.9\\\\\\\\\\\\\\\\njupyter_core ==5.1.3\\\\\\\\\\\\\\\\njupyter_server ==2.1.0\\\\\\\\\\\\\\\\njupyter_server_terminals ==0.4.4\\\\\\\\\\\\\\\\njupyterlab ==3.5.2\\\\\\\\\\\\\\\\njupyterlab - pygments ==0.2.2\\\\\\\\\\\\\\\\njupyterlab_server ==2.19.0\\\\\\\\\\\\\\\\nkeras ==2.11.0\\\\\\\\\\\\\\\\nkiwisolver ==1.4.4\\\\\\\\\\\\\\\\nlibclang ==15.0.6.1\\\\\\\\\\\\\\\\nMarkdown ==3.4.1\\\\\\\\\\\\\\\\nMarkupSafe ==2.1.1\\\\\\\\\\\\\\\\nmatplotlib ==3.6.3\\\\\\\\\\\\\\\\nmatplotlib - inline ==0.1.6\\\\\\\\\\\\\\\\nmistune ==2.0.4\\\\\\\\\\\\\\\\nmpmath ==1.3.0\\\\\\\\\\\\\\\\nnbclassic ==0.4.8\\\\\\\\\\\\\\\\nnbclient ==0.7.2\\\\\\\\\\\\\\\\nnbconvert ==7.2.8\\\\\\\\\\\\\\\\nnbformat ==5.7.3\\\\\\\\\\\\\\\\nnest - asyncio ==1.5.6\\\\\\\\\\\\\\\\nnetworkx ==3.3\\\\\\\\\\\\\\\\nnotebook ==6.5.2\\\\\\\\\\\\\\\\nnotebook_shim ==0.2.2\\\\\\\\\\\\\\\\nnumpy ==1.24.1\\\\\\\\\\\\\\\\nnvidia - cublas - cu11 ==11.10.3.66\\\\\\\\\\\\\\\\nnvidia -cuda - nvrtc - cu11 ==11.7.99\\\\\\\\\\\\\\\\nnvidia -cuda - runtime - cu11 ==11.7.99\\\\\\\\\\\\\\\\nnvidia - cudnn - cu11 ==8.5.0.96\\\\\\\\\\\\\\\\n49\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'A. Appendix\\\\\\\\\\\\\\\\noauthlib ==3.2.2\\\\\\\\\\\\\\\\nopt - einsum ==3.3.0\\\\\\\\\\\\\\\\npackaging ==23.0\\\\\\\\\\\\\\\\npandas ==2.2.3\\\\\\\\\\\\\\\\npandocfilters ==1.5.0\\\\\\\\\\\\\\\\nparso ==0.8.3\\\\\\\\\\\\\\\\npexpect ==4.8.0\\\\\\\\\\\\\\\\npickleshare ==0.7.5\\\\\\\\\\\\\\\\nPillow ==9.4.0\\\\\\\\\\\\\\\\nplatformdirs ==2.6.2\\\\\\\\\\\\\\\\nprometheus - client ==0.15.0\\\\\\\\\\\\\\\\nprompt - toolkit ==3.0.36\\\\\\\\\\\\\\\\nprotobuf ==3.19.6\\\\\\\\\\\\\\\\npsutil ==5.9.4\\\\\\\\\\\\\\\\nptyprocess ==0.7.0\\\\\\\\\\\\\\\\npure -eval ==0.2.2\\\\\\\\\\\\\\\\npyasn1 ==0.4.8\\\\\\\\\\\\\\\\npyasn1 - modules ==0.2.8\\\\\\\\\\\\\\\\npycparser ==2.21\\\\\\\\\\\\\\\\npygame ==2.1.0\\\\\\\\\\\\\\\\nPygments ==2.14.0\\\\\\\\\\\\\\\\npyparsing ==3.0.9\\\\\\\\\\\\\\\\npyrsistent ==0.19.3\\\\\\\\\\\\\\\\npython - dateutil ==2.8.2\\\\\\\\\\\\\\\\npython -json - logger ==2.0.4\\\\\\\\\\\\\\\\npytz ==2022.7.1\\\\\\\\\\\\\\\\nPyYAML ==6.0\\\\\\\\\\\\\\\\npyzmq ==25.0.0\\\\\\\\\\\\\\\\nregex ==2024.11.6\\\\\\\\\\\\\\\\nrequests ==2.28.2\\\\\\\\\\\\\\\\nrequests - oauthlib ==1.3.1\\\\\\\\\\\\\\\\nrfc3339 - validator ==0.1.4\\\\\\\\\\\\\\\\n50\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'A.1. MartTest Pipeline Requirements\\\\\\\\\\\\\\\\nrfc3986 - validator ==0.1.1\\\\\\\\\\\\\\\\nrsa ==4.9\\\\\\\\\\\\\\\\nsafetensors ==0.5.3\\\\\\\\\\\\\\\\nscikit - learn ==1.6.1\\\\\\\\\\\\\\\\nscipy ==1.15.2\\\\\\\\\\\\\\\\nSend2Trash ==1.8.0\\\\\\\\\\\\\\\\nsix ==1.16.0\\\\\\\\\\\\\\\\nsniffio ==1.3.0\\\\\\\\\\\\\\\\nsoupsieve ==2.3.2. post1\\\\\\\\\\\\\\\\nstack - data ==0.6.2\\\\\\\\\\\\\\\\nswig ==4.3.0\\\\\\\\\\\\\\\\nsympy ==1.13.3\\\\\\\\\\\\\\\\ntensorboard ==2.11.2\\\\\\\\\\\\\\\\ntensorboard -data - server ==0.6.1\\\\\\\\\\\\\\\\ntensorboard - plugin - wit ==1.8.1\\\\\\\\\\\\\\\\ntensorflow ==2.11.0\\\\\\\\\\\\\\\\ntensorflow - estimator ==2.11.0\\\\\\\\\\\\\\\\ntensorflow -io -gcs - filesystem ==0.29.0\\\\\\\\\\\\\\\\ntermcolor ==2.2.0\\\\\\\\\\\\\\\\nterminado ==0.17.1\\\\\\\\\\\\\\\\nthreadpoolctl ==3.6.0\\\\\\\\\\\\\\\\ntinycss2 ==1.2.1\\\\\\\\\\\\\\\\ntokenizers ==0.21.1\\\\\\\\\\\\\\\\ntomli ==2.0.1\\\\\\\\\\\\\\\\ntorch ==2.1.2+ cu118\\\\\\\\\\\\\\\\ntorchaudio ==2.1.2+ cu118\\\\\\\\\\\\\\\\ntorchvision ==0.16.2+ cu118\\\\\\\\\\\\\\\\ntornado ==6.2\\\\\\\\\\\\\\\\ntqdm ==4.67.1\\\\\\\\\\\\\\\\ntraitlets ==5.8.1\\\\\\\\\\\\\\\\ntransformers ==4.51.3\\\\\\\\\\\\\\\\ntriton ==2.1.0\\\\\\\\\\\\\\\\n51\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'A. Appendix\\\\\\\\\\\\\\\\ntyping_extensions ==4.4.0\\\\\\\\\\\\\\\\ntzdata ==2025.1\\\\\\\\\\\\\\\\nuri - template ==1.2.0\\\\\\\\\\\\\\\\nurllib3 ==1.26.14\\\\\\\\\\\\\\\\nwcwidth ==0.2.6\\\\\\\\\\\\\\\\nwebcolors ==1.12\\\\\\\\\\\\\\\\nwebencodings ==0.5.1\\\\\\\\\\\\\\\\nwebsocket - client ==1.4.2\\\\\\\\\\\\\\\\nWerkzeug ==2.2.2\\\\\\\\\\\\\\\\nwrapt ==1.14.1\\\\\\\\\\\\\\\\nSnippet A.1: Paquetes y versiones\\\\\\\\\\\\\\\\nA.2. Conditioned Transformer Model CondGPT2\\\\\\\\\\\\\\\\nCondGPT2 (\\\\\\\\\\\\\\\\n( transformer ): GPT2Model (\\\\\\\\\\\\\\\\n( wte ): Embedding (996 , 1024)\\\\\\\\\\\\\\\\n( wpe ): Embedding (1024 , 1024)\\\\\\\\\\\\\\\\n( drop ): Dropout (p =0.1 , inplace = False )\\\\\\\\\\\\\\\\n(h): ModuleList (\\\\\\\\\\\\\\\\n(0 -11) : 12 x GPT2Block (\\\\\\\\\\\\\\\\n( ln_1 ): LayerNorm ((1024 ,) , eps =1e -05 , elementwise_affine = True )\\\\\\\\\\\\\\\\n( attn ): GPT2Attention (\\\\\\\\\\\\\\\\n( c_attn ): Conv1D (nf =3072 , nx =1024)\\\\\\\\\\\\\\\\n( c_proj ): Conv1D (nf =1024 , nx =1024)\\\\\\\\\\\\\\\\n( attn_dropout ): Dropout (p =0.1 , inplace = False )\\\\\\\\\\\\\\\\n( resid_dropout ): Dropout (p =0.1 , inplace = False )\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\n( ln_2 ): LayerNorm ((1024 ,) , eps =1e -05 , elementwise_affine = True )\\\\\\\\\\\\\\\\n( mlp ): GPT2MLP (\\\\\\\\\\\\\\\\n( c_fc ): Conv1D (nf =4096 , nx =1024)\\\\\\\\\\\\\\\\n52\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'A.3. Requirements Shakti\\\\\\\\\\\\\\\\n( c_proj ): Conv1D (nf =1024 , nx =4096)\\\\\\\\\\\\\\\\n( act ): NewGELUActivation ()\\\\\\\\\\\\\\\\n( dropout ): Dropout (p =0.1 , inplace = False )\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\n( ln_f ): LayerNorm ((1024 ,) , eps =1e -05 , elementwise_affine = True )\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\n( lm_head ): Linear ( in_features =1024 , out_features =996 , bias = False )\\\\\\\\\\\\\\\\n( cond_proj ): Sequential (\\\\\\\\\\\\\\\\n(0) : Linear ( in_features =2 , out_features =1024 , bias = True )\\\\\\\\\\\\\\\\n(1) : ReLU ()\\\\\\\\\\\\\\\\n(2) : Linear ( in_features =1024 , out_features =1024 , bias = True )\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\nA.3. Requirements Shakti\\\\\\\\\\\\\\\\nabsl -py ==2.1.0\\\\\\\\\\\\\\\\nastor ==0.8.1\\\\\\\\\\\\\\\\ncachetools ==4.2.4\\\\\\\\\\\\\\\\ncertifi ==2025.1.31\\\\\\\\\\\\\\\\ncharset - normalizer ==3.4.1\\\\\\\\\\\\\\\\ncloudpickle ==1.2.2\\\\\\\\\\\\\\\\ncycler ==0.11.0\\\\\\\\\\\\\\\\nfuture ==1.0.0\\\\\\\\\\\\\\\\ngast ==0.2.2\\\\\\\\\\\\\\\\ngoogle - auth ==1.35.0\\\\\\\\\\\\\\\\ngoogle -auth - oauthlib ==0.4.6\\\\\\\\\\\\\\\\ngoogle - pasta ==0.2.0\\\\\\\\\\\\\\\\ngrpcio ==1.62.3\\\\\\\\\\\\\\\\n53\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'A. Appendix\\\\\\\\\\\\\\\\ngym ==0.15.4\\\\\\\\\\\\\\\\nh5py ==3.8.0\\\\\\\\\\\\\\\\nidna ==3.10\\\\\\\\\\\\\\\\nimportlib - metadata ==6.7.0\\\\\\\\\\\\\\\\nKeras ==2.3.1\\\\\\\\\\\\\\\\nKeras - Applications ==1.0.8\\\\\\\\\\\\\\\\nKeras - Preprocessing ==1.1.2\\\\\\\\\\\\\\\\nkiwisolver ==1.4.5\\\\\\\\\\\\\\\\nMarkdown ==3.4.4\\\\\\\\\\\\\\\\nMarkupSafe ==2.1.5\\\\\\\\\\\\\\\\nmatplotlib ==3.1.2\\\\\\\\\\\\\\\\nnumpy ==1.18.1\\\\\\\\\\\\\\\\noauthlib ==3.2.2\\\\\\\\\\\\\\\\nopencv - python ==4.11.0.86\\\\\\\\\\\\\\\\nopt - einsum ==3.3.0\\\\\\\\\\\\\\\\nPillow ==9.5.0\\\\\\\\\\\\\\\\nprotobuf ==3.20.3\\\\\\\\\\\\\\\\npyasn1 ==0.5.1\\\\\\\\\\\\\\\\npyasn1 - modules ==0.3.0\\\\\\\\\\\\\\\\npyglet ==1.3.2\\\\\\\\\\\\\\\\npyparsing ==3.1.4\\\\\\\\\\\\\\\\npython - dateutil ==2.9.0. post0\\\\\\\\\\\\\\\\nPyYAML ==6.0.1\\\\\\\\\\\\\\\\nrequests ==2.31.0\\\\\\\\\\\\\\\\nrequests - oauthlib ==2.0.0\\\\\\\\\\\\\\\\nrsa ==4.9\\\\\\\\\\\\\\\\nscipy ==1.4.1\\\\\\\\\\\\\\\\nsix ==1.17.0\\\\\\\\\\\\\\\\ntensorboard ==2.1.1\\\\\\\\\\\\\\\\ntensorflow - estimator ==2.1.0\\\\\\\\\\\\\\\\ntensorflow - gpu ==2.1.0\\\\\\\\\\\\\\\\ntermcolor ==2.3.0\\\\\\\\\\\\\\\\n54\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'A.4. Requirements Kapil\\\\\\\\\\\\\\\\ntorch ==1.2.0+ cu92\\\\\\\\\\\\\\\\ntorchvision ==0.4.0+ cu92\\\\\\\\\\\\\\\\ntyping_extensions ==4.7.1\\\\\\\\\\\\\\\\nurllib3 ==2.0.7\\\\\\\\\\\\\\\\nWerkzeug ==2.2.3\\\\\\\\\\\\\\\\nwrapt ==1.16.0\\\\\\\\\\\\\\\\nzipp ==3.15.0\\\\\\\\\\\\\\\\nA.4. Requirements Kapil\\\\\\\\\\\\\\\\nabsl -py ==2.1.0\\\\\\\\\\\\\\\\nastor ==0.8.1\\\\\\\\\\\\\\\\nbackcall ==0.2.0\\\\\\\\\\\\\\\\ncloudpickle ==1.2.2\\\\\\\\\\\\\\\\ncycler ==0.11.0\\\\\\\\\\\\\\\\ndebugpy ==1.7.0\\\\\\\\\\\\\\\\ndecorator ==5.1.1\\\\\\\\\\\\\\\\nentrypoints ==0.4\\\\\\\\\\\\\\\\nfuture ==1.0.0\\\\\\\\\\\\\\\\ngast ==0.2.2\\\\\\\\\\\\\\\\ngoogle - pasta ==0.2.0\\\\\\\\\\\\\\\\ngrpcio ==1.62.3\\\\\\\\\\\\\\\\ngym ==0.14.0\\\\\\\\\\\\\\\\nh5py ==3.8.0\\\\\\\\\\\\\\\\nimportlib - metadata ==6.7.0\\\\\\\\\\\\\\\\nipykernel ==6.16.2\\\\\\\\\\\\\\\\nipython ==7.34.0\\\\\\\\\\\\\\\\njedi ==0.19.2\\\\\\\\\\\\\\\\njupyter_client ==7.4.9\\\\\\\\\\\\\\\\njupyter_core ==4.12.0\\\\\\\\\\\\\\\\nKeras ==2.3.1\\\\\\\\\\\\\\\\n55\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'A. Appendix\\\\\\\\\\\\\\\\nKeras - Applications ==1.0.8\\\\\\\\\\\\\\\\nKeras - Preprocessing ==1.1.2\\\\\\\\\\\\\\\\nkiwisolver ==1.4.5\\\\\\\\\\\\\\\\nMarkdown ==3.4.4\\\\\\\\\\\\\\\\nMarkupSafe ==2.1.5\\\\\\\\\\\\\\\\nmatplotlib ==2.2.4\\\\\\\\\\\\\\\\nmatplotlib - inline ==0.1.6\\\\\\\\\\\\\\\\nnest - asyncio ==1.6.0\\\\\\\\\\\\\\\\nnumpy ==1.21.6\\\\\\\\\\\\\\\\nopt - einsum ==3.3.0\\\\\\\\\\\\\\\\npackaging ==24.0\\\\\\\\\\\\\\\\nparso ==0.8.4\\\\\\\\\\\\\\\\npexpect ==4.9.0\\\\\\\\\\\\\\\\npickleshare ==0.7.5\\\\\\\\\\\\\\\\nprompt_toolkit ==3.0.48\\\\\\\\\\\\\\\\nprotobuf ==3.20.3\\\\\\\\\\\\\\\\npsutil ==7.0.0\\\\\\\\\\\\\\\\nptyprocess ==0.7.0\\\\\\\\\\\\\\\\npyglet ==1.3.2\\\\\\\\\\\\\\\\nPygments ==2.17.2\\\\\\\\\\\\\\\\npyparsing ==3.1.4\\\\\\\\\\\\\\\\npython - dateutil ==2.9.0. post0\\\\\\\\\\\\\\\\npytz ==2025.1\\\\\\\\\\\\\\\\nPyYAML ==6.0.1\\\\\\\\\\\\\\\\npyzmq ==26.2.1\\\\\\\\\\\\\\\\nscipy ==1.7.3\\\\\\\\\\\\\\\\nsix ==1.17.0\\\\\\\\\\\\\\\\ntensorboard ==1.15.0\\\\\\\\\\\\\\\\ntensorflow ==1.15.0\\\\\\\\\\\\\\\\ntensorflow - estimator ==1.15.1\\\\\\\\\\\\\\\\ntermcolor ==2.3.0\\\\\\\\\\\\\\\\ntornado ==6.2\\\\\\\\\\\\\\\\n56\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'A.5. Requirements Nihal\\\\\\\\\\\\\\\\ntraitlets ==5.9.0\\\\\\\\\\\\\\\\ntyping_extensions ==4.7.1\\\\\\\\\\\\\\\\nwcwidth ==0.2.13\\\\\\\\\\\\\\\\nWerkzeug ==2.2.3\\\\\\\\\\\\\\\\nwrapt ==1.16.0\\\\\\\\\\\\\\\\nzipp ==3.15.0\\\\\\\\\\\\\\\\nA.5. Requirements Nihal\\\\\\\\\\\\\\\\nabsl -py ==2.1.0\\\\\\\\\\\\\\\\nastor ==0.8.1\\\\\\\\\\\\\\\\nbackcall ==0.2.0\\\\\\\\\\\\\\\\ncachetools ==4.2.4\\\\\\\\\\\\\\\\ncertifi ==2025.1.31\\\\\\\\\\\\\\\\ncharset - normalizer ==3.4.1\\\\\\\\\\\\\\\\ncloudpickle ==1.2.2\\\\\\\\\\\\\\\\ncycler ==0.11.0\\\\\\\\\\\\\\\\ndebugpy ==1.7.0\\\\\\\\\\\\\\\\ndecorator ==5.1.1\\\\\\\\\\\\\\\\nentrypoints ==0.4\\\\\\\\\\\\\\\\nfuture ==1.0.0\\\\\\\\\\\\\\\\ngast ==0.2.2\\\\\\\\\\\\\\\\ngoogle - auth ==1.35.0\\\\\\\\\\\\\\\\ngoogle -auth - oauthlib ==0.4.6\\\\\\\\\\\\\\\\ngoogle - pasta ==0.2.0\\\\\\\\\\\\\\\\ngrpcio ==1.62.3\\\\\\\\\\\\\\\\ngym ==0.15.4\\\\\\\\\\\\\\\\nh5py ==3.8.0\\\\\\\\\\\\\\\\nidna ==3.10\\\\\\\\\\\\\\\\nimportlib - metadata ==6.7.0\\\\\\\\\\\\\\\\nipykernel ==6.16.2\\\\\\\\\\\\\\\\n57\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'A. Appendix\\\\\\\\\\\\\\\\nipython ==7.34.0\\\\\\\\\\\\\\\\njedi ==0.19.2\\\\\\\\\\\\\\\\njupyter_client ==7.4.9\\\\\\\\\\\\\\\\njupyter_core ==4.12.0\\\\\\\\\\\\\\\\nKeras - Applications ==1.0.8\\\\\\\\\\\\\\\\nKeras - Preprocessing ==1.1.2\\\\\\\\\\\\\\\\nkiwisolver ==1.4.5\\\\\\\\\\\\\\\\nMarkdown ==3.4.4\\\\\\\\\\\\\\\\nMarkupSafe ==2.1.5\\\\\\\\\\\\\\\\nmatplotlib ==3.1.2\\\\\\\\\\\\\\\\nmatplotlib - inline ==0.1.6\\\\\\\\\\\\\\\\nnest - asyncio ==1.6.0\\\\\\\\\\\\\\\\nnumpy ==1.21.6\\\\\\\\\\\\\\\\noauthlib ==3.2.2\\\\\\\\\\\\\\\\nopencv - python ==4.11.0.86\\\\\\\\\\\\\\\\nopt - einsum ==3.3.0\\\\\\\\\\\\\\\\npackaging ==24.0\\\\\\\\\\\\\\\\nparso ==0.8.4\\\\\\\\\\\\\\\\npexpect ==4.9.0\\\\\\\\\\\\\\\\npickleshare ==0.7.5\\\\\\\\\\\\\\\\nprompt_toolkit ==3.0.48\\\\\\\\\\\\\\\\nprotobuf ==3.20.3\\\\\\\\\\\\\\\\npsutil ==7.0.0\\\\\\\\\\\\\\\\nptyprocess ==0.7.0\\\\\\\\\\\\\\\\npyasn1 ==0.5.1\\\\\\\\\\\\\\\\npyasn1 - modules ==0.3.0\\\\\\\\\\\\\\\\npyglet ==1.3.2\\\\\\\\\\\\\\\\nPygments ==2.17.2\\\\\\\\\\\\\\\\npyparsing ==3.1.4\\\\\\\\\\\\\\\\npython - dateutil ==2.9.0. post0\\\\\\\\\\\\\\\\npyzmq ==26.2.1\\\\\\\\\\\\\\\\nrequests ==2.31.0\\\\\\\\\\\\\\\\n58\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'A.6. Requirements Khordoo\\\\\\\\\\\\\\\\nrequests - oauthlib ==2.0.0\\\\\\\\\\\\\\\\nrsa ==4.9\\\\\\\\\\\\\\\\nscipy ==1.4.1\\\\\\\\\\\\\\\\nsix ==1.17.0\\\\\\\\\\\\\\\\ntensorboard ==2.1.1\\\\\\\\\\\\\\\\ntensorflow - estimator ==2.1.0\\\\\\\\\\\\\\\\ntensorflow - gpu ==2.1.0\\\\\\\\\\\\\\\\ntermcolor ==2.3.0\\\\\\\\\\\\\\\\ntornado ==6.2\\\\\\\\\\\\\\\\ntraitlets ==5.9.0\\\\\\\\\\\\\\\\ntyping_extensions ==4.7.1\\\\\\\\\\\\\\\\nurllib3 ==2.0.7\\\\\\\\\\\\\\\\nwcwidth ==0.2.13\\\\\\\\\\\\\\\\nWerkzeug ==2.2.3\\\\\\\\\\\\\\\\nwrapt ==1.16.0\\\\\\\\\\\\\\\\nzipp ==3.15.0\\\\\\\\\\\\\\\\nA.6. Requirements Khordoo\\\\\\\\\\\\\\\\nbox2d -py ==2.3.8\\\\\\\\\\\\\\\\ncloudpickle ==1.2.2\\\\\\\\\\\\\\\\nfuture ==1.0.0\\\\\\\\\\\\\\\\ngym ==0.15.4\\\\\\\\\\\\\\\\nnumpy ==1.18.1\\\\\\\\\\\\\\\\nopencv - python ==4.11.0.86\\\\\\\\\\\\\\\\nPillow ==9.5.0\\\\\\\\\\\\\\\\npyglet ==1.3.2\\\\\\\\\\\\\\\\nscipy ==1.7.3\\\\\\\\\\\\\\\\nsix ==1.17.0\\\\\\\\\\\\\\\\ntorch ==1.2.0+ cu92\\\\\\\\\\\\\\\\ntorchvision ==0.4.0+ cu92\\\\\\\\\\\\\\\\n59\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'A. Appendix\\\\\\\\\\\\\\\\nA.7. Requirements Sanket\\\\\\\\\\\\\\\\nbox2d -py ==2.3.4\\\\\\\\\\\\\\\\ncloudpickle ==1.2.2\\\\\\\\\\\\\\\\nfuture ==1.0.0\\\\\\\\\\\\\\\\ngym ==0.15.4\\\\\\\\\\\\\\\\nnumpy ==1.21.6\\\\\\\\\\\\\\\\nopencv - python ==4.11.0.86\\\\\\\\\\\\\\\\nPillow ==9.5.0\\\\\\\\\\\\\\\\npyglet ==1.3.2\\\\\\\\\\\\\\\\nscipy ==1.7.3\\\\\\\\\\\\\\\\nsix ==1.17.0\\\\\\\\\\\\\\\\ntorch ==1.2.0+ cu92\\\\\\\\\\\\\\\\ntorchvision ==0.4.0+ cu92\\\\\\\\\\\\\\\\nA.8. Requirements Rokenes\\\\\\\\\\\\\\\\nabsl -py ==2.1.0\\\\\\\\\\\\\\\\nastor ==0.8.1\\\\\\\\\\\\\\\\nbox2d -py ==2.3.4\\\\\\\\\\\\\\\\ncloudpickle ==1.2.2\\\\\\\\\\\\\\\\nfuture ==1.0.0\\\\\\\\\\\\\\\\ngast ==0.6.0\\\\\\\\\\\\\\\\ngoogle - pasta ==0.2.0\\\\\\\\\\\\\\\\ngrpcio ==1.62.3\\\\\\\\\\\\\\\\ngym ==0.15.4\\\\\\\\\\\\\\\\nh5py ==3.8.0\\\\\\\\\\\\\\\\nimportlib - metadata ==6.7.0\\\\\\\\\\\\\\\\nKeras - Applications ==1.0.8\\\\\\\\\\\\\\\\nKeras - Preprocessing ==1.1.2\\\\\\\\\\\\\\\\nMarkdown ==3.4.4\\\\\\\\\\\\\\\\nMarkupSafe ==2.1.5\\\\\\\\\\\\\\\\n60\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nA.8. Requirements Rokenes\\\\\\\\\\\\\\\\nnumpy ==1.21.6\\\\\\\\\\\\\\\\nopencv - python ==4.11.0.86\\\\\\\\\\\\\\\\nprotobuf ==3.20.3\\\\\\\\\\\\\\\\npyglet ==1.3.2\\\\\\\\\\\\\\\\nscipy ==1.7.3\\\\\\\\\\\\\\\\nsix ==1.17.0\\\\\\\\\\\\\\\\ntensorboard ==1.14.0\\\\\\\\\\\\\\\\ntensorflow ==1.14.0\\\\\\\\\\\\\\\\ntensorflow - estimator ==1.14.0\\\\\\\\\\\\\\\\ntermcolor ==2.3.0\\\\\\\\\\\\\\\\ntyping_extensions ==4.7.1\\\\\\\\\\\\\\\\nWerkzeug ==2.2.3\\\\\\\\\\\\\\\\nwrapt ==1.16.0\\\\\\\\\\\\\\\\nzipp ==3.15.0\\\\\\\\\\\\\\\\n61\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'63\\\\\\\\\\\\\\\\nBibliography\\\\\\\\\\\\\\\\nThe references are sorted alphabetically by first author.\\\\\\\\\\\\\\\\n[1] Lionel C. Briand Fellow IEEE Mojtaba Bagherzadeh Amirhossein Zolfagharian, Manel Ab-\\\\\\\\\\\\\\\\ndellatif and Ramesh S.A Search-Based Testing Approach for Deep Reinforcement Learning\\\\\\\\\\\\\\\\nAgents. IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, vol. 49, no. 7 edition,\\\\\\\\\\\\\\\\n2023. ISBN 0-201-37921-X.\\\\\\\\\\\\\\\\n[2] Matteo Biagiola and Paolo Tonella.Testing the Plasticity of Reinforcement Learning Based\\\\\\\\\\\\\\\\nSystems. Università della Svizzera italiana, Switzerland, revised edition, 2022. ISBN 0-201-\\\\\\\\\\\\\\\\n37921-X.\\\\\\\\\\\\\\\\n[3] Matteo Biagiola and Paolo Tonella.Testing of Deep Reinforcement Learning Agents with\\\\\\\\\\\\\\\\nSurrogate Models. Università della Svizzera italiana, Switzerland, revised edition, 2023.\\\\\\\\\\\\\\\\nISBN 0-201-37921-X.\\\\\\\\\\\\\\\\n[4] Kapil Chauhan. CartPole_DQN: CartPole_v0 Jupyter Notebook. https://github.\\\\\\\\\\\\\\\\ncom/kapilnchauhan77/CartPole_DQN/blob/master/CartPole_v0.ipynb, 2019. Accessed:\\\\\\\\\\\\\\\\nMay 15, 2025.\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'[5] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.Deep Learning. MIT Press, 2016.\\\\\\\\\\\\\\\\nhttp://www.deeplearningbook.org.\\\\\\\\\\\\\\\\n[6] IBM. What is natural language processing? https://www.ibm.com/think/topics/\\\\\\\\\\\\\\\\nnatural-language-processing, n.d. Accessed: May 23, 2025.\\\\\\\\\\\\\\\\n[7] Mahmood Khordoo. Deep-Reinforcement-Learning-with-PyTorch: n-\\\\\\\\\\\\\\\\nstep DQN for LunarLander-v2. https://github.com/khordoo/\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'Bibliography\\\\\\\\\\\\\\\\nDeep-Reinforcement-Learning-with-PyTorch/blob/example/examples/DQN/\\\\\\\\\\\\\\\\nlunarlander_v2-dqn-n-step.py , 2020. Accessed: May 15,2025.\\\\\\\\\\\\\\\\n[8] Shakti Kumar. adaptiveSystems: RL_Benchmarks README. https://github.\\\\\\\\\\\\\\\\ncom/shaktikshri/adaptiveSystems/blob/master/RL_Benchmarks/README.md, 2019. Ac-\\\\\\\\\\\\\\\\ncessed: May 15, 2025.\\\\\\\\\\\\\\\\n[9] Emmanouil D. Oikonomou, Petros Karvelis, Nikolaos Giannakeas, Aristidis Vrachatis, Evri-\\\\\\\\\\\\\\\\npidis Glavas, and Alexandros T. Tzallas. How natural language processing derived tech-\\\\\\\\\\\\\\\\nniques are used on biological data: a systematic review. Network Modeling Analysis in\\\\\\\\\\\\\\\\nHealth Informatics and Bioinformatics, 13(23), 2024. DOI 10.1007/s13721-024-00458-1.\\\\\\\\\\\\\\\\n[10] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\\\\\\\\\\\\\\\nunderstanding by generative pre-training. https://cdn.openai.com/research-covers/\\\\\\\\\\\\\\\\nlanguage-unsupervised/language_understanding_paper.pdf, 2018. OpenAI Report.\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'[11] Nihal T. Rao. RL-Double-DQN: Double DQN Implementation for CartPole-v0. https:\\\\\\\\\\\\\\\\n//github.com/nihal-rao/RL-Double-DQN , 2020. Accessed: May 15, 2025.\\\\\\\\\\\\\\\\n[12] SigveRokenes. learning-rl/gym/lunarlander-v2: DQNExampleforLunarLander-v2. https:\\\\\\\\\\\\\\\\n//github.com/evgiz/learning-rl/tree/master/gym/lunarlander-v2, 2019. Accessed:\\\\\\\\\\\\\\\\nMay 15, 2025.\\\\\\\\\\\\\\\\n[13] Richard S. Sutton and Andrew G. Barto.Reinforcement Learning: An Introduction. The\\\\\\\\\\\\\\\\nMIT Press Cambridge, Massachusetts,London, England, revised edition, 2015. ISBN 0-201-\\\\\\\\\\\\\\\\n37921-X.\\\\\\\\\\\\\\\\n[14] Sanket Thakur. LunarLander_DQN: DQN Implementation for LunarLander-\\\\\\\\\\\\\\\\nv2. https://github.com/sanketsans/openAIenv/blob/master/DQN/LunarLander/\\\\\\\\\\\\\\\\nLunarLander_DQN.ipynb, 2020. Accessed: May 15, 2025.\\\\\\\\\\\\\\\\n[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\\\\\\\\\\\\\\\\nGomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In\\\\\\\\\\\\\\\\nAdvances in Neural Information Processing Systems , volume 30. Curran Asso-\\\\\\\\\\\\\\\\n64\\\\\\'\"\\'',\n",
       " '\\'\"\\\\\\'Bibliography\\\\\\\\\\\\\\\\nciates, Inc., 2017. URL https://papers.nips.cc/paper_files/paper/2017/file/\\\\\\\\\\\\\\\\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\\\\\\\\\\\\\\\n65\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nBibliography\\\\\\\\\\\\\\\\n66\\\\\\'\"\\'']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['first_tokenization']['values']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
