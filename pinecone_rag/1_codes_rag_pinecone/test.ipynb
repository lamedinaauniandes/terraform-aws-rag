{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a2982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone,ServerlessSpec\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338f5dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Laboratorio\\Landing\\terraform-aws-rag\\pinecone_rag\\.env\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path(os.getcwd()).parent\n",
    "PATH_ENV = os.path.join(BASE_DIR,\".env\")\n",
    "load_dotenv(override=True,dotenv_path=PATH_ENV)\n",
    "openai_key_pinecone = os.getenv(\"token_pinecone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3ed6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc  = Pinecone(api_key=openai_key_pinecone)\n",
    "print(pc.list_indexes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b78e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = r\"C:\\Users\\ASUS\\Laboratorio\\Landing\\terraform-aws-rag\\pinecone_rag\\names_spaces\\investigations\\Constructing a language for testing RL.pdf\"\n",
    "reader = PdfReader(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "937910a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reader.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f6688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_texts = [p.extract_text().strip() for p in reader.pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aebc79ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Maestría en Ingeniería \\nde Sistemas \\ny Computación\\nDissertation\\nConstructing a language for testing\\nReinforcement Learning programs using\\nNLP techniques\\nLUIS ALEJANDRO MEDINA\\nJuly 19, 2025\\nThis thesis is submitted in partial fulfillment of the requirements\\nfor a degree of Master in Systems and Computing Engineering\\n(MISIS).\\nThesis Committee:\\nProf. Nicolás Cardozo (Promotor) Universidad de los Andes, Colombia\\nProf. Reviewer 1Ivana Dusparic Trinity College Dublin, Ireland\\nProf. Reviewer 2Ruben Manrique Universidad de los Andes, Colombia',\n",
       " 'Constructing a language for testing Reinforcement Learning pro-\\ngrams using NLP techniques\\n© YEAR LUIS ALEJANDRO MEDINA\\nSystems and Computing Engineering Department\\nFLAG lab\\nFaculty of Engineering\\nUniversidad de los Andes\\nBogotá, Colombia',\n",
       " 'With all my heart: To my dear dad, Luis Hernando Medina,\\neverything I have achieved is thanks to you. To all my family,\\nthank you for never abandoning me. And to my beloved Martín, I\\nwill always love you.\\n... And I will always be two steps behind you. “Two Steps Behind”, Def Leppard.',\n",
       " 'iv\\nAbstract\\nReinforcementLearning(RL)hasgarneredsignificantattentionfromtheresearchcommunitydue\\nto its expanding applications across various fields, including recommendation systems, robotics,\\nautonomous vehicles, and more. Consequently, there has been a growing interest in testing Deep\\nReinforcement Learning (DRL) agents and identifying potential faults that could lead to critical\\nfailures during their execution.\\nVarious techniques exist for testing systems, including white-box, black-box, and data-quality\\ntesting methods. White-box techniques rely on the assumption that complete information about\\nthe system (e.g., code, data, and architecture) is available. Black-box techniques, on the other\\nhand, operate under the premise that no internal details of the system are accessible, focusing\\ninstead on assessing the system’s performance and correctness based solely on specified require-\\nments. Due to the potentially vast amount of data generated by these systems, data-quality\\ntechniques leverage this information to infer system behavior and detect possible issues.\\nIn (RL), various methods exist for the testing and evaluation of agents. Adversarial attacks,\\nfocusing on perturbing the raw input of the RL agent, Mutation testing and debuggers focus\\non assessing the code, statistical methods for building performance metrics and Search-based\\ntesting focusing on the data produced by the agent as genetics algorithms, Plasticity maps, and\\nConstructing Language and Natural Language Process (NLP) techniques.\\nConstructing language models using NLP techniques for testing in Reinforcement Learning is\\nbased on the idea of communication between the agent and the environment, thereby establishing\\na shared language. An inference language is key to finding ways to detect possible agent failures,\\nincorrect behaviors, and low-reward outcomes.\\nOur evaluation uses different implementations from two RL application benchmarks—CartPole\\nand LunarLander—to generate testing scenarios for each. We analyze two key metrics: the',\n",
       " 'average rewardand theprobability of fault. This is important because agents can achieve\\nhigh average rewards that may overcompensate for cases with lower performance. However, our\\nresults suggest that the proposed technique is capable of identifyingregions of lower reward.\\nFor example, in the case of theShakti agent, the average reward during normal execution\\nis 185.3, whereas in our testing scenarios, it drops to 96.98. This indicates that our test case\\ngenerator can uncover regions of reduced performance.\\nOn the other hand, even for theKapil agent, which consistently shows a perfect success rate\\nduring normal execution, we were able to identify testing regions where the success rate drops\\nsignificantly—to 42.8%.\\nThe results indicate that the proposed test case generator was able to identify failure-prone\\nregions and lower rewards regions in multiple agents— even in those with perfect performance\\nunder normal conditions. Statistical analysis confirmed that, for most agents, the generated test\\ncases led to significantly lower success rates or reduced average rewards compared to standard\\nexecution.\\nv',\n",
       " 'vi\\nContents\\nAbstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv\\nList of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii\\n1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\n2. Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1. Reinforment Learning, Makov process desicion (MDP) . . . . . . . . . . . . . . . 5\\n2.2. Natural Language Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2.1. Early Neural Network-Based Models . . . . . . . . . . . . . . . . . . . . . 6\\n2.3. Abstract Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n2.3.1. Definition π∗-irrelevance abstraction . . . . . . . . . . . . . . . . . . . . . 10\\n2.3.2. Definition Q∗-irrelevance abstraction . . . . . . . . . . . . . . . . . . . . . 10\\n2.3.3. Relax Q∗-irrelevance abstraction . . . . . . . . . . . . . . . . . . . . . . . 10\\n3. MarTest: An NLP Approach for test sequence generation . . . . . . . . . . . . . . . 11\\n3.1. Vocabulary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n3.2. Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n3.3. Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n3.3.1. Average reward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n3.3.2. Failure probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n3.3.3. Random Forest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.4. Model-Conditioned Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n3.4.1. Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4. MarTest Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n4.1. Pipeline Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n4.2. Module 1: Data acquisition and preparation . . . . . . . . . . . . . . . . . . . . . 16\\n4.2.1. Inputs training and execution logs . . . . . . . . . . . . . . . . . . . . . . 16\\n4.2.2. Data Balacing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n4.2.3. Q-Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4.3. Module 2: Abstract Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4.3.1. Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4.4. Module 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4.4.1. Abstract Episodes generation . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4.5. Module 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n4.5.1. Random Forest Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n4.6. Module 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n4.6.1. Conditioned Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n4.6.2. Test Regions Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n5. Experimental Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n5.1. CartPole-v0 Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22',\n",
       " 'CONTENTS\\n5.2. LunarLander-v2 Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n5.2.1. Sanket Thakur (sanket) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n5.2.2. Mahmood Khordoo (khordoo) . . . . . . . . . . . . . . . . . . . . . . . . 24\\n5.2.3. Sigve Rokenes (rokenes) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n5.3. Conditioned transformer configurations and metrics . . . . . . . . . . . . . . . . 25\\n5.3.1. Conditional transformer parameters for cartpole . . . . . . . . . . . . . . 26\\n5.3.2. Conditional transformer parameters for Lunar-Lander . . . . . . . . . . . 26\\n5.3.3. Training and validations loss . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n5.4. Concrete Test Case Generator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n5.5. An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n5.6. Evaluation and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n5.6.1. Cartpole . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n5.6.2. Lunar lander . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n5.6.3. A Small Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n5.6.4. Threats to Validity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n6. Conclusion, Limitations and Future Work . . . . . . . . . . . . . . . . . . . . . . . . 43\\n6.1. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n6.2. Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n6.3. Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\nA. Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\nA.1. MartTest Pipeline Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\nA.2. Conditioned Transformer Model CondGPT2 . . . . . . . . . . . . . . . . . . . . . 52\\nA.3. Requirements Shakti . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\nA.4. Requirements Kapil . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\\nA.5. Requirements Nihal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\nA.6. Requirements Khordoo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\nA.7. Requirements Sanket . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\nA.8. Requirements Rokenes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\nvii',\n",
       " 'viii\\nList of Figures\\n2.1. The transformer architecture, Attention Is All You Need, 2017. . . . . . . . . . . 8\\n2.2. Transformer achitecture, Improving Language Understanding by Generative Pre-\\nTraining. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.3. ConditionedTransformer, rewardˆrandfailureprobability Pf arefirstprocessedby\\nan auxiliary neural network, then added element-wise to the word-token embeddings. 10\\n4.1. Martest pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n5.1. CartPole-v0. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n5.2. LunarLander-v2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n5.3. Training and Evaluation Loss per Epoch, Shakti . . . . . . . . . . . . . . . . . . 27\\n5.4. Training and Evaluation Loss per Epoch, Kapil . . . . . . . . . . . . . . . . . . . 27\\n5.5. Training and Evaluation Loss per Epoch, Nihal . . . . . . . . . . . . . . . . . . . 28\\n5.6. Training and Evaluation Loss per Epoch, Sanket . . . . . . . . . . . . . . . . . . 28\\n5.7. Training and Evaluation Loss per Epoch, Khordoo . . . . . . . . . . . . . . . . . 29\\n5.8. Training and Evaluation Loss per Epoch, Rokenes . . . . . . . . . . . . . . . . . . 29\\n5.9. Initial state of the CartPole environment with cart position−0.73915684, cart\\nvelocity −0.89601650, pole angle−0.03282847, and pole angular velocity0.23753740. 37\\n5.10.Final state of the CartPole environment with cart position−2.41205427, cart ve-\\nlocity −1.42968531, pole angle−0.06067818, and pole angular velocity−0.02501499. 38',\n",
       " '1\\nChapter 1\\nIntroduction\\nReinforment Learning (RL) and Natural Language Processing (NLP) have seen increasing inter-\\nest and research advances in recent years. The combination of deep learning and RL, referred to\\nas Deep Reinforcement Learning (DRL), has achieved success to solve various decision-making\\nproblems such as autonomous driving and robotics, while NLP has experienced a notable suc-\\ncess with the introduction of transformer architecture, particularly the decoder-only transformer\\nmodel and other architectures, such as Recurrent Neural Networks (RNNs) and Long Short-Term\\nMemory (LSTMs) with their primary distinction lying in their capacity to handle memory with\\nlong sequences and parallel processing.\\nDRL has recently been applied in many practical contexts. For instance, Netflix uses it to\\nrecommend which movie to show to a user in order to maximize engagement (System of recom-\\nmendation) [3], Microsoft developed Personalizer, a service developer that can be use for content\\nrecomendation [3]. Other applications include the mastery of strategy games such as Dota 2 and\\nStarCraft 2 in which it has defeated human players [2].\\nWith the introduction of the paper \"Attention is All you need\" Vaswani et al. [15] published\\nin 2017 where they introduce the transformer model laying down the foundation for modern\\nNLP and sequence modeling, such techniques have influenced the development of architectures\\nlike BERT, GPT, and other state-of-the-art NLP models, gaining huge popularity in all fields.\\nThe utility and use of NLP-based tools has been a great advantage for society and probably a\\nprincipal engine for progress.\\nFor the constantly growing use of RL models in many fields, there has been a growing need',\n",
       " '1. Introduction\\nof testing such models. Therefore, we have built a novel way for testing RL agents by bridging\\ntwo fields. To reconcile both fields, we need to reconcile the infinite state spaces in the case of\\n(continuous) RL agents, and the finite vocabulary/alphabet used in NLP models.\\nWe put forward a language as a means to reconcile RL and NLP. Through the execution of an\\nagent, this is in continuous interaction with the environment establishing a language the proposed\\napproach build such language is based on the mathematical concept of abstraction, often called\\n’equivalences classes’ or ’abstracts classes’. The vocabulary for testing an RL program consist of\\nabstract classes inputs for NLP models, wich then can produce scenarios (i.e., paths of abstract\\nstates - abstract paths) with possible fails or lower rewards.\\nNLP models model/generate abstracts paths, that is, sequences of state representative ab-\\nstracts classes and the actions. Afterwards we validate that the generated abstract paths indeed\\nare possible and lead to a diminished agent performance. To do this, we developed a test gen-\\nerator cases program extracts regions from abstract paths and disrupt representative states in\\neach class to observe the agent’s behavior.\\nWe evaluate our approach using agents selected from the GitHub OpenAI Gym leaderboard.\\nFor each agent, we first generate logs from both training and execution, which serve as inputs to\\nour implementation,MarTest-Pipeline. Based on the forecasted regions produced by the pipeline,\\nwe then construct concrete test cases to evaluate the agent’s performance.\\nOur results show that the implementation is able to identify regions associated with lower\\naverage rewards and reduced success rates. Moreover, we demonstrate that the performance on\\nthe testing samples is statistically significantly lower than that observed under normal execution,\\nhighlighting that high performance of an agent trained in a normal execution does not guarantee\\nrobustness under targeted testing conditions.\\nThesis Outline\\nThe remainder of this thesis is organized as follows:\\n• Chapter 2 – Background:Introduces the theoretical foundations relevant to this work,\\nincluding reinforcement learning and Markov decision processes (MDPs), natural language\\nprocessing models, and various forms of abstraction used to represent agent-environment\\n2',\n",
       " 'interactions symbolically.\\n• Chapter 3 – Approach:Describes the conceptual framework of the proposed method,\\nincluding the symbolic vocabulary, abstract corpus construction, and the output model\\nbased on a conditioned transformer. Assumptions and design decisions are also discussed.\\n• Chapter 4 – Implementation:Presents the detailed structure of theMarTest-Pipeline,\\nexplaining each module in the pipeline, from the processing of logs and Q-values to the\\ngeneration of abstract episodes, classification models, and the conditioned transformer used\\nto produce test case regions.\\n• Chapter 5 – Experimental Design: Details the experimental setup, including the\\nselected agents from theCartPole and LunarLander environments. It also describes how\\nconcrete test cases are generated from the abstract representations and evaluates the agents\\nusing statistical analysis. The chapter concludes with a discussion on threats to validity.\\n• Chapter 6 – Limitations and Future Work:Discusses the current limitations of the\\napproach and outlines possible future directions, including the refinement of abstraction\\nlevels, integrationoflearning-basedrepresentations, anddeeperunificationofreinforcement\\nlearning and natural language processing techniques.\\n3',\n",
       " '1. Introduction\\n4',\n",
       " '5\\nChapter 2\\nBackground\\nTo motivate the problem and to provide an overview of our approach we give the definitions of\\nthe concepts required to understand this work, regarding RL, NLP and Abstract classes.\\n2.1. Reinforment Learning, Makov process desicion (MDP)\\nReinforcement Learning trains an agent that interacts with an environmento maximize the ob-\\nserved reward obtained from the environment after action execution, RL uses a trial and error\\nstrategy to explore the environment. At every state, the agent chooses an action to execute from\\nthe set of all possible actions for the state, and receive a corresponding reward for the executed\\naction.\\nA Markov Decision Process (MDP) is defined as a 4-tuple< S,A,T,R > where S is the set\\nof continuous non-enumerable states (|S|≥| N|) ,A is the set of (continuous) actions,T is the\\ntransition function whereT : SXAXS →[0,1] such thatT(s,a,s ′) determines the probability\\nof reaching a states′ by performing an actiona in state s, R : SXA →R a reward function\\nthat determines a reward for a pair of an action and a state. At each time step, the agent uses\\na mapping from state to probabilities of selecting each possible action. This mapping is called\\nthe agent’s policyπ: SXA →[0,1] considered the solution of the MDP.',\n",
       " '2. Background\\nMarkov Property\\nMarkov Propertyis defined as the environment’s response att+ 1 depends only on the state and\\naction representations att:\\nPr{Rt+1 = r,St+1 = s′|S0,A0,R1,....Rt,St,At}= Pr{Rt+1 = r,St+1|St,At}\\nfor allr,s′,St,At. [13].\\n2.2. Natural Language Processing\\nNatural Language Processing (NLP)[6] is a subfield of artificial intelligence (AI) and linguistics\\nfocused on enabling computers to understand, interpret, and generate human language in a\\nway that is both meaningful and useful. The key goals of NLP are understanding language,\\ngenerating language and interacting with humans, the applications of NLP techniques are on\\ntext classification, machine translation, speech recognition, text generation, among others. NLP\\ntechniques has been used in other fields as DNA (or RNA) sequences, the field is often referred to\\nas Computational Biology or Bioinformatics, specically in the subfield of Genomic Data Analysis\\nwhere DNA and RNA sequences can be thought of as \"biological texts\"[9].\\nThere are a considerably amount of models in NLP, we will mention those are in the beginning\\na good start for the purpose of our approach.\\n2.2.1. Early Neural Network-Based Models\\nEarly Neural Network-Based Models refer to the first wave of neural network architectures and\\nmethods applied NLP tasks before the dominance of Transformer-based models. These models\\nprimarily relied on feedforward neural networks, recurrent architectures, and word embeddings,\\noffering improvements over traditional statistical and rule-based methods.\\nRNNs\\nRecurrent Neural Networks (RNNs) belong to a family of neural networks designed for processing\\nsequential data. They can process inputs of variable size, such as images, and can scale to much\\nlonger sequences than networks without sequence-based specialization [5].\\n6',\n",
       " '2.2. Natural Language Processing\\nRNNs process data sequentially, meaning the output at a given step depends not only on the\\ncurrent input but also on information from previous steps, they have a hidden state that serves\\nas a \"memory,\" capturing information from previous time steps.\\nThe architecture for RNNs given a input sequenceX = [x1,x2,...,x T] where T is the sequence\\nlength, we have:\\nht = f(Wtht−1 + Wxxt + b)\\nyt = g(Wyht + c)\\nWhere Wt,Wx,Wy,b,c are parameters of the model andg is the output activation function\\n(e.g, softmax for classification)\\nLSTMs\\nLong Short-Term Memory (LSTMs) are a type of Recurrent Neural Network (RNN) known\\nas gated RNNs. They are based on the idea of introducing self-loops to create paths through\\nwhich the gradient can flow for long durations. LSTMs have been found to be extremely suc-\\ncessful in many applications, such as unconstrained handwriting recognition, speech recognition,\\nhandwriting generation, machine translation, image captioning, and parsing [5].\\nTransformer\\nThe Transformer architecture (Vaswani et al., 2017), introduced in the seminal paper \"Attention\\nIs All You Need\" (2017), revolutionized the field of NLP and later impacted other domains like\\ncomputer vision. It replaced traditional sequential processing models, such as RNNs and LSTMs,\\nby introducing self-attention mechanisms, which enable efficient parallelization and long-range\\ndependency modeling see Figure 2.1.\\n7',\n",
       " '2. Background\\nFigure 2.1: The transformer architecture, Attention Is All You Need, 2017.\\nTransformer decoder only\\nTheTransformerDecoder-OnlyArchitecture, popularizedbyGPT(GenerativePre-trainedTrans-\\nformer), is a variant of the original Transformer architecture introduced by Vaswani et al. in\\n\"Attention is All You Need\" (2017). GPT uses only the decoder part of the Transformer for its\\ndesign, focusing on autoregressive text generation tasks[10]. For the achitecture see Figure 2.2\\n8',\n",
       " '2.2. Natural Language Processing\\nFigure 2.2: Transformer achitecture, Improving Language Understanding by Generative Pre-\\nTraining.\\nModel-Conditioned Transformer\\nIn this work, we employ the Model-Conditioned Transformer, a GPT-based architecture that\\nprocesses no only conventional word token but also additional conditioning variables. Analogous\\nto music-generation models that incorporate parameters such as tempo and velocity, our model\\nreceives two supplementary numerical inputs:\\n1. Performance reward.\\n2. Failure probability.\\nBoth values are generated during agent execution: the performance reward quantifies the\\nagent’s effectiveness, while the failure probability estimates the likelihood of erroneous actions.\\nSpecifically, both the performance reward and the failure probability are first processed by an\\nauxiliary neural network to produce conditioning vectors. These vectors are then added element-\\nwise to the word-token embeddings, and the resulting combined representations are passed into\\nthe transformer decoder, see Figure 2.3 .\\n9',\n",
       " '2. Background\\nFigure 2.3: Conditioned Transformer, rewardˆr and failure probabilityPf are first processed by\\nan auxiliary neural network, then added element-wise to the word-token embeddings.\\n2.3. Abstract Classes\\nThe definition of Abstracts Classes relies in the definition of equivalences class, a partition of a\\nset. A State Abstraction is defined as a mapping from an original states ∈S to an abstract\\nstate sϕ ∈Sϕ\\nϕ: S →Sϕ\\nwhere Sϕ ∈P(S) and fulfills equivalence conditions (reflexive, symmetric, and transitive) [1].\\n2.3.1. Definition π∗-irrelevance abstraction\\ns1 and s2 are in the same abstract classϕ(s1) = ϕ(s2) if π∗(s1) = π(s2) where π∗is the optimal\\npolicy.\\n2.3.2. Definition Q∗-irrelevance abstraction\\ns1 ands2 are in the same abstract classϕ(s1) = ϕ(s2) if for all actiona∈A, Q∗(s1,a) = Q∗(s2,a)\\nwhere Q∗(s,a) is the optimalQ-value function the maximum expected reward.\\n2.3.3. Relax Q∗-irrelevance abstraction\\ns1 and s2 are in the same abstract classϕ(s1) = ϕ(s2) if:\\n∀a∈A: ⌈Q∗(s1,a)/d⌉= ⌈Q∗(s2,a)/d⌉\\nwhere d is a control parameter (abstraction level) [1].\\n10',\n",
       " '11\\nChapter 3\\nMarTest: An NLP Approach for test sequence generation\\nWe aim to identify the principal potential failures or “vulnerabilities” that an agent may en-\\ncounter during execution of its policy within an environment. To this end, we leverage all\\navailable execution data—hereinafter referred to as logs (in the context of data testing). These\\nlogs consist of records of episodes, i.e. sequences of states, actions and rewards, from which we\\nseek to predict possible failures.\\nAs noted above, if the state spaceS is uncountable then the set of all possible episodes is\\nlikewise uncountable|Sn|>|N|for n= 1,2,3,4....\\nHence, the task of enumerating all potential failures is unfeasible. As a first step toward\\ntractability, we introduce a reduction via abstract classes, mappingSinto a countable abstraction\\nSϕ:\\nϕ: S →Sϕ and |Sϕ|<|N|\\nOncethisreductionisimplemented, theproblemcanbeusingtheresultingcountablespaceand\\naddressed from a language-processing perspective by regarding each abstract class as a distinct\\nlexical token.\\n3.1. Vocabulary\\nWe define a vocabulary for getting a finite set of sequences:\\nlet V be a vocabulary, ifw∈V then w= ˆs or w= a for ˆs∈ˆS and ˆS is a set of partitions ofS',\n",
       " '3. MarTest: An NLP Approach for test sequence generation\\nNote that defining the vocabulary is the first step in framing our problem as an NLP problem.\\nThen, we define the collection of all texts, known as the corpus.\\n3.2. Corpus\\nLet τ be a concrete sequence as s0a0s1a1...snan of state-action tuples and ϕ an abstraction\\nfunction, we sayˆτ is an abstraction sequence or abstract path for a sequenceτ under function\\nϕ(τ) = ˆτ if:\\nfor ˆτ = ˆS0a0 ˆS1a1... ˆSnan we have thatϕ(si) = ˆSi for i= 1,2,3...n\\nLet ˆDϕ be a corpus or abstract dataset such thatˆDϕ is composed of abstract pathsˆτ.\\nIf we were to take the setˆDϕ as the training set for an NLP model, the model would most\\nlikely imitate the agent’s behavior in an abstract way, since we would be training it on behavior\\npatterns associated with each abstract class. However, our goal is not to replicate the agent’s\\nbehavior, but rather to identify low-reward regions and potential failures. For this purpose, we\\nextend the definition of our dataset.\\n3.3. Dataset\\nLet D be the training dataset for the conditioned Transformer model, such that(ˆτ,¯r,Pf ) ∈D\\nif ˆτ ∈ ˆDϕ, ¯r is the average reward ofˆτ, andPf is the failure probability ofˆτ.\\n3.3.1. Average reward\\nSince ˆτ may represent multiple concrete sequences—that is, it may happen thatϕ(τ1) = ϕ(τ2) =\\n··· = ϕ(τn) = ˆτ for sequences τ1,τ2,...,τ n of state-action tuples—we take the total rewards\\nassociated with eachτi and compute their average to obtain the mean reward¯r.\\n3.3.2. Failure probability\\nWe can associate eachˆτ ∈ ˆDϕ with a failure probability. Given a sequence of concrete trajectories\\nτ1,τ2,...,τ n executed by an agent, we know that each trajectory either resulted in failure or not.\\n12',\n",
       " '3.3. Dataset\\nThat is, we can construct a list of pairs(τ1,j1),(τ2,j2),..., (τn,jn), wherejk = 1 if the execution\\nof τk was successful, andjk = 0 otherwise.\\nTherefore, wecanderiveacorrespondinglistofpairs (ˆτ1,j1),(ˆτ2,j2),..., (ˆτn,jn), whereϕ(τk) =\\nˆτk for k= 1,2,...,n . In this way, we obtain a dataset of abstract trajectories labeled as success\\nor failure. With the appropriate model, we can then predict the failure probability for eachˆτ.\\n3.3.3. Random Forest\\nTo predict the failure probability of an abstract trajectoryˆτ, we follow the approach proposed by\\nZolfagharian [1, Section IV, Subsection 7], which consists of representingˆτ as a binary sequence\\ndetermined by the presence of abstract states withinˆτ. This representation, together with the\\ncorresponding failure label, serves as input for aRandom Forestmodel.\\nAssume that ˆS0, ˆS1,..., ˆSm are all the abstract classes produced by a mappingϕ. Then, for a\\ngiven abstract trajectoryˆτ, we can construct the following binary representation:\\nAbstract Sequence ˆS0 ˆS1 ˆS2 ... ˆSm\\nˆτ 0 1 1 ... 0\\nTable 3.1: Binary representation of an abstract trajectory\\nTherefore, by constructing a binary representation for each sequence followed by the agent, and\\nlabeling the corresponding performance outcome as either failure (1) or success (0), we obtain a\\ndataset suitable for training a classification algorithm:\\nIndex ˆS0 ˆS1 ˆS2 ... ˆSm Failure\\ni 0 1 1 ... 0 1\\ni+ 1 1 0 0 ... 0 0\\nTable 3.2: Training data representation for a classification model\\n13',\n",
       " '3. MarTest: An NLP Approach for test sequence generation\\n3.4. Model-Conditioned Transformer\\nIn this approach, any natural language processing (NLP) technique can be employed to generate\\nabstract sequences associated with a certain probability of failure. However, as previously stated,\\nthis work proposes the use of a conditioned transformer to identify risk regions.\\nGiven the dataset D, composed of 3-tuples (ˆτ,¯r,Pf ), we have a training set for a Model-\\nConditioned Transformer, as described in Section 2.2.1. Once the conditioned transformer is\\ntrained, it can be used to generate abstract sequences given a rewardr and a failure probability\\nPf.\\nThese sequences, composed of abstract statesˆS interpreted by the transformer as tokensw,\\ncan also be seen as regions associated with a certain likelihood of failure and low reward. The\\nselection and handling of these regions may vary depending on the objective. In Section 5.4, we\\nintroduce a method for generating test cases based on these sequences as part of our experimental\\nsetup.\\n3.4.1. Assumptions\\nIn this work, we focus on RL agents with discrete actions we could extend the theory for infinity\\nof set of actions, but for simplicity we will do in a future work.\\n14',\n",
       " '15\\nChapter 4\\nMarTest Pipeline\\nIn this chapter, we present the implementation of our approach, called MarTest-Pipeline, a\\nsoftware tool designed to produce risk regions for testing reinforcement learning agents. The\\nimplementation is available athttps://github.com/lamedinaauniandes/Martest-pipeline.\\nWe describe the purpose and functionality of each module at a high level, emphasizing that all\\ncomponents are configurable and support module-specific arguments. This overview is intended\\ntohelpthereadernavigatetheimplementationwithgreaterclarity. Fordetailsregardingsoftware\\nrequirements and environment setup, please refer to Appendix A.1.\\n4.1. Pipeline Implementation\\nOur implementation comprises five principal modules, each corresponding to a distinct stage in\\nthe log-processing pipeline for any agent (shown in Figure 4.1).\\nEach module performs its designated processing tasks and produces the artifacts required by\\nthe subsequent module. This modular design offers us considerable flexibility for future work:\\nfor instance, researchers wishing to focus on the abstract-class graph can directly leverage the\\noutputs generated by the second module,2. Abstract Classes, as the basis for their analyses.',\n",
       " '4. MarTest Pipeline\\nFigure 4.1: Martest pipeline .\\n4.2. Module 1: Data acquisition and preparation\\n4.2.1. Inputs training and execution logs\\nThe pipeline begins with two types of logs: training logs, which record the agent’s behavior\\nduring the training phase, and execution logs, which capture its behavior after training (i.e.,\\nduring real-world deployment). Although it is not strictly necessary to provide both types of\\nlogs, combining them can help balance the dataset. In particular, if the execution logs contain an\\ninsufficient number of failure episodes —as may happen in some cases where the agent performs\\nperfectly with no observed failures— an effective strategy is to augment them with records from\\nthe training phase.\\n4.2.2. Data Balacing\\nIn this stage of the pipeline, we specify a mixture ratiop∈[0,1] and sample from the training\\nlogs accordingly. Since early training episodes may not accurately represent the agent’s behavior,\\nwe drawm episodes from training data wherem = p·Ntrain and are sample according to the\\nprobability distribution over episodes introduced by Zolfagharian [1, Sección IV,Ec. (6)]:\\nP(ei) = i\\nNtrain∑\\nj=1\\nj\\n, i = 1,2,...,m,\\nwhere Ntrain denotes the total number of training episodes. This weighting favors latter (more\\nrepresentative) episodes while still incorporating early-stage data with more state exploration.\\nThe output of this stage is an execution log combining episodes of both training and execution.\\n16',\n",
       " '4.3. Module 2: Abstract Classes\\n4.2.3. Q-Values\\nDuring this step in the process, we calculate the total reward obtain for each state and action\\nfor all episodes in the logs, based in the definition of theaction-value for a states, actiona, and\\npolicy π [13, Sección 3.7, Ec. (3.11)] :\\nqπ(s,a) = Eπ\\n[\\nGt |St = s, At = a\\n]\\n= Eπ\\n[∞∑\\nk=0\\nγkRt+k+1\\n⏐⏐⏐St = s, At = a\\n]\\nThis equation calculates the value expected to take an actiona in state s, accounting for\\nfuture rewards, in our case we do not use expected values because the policy has already been\\nimplemented. Furthermore, as we have already obtained the rewards, we do not discount the\\nrewards. As a consequence we calculate the q-values as:γ = 1,\\nqπ(s,a) =\\n∞∑\\nk=0\\nRt+k+1\\nThe end result of this is the q-table with the q-value for each state-action pair.\\n4.3. Module 2: Abstract Classes\\n4.3.1. Construction\\nOnce we have the Q-table, we proceed to build the abstract classes using the definition of Relaxed\\nQ∗-irrelevance abstraction (see Section 2.3.3). This module generates a JSON file containing a\\ndictionary, where the keys are the representatives of the abstract classes and the values are lists\\nof the states grouped within each class.\\nSince the definition ofQ∗-irrelevance abstraction depends on the abstraction level—a config-\\nurable parameter—the user can set this value according to the desired granularity.\\nNote that at this stage, we are constructing the vocabulary referenced in Section 3.1.\\n4.4. Module 3\\n4.4.1. Abstract Episodes generation\\nIn this module, we proceed to build the corpusˆDϕ, as defined in Section 3.2, and compute the\\naverage reward of each abstract episode. The output consists of CSV files containing abstract\\n17',\n",
       " '4. MarTest Pipeline\\nsequences along with their corresponding average rewards.\\n4.5. Module 4\\n4.5.1. Random Forest Model\\nIn this module, we compute the failure probability for each abstract sequence obtained in the\\nprevious module. To achieve this, we generate CSV files containing the binary representations of\\neach abstract sequence along with their corresponding failure labels, as described in Section 3.3.3,\\nin order to estimate the failure probability of each abstract sequence.\\nFinally, we generate CSV files containing the datasetD, as described in Section 3.3, which\\nserves as input for the Transformer model.\\nInthismoduleweusethelibrary sklearnbyscikit-learn, asimpleandefficienttoolforpredictive\\nanalysis, we use the classsklearn.ensemble.RandomForestClassifierprecisely and show us a great\\nprecision and performance.\\n4.6. Module 5\\n4.6.1. Conditioned Transformer\\nIn this module, we train the Conditioned Transformer using the datasetD produced in the\\nprevious module. Once the model is trained, we can also generate abstract sequences given a\\nreward valuer and a failure probabilityPf, thereby identifying regions with potential risk of\\nfailure or low reward, as described in Section 3.4.\\nWe use thetransformers library fromHugging Face, which integrates with PyTorch. Specifi-\\ncally, we base our implementation on thetransformers.GPT2LMHeadModel class, which follows\\nthe GPT-2 architecture—a decoder-only Transformer commonly used for text generation. We\\nmodified this architecture to also receive the average reward and failure probability as inputs in\\norder to predict tokens (i.e., abstract states or regions) associated with specific levels of risk or\\nfailure likelihood. We refer to this modified model as theConditioned Transformer.\\nThe Conditioned Transformer is implemented by extending theGPT2LMHeadModel class to\\n18',\n",
       " '4.6. Module 5\\na new class namedCondGPT2. The core idea is to preprocess the average reward and failure\\nprobability through a small feedforward network, producing a vector representation that is then\\nadded to the token embeddings of the abstract sequence before being passed to the GPT-2 model.\\nFor further details, see Section A.2.\\n4.6.2. Test Regions Cases\\nAt the end of the pipeline, we obtain an output in the form of a list ofpossible abstract\\nepisodes—that is, sequences of abstract states and actions (e.g.,’w2 1 w1 2 w3 1 w6 ... w5\\n1 True’) interpreted by the transformer astokens or words. However, since these tokens\\nwi represent abstract classes, they can also be understood assequences of regions and\\nactions.\\nGiven that the transformer has been designed to receive the average reward and fail-\\nure probabilityas input parameters, we interpret these region–action sequences aspossible\\nepisodes conditioned on those parameters.\\nBased on this representation, multiple strategies can be devised to generateconcrete and\\nactionable test cases. In the following chapter, we present the specific testing approaches\\napplied to the selected agents.\\n19',\n",
       " '4. MarTest Pipeline\\n20',\n",
       " '21\\nChapter 5\\nExperimental Design\\nWe evaluate our implementation on six agents selected from the publicly editable OpenAI Gym\\nleaderboard wiki page1. Three of these agents implement in theCartPole-v0 environment, and\\nthe remaining three are implement theLunarLander-v2.\\nFor all agents, we apply the definition ofRelaxedQ∗-irrelevance abstraction( Section 2.3.3), as\\nwe consider this abstraction suitable for capturing the agent’s general perception formed through\\nits interaction with the environment and its learning dynamics.\\nThe constructed abstraction assigns avalue to each experience and allows for controlling the\\ngranularity of the abstract classes through the abstraction-level parameter. In our experiments,\\nthis parameter was set tod= 4, chosen as a midpoint based on the experimental settings reported\\nby Zolfagharian [1, Section V, Table I].\\nFor each agent, we introduced a lightweight instrumentation module into the codebase to\\nrecord logs during both training and execution. From this point onward, we refer to this logging\\nprocess asinstrumentation.\\nThe server used to run the environments in the experiment has the following specifications:\\n• CPU: 3×Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz (model 79)\\n• RAM: 7.8GiB (MemTotal)\\n• Primary storage:1× 60GB (device sda)\\n1https://github.com/openai/gym/wiki/Leaderboard',\n",
       " '5. Experimental Design\\n• Operating system:Ubuntu 22.04.5 LTS (Jammy)\\nAll experimental data produced by the MarTest-Pipeline software, as a result of each agent’s\\nexecution, is available athttps://zenodo.org/records/15485620. The specifications of the\\nmachines used to run the MarTest-Pipeline software were as follows:\\n• CPU:48 logical cores (2×Intel® Xeon® Silver 4310 CPU @ 2.10GHz; 12 cores per socket,\\n2 threads per core)\\n• RAM: 251.3GiB (MemTotal)\\n• Primary storage:1 ×7 TB (devicesda)\\n• OS: Ubuntu 22.04.4 LTS (Jammy)\\n• GPU: 4×NVIDIA A40 (46068MiB VRAM each); Driver Version: 560.35.03, CUDA\\nVersion: 12.6\\n5.1. CartPole-v0 Agents\\nWe begin by presenting a description of theCartPole-v0 environment provided by OpenAI Gym:\\n“A pole is attached by an un-actuated joint to a cart, which moves along a frictionless\\ntrack. The pendulum is placed upright on the cart and the goal is to balance the pole\\nby applying forces in the left and right direction on the cart.”2\\nTherefore, the only actions available to the agent are pushing the cart to the left or to the\\nright, represented by the action setA= {0,1}. The agent observes the state of the environment\\nas a list of four continuous values corresponding to:cart position, cart velocity, pole angle, and\\npole velocity at the tip(see Figure 5.1).\\n2https://gymnasium.farama.org/environments/classic_control/cart_pole/\\n22',\n",
       " '5.1. CartPole-v0 Agents\\nFigure 5.1: CartPole-v0.\\nFor this environment the selected agents are:\\nShakti Kumar (shakti)\\nRanked fourth on the OpenAI Gym leaderboard, this agent is reported to solve the environment\\nin 0 episodes. The code [8, shakti], dated from December 2019, requiring to set up a custom\\nenvironment based on Python 3.7.\\nThe necessary dependencies are listed in Appendix A.3. Due to the legacy state of the code,\\nwe installed PyTorch manually from pytorch.org using the following command for Ubuntu:\\npip install torch ==1.2.0+ cu92 torchvision ==0.4.0+ cu92 -f https ://\\ndownload . pytorch . org / whl / torch_stable . html\\nKapil Chauhan (kapil)\\nRanked ninth on the OpenAI Gym leaderboard, this agent is reported to solve the environment\\nin 4 episodes. The code [4, kapil], dated from December 2019, also requires setting up a custom\\nenvironment based on Python 3.7. The required dependencies are listed in Appendix A.4.\\nNihal T. Rao (nihal)\\nRanked twenty-ninth on the OpenAI Gym leaderboard, this agent is reported to solve the en-\\nvironment in 184 episodes. The code [11, Nihal], dated from June 2020, also requires setting\\n23',\n",
       " '5. Experimental Design\\nup a custom environment based on Python 3.7. The necessary dependencies are listed in Ap-\\npendix A.5.\\n5.2. LunarLander-v2 Agents\\nThe description of theLunarLander-v2 environment, as provided by OpenAI Gym is:\\n“This environment is a classic rocket trajectory optimization problem. According to\\nPontryagin’s maximum principle, it is optimal to fire the engine at full throttle or\\nturn it off. This is the reason why this environment has discrete actions: engine on\\nor off... The landing pad is always at coordinates (0,0). The coordinates are the first\\ntwo numbers in the state vector. Landing outside of the landing pad is possible. Fuel\\nis infinite, so an agent can learn to fly and then land on its first attempt.”3\\nThere are four discrete actions available:0 – do nothing,1 – fire left orientation engine,2 –\\nfire main engine,3 – fire right orientation engine.\\nThe state is represented as an 8-dimensional vector, which includes: the lander’s position\\ncoordinates (x,y), its linear velocities(vx,vy), its angle and angular velocity, and two boolean\\nvalues indicating whether the left and right legs are in contact with the ground.4 (See Figure 5.2.)\\nFor this environment, the selected agents are:\\n5.2.1. Sanket Thakur (sanket)\\nRanked ninth on the OpenAI Gym leaderboard, this agent is reported to solve the environment\\nin 454 episodes. The code [14, sanket], dated from April 2020, requires setting up a custom\\nenvironment based on Python 3.7. The required dependencies are listed in Appendix A.7.\\n5.2.2. Mahmood Khordoo (khordoo)\\nRanked tenth on the OpenAI Gym leaderboard, this agent is reported to solve the environment\\nin 602 episodes. The code [7, khordoo], dated from April 2020, also requires setting up a custom\\n3https://gymnasium.farama.org/environments/box2d/lunar_lander/#description\\n4https://gymnasium.farama.org/environments/box2d/lunar_lander/#description\\n24',\n",
       " '5.3. Conditioned transformer configurations and metrics\\nFigure 5.2: LunarLander-v2.\\nenvironment based on Python 3.7. The required dependencies are listed in Appendix A.6.\\n5.2.3. Sigve Rokenes (rokenes)\\nRankednineteenontheOpenAIGymleaderboard, thisagentisreportedtosolvetheenvironment\\nin 1,590 episodes. The code [12, rokenes], dated from January 2019, requires setting up a custom\\nenvironment based on Python 3.7. The necessary dependencies are listed in Appendix A.8.\\n5.3. Conditioned transformer configurations and metrics\\nNext, we present the configurations and training results of the conditioned transformer for each\\nexperiment. We reportvocab_size, the number of tokens in the model’s vocabulary;n_embd, the\\ndimensionality of the token embeddings and hidden states;n_layer, the number of transformer\\ndecoder blocks (also called layers); and n_head, the number of attention heads in each self-\\nattention layer.\\n25',\n",
       " '5. Experimental Design\\n5.3.1. Conditional transformer parameters for cartpole\\nAgent vocab_size n_embd n_layer n_head\\nShakti 101 1024 12 16\\nKapil 101 1024 12 16\\nNihal 101 1024 12 16\\nTable 5.1: Parameters of conditional transformers for cartpole.\\n5.3.2. Conditional transformer parameters for Lunar-Lander\\nAgent vocab_size n_embd n_layer n_head\\nSanket 840 512 6 8\\nKhordoo 711 512 6 8\\nRokenes 868 512 6 8\\nTable 5.2: Parameters of conditional transformer for Lunar-lander.\\n5.3.3. Training and validations loss\\nNext, we present the loss metrics for each agent in the CartPole and LunarLander environments.\\nAs shown, the loss for all agents decreases over time and converges to zero. This suggests that\\nNLP techniques are sufficient to solve these tasks, which is likely due to the relatively small\\nvocabulary size compared to other NLP problems, such as machine translation or large language\\nmodels.\\n26',\n",
       " '5.3. Conditioned transformer configurations and metrics\\nTraining and Evaluation Loss per Epoch, Shakti\\nFigure 5.3: Training and Evaluation Loss per Epoch, Shakti\\nTraining and Evaluation Loss per Epoch, Kapil\\nFigure 5.4: Training and Evaluation Loss per Epoch, Kapil\\n27',\n",
       " '5. Experimental Design\\nTraining and Evaluation Loss per Epoch, Nihal\\nFigure 5.5: Training and Evaluation Loss per Epoch, Nihal\\nTraining and Evaluation Loss per Epoch, Sanket\\nFigure 5.6: Training and Evaluation Loss per Epoch, Sanket\\n28',\n",
       " '5.4. Concrete Test Case Generator\\nTraining and Evaluation Loss per Epoch, Khordoo\\nFigure 5.7: Training and Evaluation Loss per Epoch, Khordoo\\nTraining and Evaluation Loss per Epoch, Rokenes\\nFigure 5.8: Training and Evaluation Loss per Epoch, Rokenes\\n5.4. Concrete Test Case Generator\\nOnce the MarTest Pipeline Regionsmodule is deployed, we obtain a list of possible regions\\nassociated with failure risk or low rewards. The next step is to take advantage of this output in\\n29',\n",
       " '5. Experimental Design\\norder to effectively test the agents.\\nTo define our strategy, we refer to the Markov property, which states that the environment’s\\nresponse at timet+ 1 depends only on the current state and action at timet (see Section 2.1).\\nFollowing this principle, we consider it valid to infer possible failures or low-reward outcomes\\nstarting from an initial state. Our model provides a list of such high-risk initial regions, indicating\\nsituations where failure may occur within a limited number of steps.\\nFrom each initial region within an abstract sequence produced by the conditioned Trans-\\nformer (e.g.,w50 in ˆτ = w50 1 w12 0 ...w 223), we sampled 100 states and applied perturbations\\nto each point. Specifically, we added noise sampled from a uniform distribution in the range\\n[−0.001,0.001] to each coordinate. This perturbation step enhances the testing process by intro-\\nducing slight variability into the state space.\\nFor each perturbed state, we initialized the environment and executed the agent starting from\\nthat point. We then compared the resulting rewards with those obtained by executing the agent\\nunder normal conditions.\\nDue to the nature of theLunarLander-v2 environment, an additional consideration is required.\\nIn this environment, the lander always starts at coordinatex= 0.0 and approximatelyy≈1.41\\nin normalized coordinates. In the internalBox2D coordinate system, this corresponds to a fixed\\nstarting point near(10.0, 13.33).\\nTherefore, it is necessary to validate whether a given state can be considered a valid initial\\nstate for the environment. Only those states satisfying this condition are used, and perturbations\\nare applied to them within the uniform random range described earlier. These perturbed states\\nare then used to initialize the environment during testing.\\n5.5. An Example\\nIn this section we show a example produced by test generator, we show you how start and all\\ntaking of decisions through the sequence of states.\\n30',\n",
       " '5.6. Evaluation and Results\\n5.6. Evaluation and Results\\nIn this section, we present the evaluation and results for each agent. Two types of analyses are\\nincluded:\\nFirst, we report statistical comparisons of the average rewards. The goal is to determine\\nwhether, in general, the regions proposed by the test case generator result in lower rewards\\ncompared to normal agent execution.\\nSecond, we evaluate the probability of success, defined as the proportion of episodes where the\\nagent achieves a total reward greater than or equal to 200. It is important to clarify that using\\nthe mean reward alone may be misleading in some cases: an agent could obtain a few episodes\\nwith very high rewards, raising the average, while still failing to consistently solve the task (i.e.,\\nachieving fewer successful episodes). Therefore, both the average performance and the success\\nrate are considered in the analysis.\\n5.6.1. Cartpole\\nThe following section provides a summary of the vocabulary size (i.e., number of abstract classes)\\ngenerated for each agent in cartpole environment, as well as the number of regions identified as\\ninitial regions with a high probability of failure or low expected reward by the transformer. We\\nalso report the sample size, defined as the number of test cases generated by the Concrete Test\\nCase Generator.\\nAgent Vocabulary Size Number of Initial Regions Sample Size\\nShakti 101 42 4000\\nKapil 101 42 4200\\nNihal 101 42 3900\\nTable 5.3: Summary of vocabulary size, number of initial regions and sample size used for each\\nagent, Cartpole.\\nThe sample size represents too the total number of episodes executed for both normal execution\\n31',\n",
       " '5. Experimental Design\\nand simulated testing. The variation in sample size across agents arises from the design of the\\ntest case generator, which attempts to sample up to 100 states per risk region. However, in\\npractice, some abstract classes contain fewer than 100 states, limiting the number of available\\ntest cases.\\nWe now present the table of statistical comparisons based on average rewards:\\nAgent Mean SD SEM t-statistic P-value µtesting <µexecution\\nShakti\\nSample Testing 96.98 67.95 1.07 -80.845 p< 1 ×10−10 yes\\nNormal Execution 185.30 12.52 0.20\\nKapil\\nSample Testing 190.86 35.06 0.54 -76.536 p< 1 ×10−10 yes\\nNormal Execution 235.06 13.09 0.20\\nNihal\\nSample Testing 182.47 82.53 1.32 7.371 p= 1.000 no\\nNormal Execution 169.23 75.92 1.22\\nTable 5.4: Comparison of sample testing and normal execution for each agent in cartpole envi-\\nronment.\\nFor the agents Kapil and Shakti (CartPole), we can conclude that there is sufficient statistical\\nevidence to support the claim that the test case generator identifies regions of significantly lower\\nreward.\\nIn particular, note that for Kapil—an agent with a perfect success rate under normal conditions\\n(as we will discuss later)— the generator produced test cases with an average reward of 190.86,\\ncompared to 235.06 under normal execution.\\nSimilarly, for Shakti, the mean reward under generated test cases dropped to 96.98, nearly\\nhalf of the 185.30 average under normal conditions, indicating that the generator successfully\\nidentified regions associated with very low performance.\\nWe now present the success rate comparison for all agents, where success is defined as achieving\\n32',\n",
       " '5.6. Evaluation and Results\\na total reward of at least 200 per episode.\\nCondition Success Rate z-statistic p-value Ptesting <Pexecution\\nShakti\\nSample Testing 0.047 -14.418 p= 2.005 ×10−47 yes\\nNormal Execution 0.141\\nKapil\\nSample Testing 0.428 -57.982 p< 1 ×10−10 yes\\nNormal Execution 1.000\\nNihal\\nSample Testing 0.305 8.135 p= 1.000 no\\nNormal Execution 0.224\\nTable 5.5: Summary of success rate across agents and conditions in cartpole environment.\\nBased on the results, we can conclude that for the agents Kapil and Shakti (CartPole), there\\nis sufficient statistical evidence to affirm that the probability of success is significantly lower in\\nthe test case scenarios than under normal execution conditions.\\nNotably, even for Kapil—an agent that achieved perfect performance under normal conditions,\\nnever obtaining a reward below 200— the test case generator was able to identify scenarios where\\nfailures were possible, demonstrating its capacity to expose potential weaknesses in otherwise\\nrobust policies.\\n5.6.2. Lunar lander\\nThe following section provides a summary of the vocabulary size (i.e., number of abstract classes)\\ngenerated for each agent in Lunar lander environment, as well the number of regions identified\\nas initial regions with a high probability of failure or low expected reward by the transformer.\\nWe also report the sample size, defined as the number of test cases generated by the Concrete\\nTest Case Generator.\\n33',\n",
       " '5. Experimental Design\\nAgent Vocabulary Size Number of Initial Regions Sample Size\\nSanket 840 58 781\\nKhordoo 711 58 1892\\nRokenes 868 64 779\\nTable 5.6: Summary of vocabulary size, number of initial regions and sample size used for each\\nagent, Lunar Lander.\\nThe sample size represents too the total number of episodes executed for both normal execution\\nand simulated testing. The variation in sample size across agents arises from the design of the\\ntest case generator, which attempts to sample up to 100 states per risk region. However, in\\npractice, some abstract classes contain fewer than 100 states, limiting the number of available\\ntest cases.\\nThis effect is more pronounced in the LunarLander agents, where not all states within an\\nabstract class can be considered valid initial states for the environment. As a result, the effective\\nnumber of testable samples per region may be reduced, impacting the total sample size.\\nWe now present the table of statistical comparisons based on average rewards:\\n34',\n",
       " '5.6. Evaluation and Results\\nAgent Mean SD SEM t-statistic P-value µtesting <µexecution\\nSanket\\nSample Testing 234.24 34.91 1.25 -2.738 p= 3.132×10−3 no\\nNormal Execution 239.81 44.94 1.61\\nKhordoo\\nSample Testing 200.58 71.18 1.64 -4.980 p< 3.328×10−7 yes\\nNormal Execution 212.07 70.77 1.63\\nRokenes\\nSample Testing 263.70 76.17 2.73 -1.487 p= 6.857×10−2 no\\nNormal Execution 268.92 61.64 2.21\\nTable 5.7: Comparison of sample testing and normal execution for each agent in Lunar Lander\\nenvironment.\\nFor the agent Khordoo (LunarLander), we can conclude that there is sufficient statistical\\nevidence to support the claim that the test case generator identifies regions with significantly\\nlower rewards.\\nHowever, for the agents Sanket and Rokenes, we cannot assert that the generator found regions\\nwith lower rewards compared to a normal execution.\\nWe now present the success rate comparison for all agents, where success is defined as achieving\\na total reward of at least 200 per episode.\\n35',\n",
       " '5. Experimental Design\\nCondition Success Rate z-statistic p-value Ptesting <Pexecution\\nSanket\\nSample Testing 0.862 -4.964 p= 3.444 ×10−7 yes\\nNormal Execution 0.937\\nKhordoo\\nSample Testing 0.627 -7.295 p= 1.490 ×10−13 yes\\nNormal Execution 0.737\\nRokenes\\nSample Testing 0.910 -2.529 p= 5.713 ×10−3 yes\\nNormal Execution 0.944\\nTable 5.8: Summary of success rate across agents and conditions in Lunar Lander.\\nBased on the results, we can conclude that for the agents Khordoo, Sanket, and Rokenes\\n(LunarLander), there is sufficient statistical evidence to affirm that the probability of success is\\nsignificantly lower in the test case scenarios than under normal execution conditions.\\n5.6.3. A Small Example\\nIn this subsection, we present an example in which the Shakti agent fails, achieving a total reward\\nof 81.0. Recall that a state in the CartPole environment is represented by anndarray of four\\ncomponents, as described in Table 5.9:\\n36',\n",
       " '5.6. Evaluation and Results\\nIndex Observation Minimum Maximum\\n0 Cart Position −4 .8 4 .8\\n1 Cart Velocity −∞ ∞\\n2 Pole Angle −0 .418 rad (−24 ◦) 0 .418 rad (24 ◦)\\n3 Pole Angular Velocity −∞ ∞\\nTable 5.9: Observation space of the CartPole environment: anndarray of shape(4,).\\nWe initialize the environment to the following state, generated by our test generator:\\nx0 =\\n[\\n−0.73915684 −0.89601650 −0.03282847 0 .23753740\\n]\\n.\\nFigure 5.9 illustrates this initial configuration:\\nFigure 5.9: InitialstateoftheCartPoleenvironmentwithcartposition −0.73915684, cartvelocity\\n−0.89601650, pole angle−0.03282847, and pole angular velocity0.23753740.\\nFigure 5.10 shows the final state of the episode, wherein the agent has pushed the cart beyond\\nthe left boundary:\\n37',\n",
       " '5. Experimental Design\\nFigure 5.10: Final state of the CartPole environment with cart position−2.41205427, cart veloc-\\nity −1.42968531, pole angle−0.06067818, and pole angular velocity−0.02501499.\\nConsequently, the agent performs more “left” actions (0) than “right” actions (1), demon-\\nstrating an inability to counteract the tendency toward negative velocity. Below, we present the\\ncomplete sequence of actions taken during the episode:\\n[−0.73915684 −0.8960165 −0.03282847 0.2375374 ] 1 1.0 False\\n[−0.75707717 −0.70044128 −0.02807772 −0.06531717] 1 1.0 False\\n[−0.771086 −0.50492826 −0.02938406 −0.36672488] 0 1.0 False\\n[−0.78118456 −0.6996206 −0.03671856 −0.0834501 ] 0 1.0 False\\n[−0.79517697 −0.89419749 −0.03838756 0.19742567] 1 1.0 False\\n[−0.81306092 −0.69854809 −0.03443905 −0.1071154 ] 0 1.0 False\\n[−0.82703189 −0.89316003 −0.03658136 0.17450633] 1 1.0 False\\n[−0.84489509 −0.69753415 −0.03309123 −0.12948869] 0 1.0 False\\n[−0.85884577 −0.89216682 −0.03568101 0.15257344] 1 1.0 False\\n[−0.87668911 −0.69655261 −0.03262954 −0.1511491 ] 0 1.0 False\\n[−0.89062016 −0.89119251 −0.03565252 0.13106395] 1 1.0 False\\n[−0.90844401 −0.69557846 −0.03303124 −0.17265027] 0 1.0 False\\n38',\n",
       " '5.6. Evaluation and Results\\n[−0.92235558 −0.89021247 −0.03648425 0.10943207] 1 1.0 False\\n[−0.94015983 −0.69458722 −0.0342956 −0.19453452] 0 1.0 False\\n[−0.95405157 −0.88920223 −0.03818629 0.0871354 ] 1 1.0 False\\n[−0.97183562 −0.69355431 −0.03644359 −0.21734658] 0 1.0 False\\n[−0.9857067 −0.88813685 −0.04079052 0.06362139] 0 1.0 False\\n[−1.00346944 −1.08265094 −0.03951809 0.34316074] 1 1.0 False\\n[−1.02512246 −0.88698972 −0.03265488 0.03828275] 1 1.0 False\\n[−1.04286225 −0.69141508 −0.03188922 −0.26452163] 0 1.0 False\\n[−1.05669055 −0.8860677 −0.03717965 0.01793501] 0 1.0 False\\n[−1.07441191 −1.08063728 −0.03682095 0.29865938] 1 1.0 False\\n[−1.09602465 −0.88501034 −0.03084776 −0.00540505] 1 1.0 False\\n[−1.11372486 −0.68945988 −0.03095587 −0.30765899] 0 1.0 False\\n[−1.12751406 −0.88412737 −0.03710905 −0.02489722] 0 1.0 False\\n[−1.14519661 −1.07869804 −0.03760699 0.2558503 ] 1 1.0 False\\n[−1.16677057 −0.8830599 −0.03248998 −0.04845328] 1 1.0 False\\n[−1.18443176 −0.68748749 −0.03345905 −0.35120743] 0 1.0 False\\n[−1.19818151 −0.88211804 −0.0404832 −0.06926014] 0 1.0 False\\n[−1.21582387 −1.07663691 −0.0418684 0.21038029] 1 1.0 False\\n[−1.23735661 −0.8809421 −0.0376608 −0.09521039] 0 1.0 False\\n[−1.25497546 −1.0755046 −0.039565 0.18535677] 1 1.0 False\\n[−1.27648555 −0.87983955 −0.03585787 −0.1195402 ] 0 1.0 False\\n[−1.29408234 −1.07442989 −0.03824867 0.16161772] 1 1.0 False\\n[−1.31557094 −0.87878184 −0.03501632 −0.14288208] 0 1.0 False\\n[−1.33314657 −1.07338526 −0.03787396 0.13855142] 1 1.0 False\\n[−1.35461428 −0.87774189 −0.03510293 −0.16583546] 0 1.0 False\\n[−1.37216912 −1.07234423 −0.03841964 0.11557008] 1 1.0 False\\n[−1.393616 −0.87669345 −0.03610824 −0.18898212] 0 1.0 False\\n[−1.41114987 −1.07128071 −0.03988788 0.092095 ] 1 1.0 False\\n[−1.43257548 −0.87561041 −0.03804598 −0.21290092] 0 1.0 False\\n39',\n",
       " '5. Experimental Design\\n[−1.45008769 −1.07016834 −0.042304 0.06754196] 1 1.0 False\\n[−1.47149106 −0.87446621 −0.04095316 −0.23818226] 0 1.0 False\\n[−1.48898038 −1.0689799 −0.0457168 0.04130677] 1 1.0 False\\n[−1.51035998 −0.87323321 −0.04489067 −0.26544255] 0 1.0 False\\n[−1.52782465 −1.06768666 −0.05019952 0.01275036] 0 1.0 False\\n[−1.54917838 −1.26205408 −0.04994451 0.28918176] 1 1.0 False\\n[−1.57441946 −1.06625682 −0.04416088 −0.01882548] 1 1.0 False\\n[−1.5957446 −0.87053029 −0.04453739 −0.32510813] 0 1.0 False\\n[−1.6131552 −1.06499074 −0.05103955 −0.04679636] 0 1.0 False\\n[−1.63445502 −1.25934507 −0.05197548 0.22935638] 1 1.0 False\\n[−1.65964192 −1.0635204 −0.04738835 −0.07925787] 0 1.0 False\\n[−1.68091233 −1.25793212 −0.04897351 0.19810537] 1 1.0 False\\n[−1.70607097 −1.06214514 −0.0450114 −0.10961544] 0 1.0 False\\n[−1.72731387 −1.25659416 −0.04720371 0.16853378] 1 1.0 False\\n[−1.75244575 −1.06082944 −0.04383303 −0.13865894] 1 1.0 False\\n[−1.77366234 −0.86510799 −0.04660621 −0.44484192] 0 1.0 False\\n[−1.7909645 −1.05954062 −0.05550305 −0.16720693] 0 1.0 False\\n[−1.81215532 −1.25382597 −0.05884719 0.10746281] 1 1.0 False\\n[−1.83723184 −1.05791226 −0.05669793 −0.20319016] 0 1.0 False\\n[−1.85839008 −1.25217946 −0.06076174 0.07108213] 1 1.0 False\\n[−1.88343367 −1.05624141 −0.05934009 −0.24013553] 0 1.0 False\\n[−1.9045585 −1.25046768 −0.0641428 0.03325534] 0 1.0 False\\n[−1.92956785 −1.44461395 −0.0634777 0.30503082] 1 1.0 False\\n[−1.95846013 −1.24864754 −0.05737708 −0.00697668] 0 1.0 False\\n[−1.98343308 −1.44290169 −0.05751661 0.26706543] 1 1.0 False\\n[−2.01229112 −1.24700805 −0.05217531 −0.04318969] 0 1.0 False\\n[−2.03723128 −1.44134451 −0.0530391 0.23258573] 1 1.0 False\\n[−2.06605817 −1.24550639 −0.04838738 −0.07634453] 1 1.0 False\\n[−2.09096829 −1.04972537 −0.04991428 −0.38389269] 0 1.0 False\\n40',\n",
       " '5.6. Evaluation and Results\\n[−2.1119628 −1.24410443 −0.05759213 −0.10735593] 0 1.0 False\\n[−2.13684489 −1.43835581 −0.05973925 0.16661531] 1 1.0 False\\n[−2.16561201 −1.24243186 −0.05640694 −0.14429925] 0 1.0 False\\n[−2.19046064 −1.43670254 −0.05929293 0.13006845] 1 1.0 False\\n[−2.21919469 −1.24078355 −0.05669156 −0.18071551] 0 1.0 False\\n[−2.24401037 −1.43505037 −0.06030587 0.09355819] 1 1.0 False\\n[−2.27271137 −1.23911823 −0.0584347 −0.21752494] 0 1.0 False\\n[−2.29749374 −1.43335827 −0.0627852 0.05616779] 1 1.0 False\\n[−2.3261609 −1.23739489 −0.06166185 −0.25564483] 0 1.0 False\\n[−2.3509088 −1.43158473 −0.06677474 0.01696925] 0 1.0 False\\n[−2.3795405 −1.62568867 −0.06643536 0.28785909] 1 1.0 True\\n[−2.41205427 −1.42968531 −0.06067818 −0.02501499] total_reward : 81.0\\n5.6.4. Threats to Validity\\nThreats to external validityOur experiment aims to validate the use of the test case gener-\\nator for evaluating reinforcement learning agents. We selected a total of six agents from two\\ndifferent environments—CartPole and LunarLander—ranked at different positions in the public\\nleaderboard. This selection strategy was designed to capture a range of agent performances\\nand architectures, thereby reducing the risk of bias and improving the generalizability of our\\nfindings.\\nHowever, we observed that in some cases, the agents’ behavior did not align with expectations\\nbased on their leaderboard rankings. The most notable example was the agent Shakti, which,\\ndespite being ranked highest among the CartPole agents, showed the worst performance during\\nnormal execution, with a success rate of only 14%.\\nSimilarly, although the selected LunarLander agents were distributed across different leader-\\nboard positions, all of them achieved average rewards above 200 in our experiments. In partic-\\nular, the agent Rokenes achieved both the highest average reward and the highest success rate,\\ndespite not being the top-ranked agent in the public leaderboard.\\n41',\n",
       " '5. Experimental Design\\nThese discrepancies suggest that leaderboard rankings may not fully reflect agent robustness un-\\nder different testing conditions, and highlight the importance of evaluating agents using diverse\\nand controlled scenarios.\\nThreats to internal validityOne potential threat to internal validity arises from the specific\\nhandling required for the LunarLander environment. In this case, it was necessary to validate\\nwhether the sampled states could be considered valid initial states, since the environment can\\nonly be initialized from a limited region of the state space.\\nBy default, LunarLander starts from normalized coordinates(x= 0.0, y≈1.41), which corre-\\nspond to approximately(10.0, 13.33) in Box2D coordinates. Any perturbation or state sampled\\noutside of this valid initialization region would not be accepted by the environment as a true\\nstarting state.\\nAs a result, the number of valid samples per region was often reduced, leading to smaller overall\\nsample sizes for these agents. This constraint may have introduced a bias in the testing data or\\naffected the statistical power of the results in the LunarLander experiments.\\n42',\n",
       " '43\\nChapter 6\\nConclusion, Limitations and Future Work\\n6.1. Conclusion\\nThis thesis explores the problem of testing reinforcement learning agents through abstract classes\\nand partitions.\\nWe proposedMarTest-Pipeline, an implementation that generates abstract representations of\\nagent behavior, uses a transformer conditioned on reward and failure probability to identify\\nfailure-prone regions, and constructs targeted test cases from those regions.\\nOur results demonstrated that this approach can uncover regions of reduced performance in\\nmultiple agents, including those with perfect scores under normal execution. These findings\\nsupport the idea that standard agent evaluations may overlook critical edge cases and that\\nsymbolic representations—when combined with generative models—can enhance the testing and\\nunderstanding of agent behavior.\\nBeyondtheimplementation, thisworkproposesabroaderconceptualview: thattheinteraction\\nbetween an agent and its environment can be seen as a form of emergent language. Reward,\\nin this context, is not only a signal of performance, but a reflection of how well the agent has\\ninternalized a language and this thesis constitutes the first step in a research direction.',\n",
       " '6. Conclusion, Limitations and Future Work\\n6.2. Limitations\\nThis work presents a novel approach to testing reinforcement learning agents using a symbolic\\ngenerator based on transformer models and abstract state-action representations. While the\\nresults are promising and demonstrate the generator’s ability to identify failure-prone regions,\\nseveral limitations must be acknowledged:\\n• Dependence on initial state space constraints:In the LunarLander environment,\\nonly a limited subset of states can be used as valid initial states. This required an explicit\\nvalidation step to ensure sampled states could be used for environment initialization. As\\na consequence, the effective sample size for LunarLander agents was reduced, potentially\\nlimiting the statistical power of the results.\\n• Uneven sample sizes across agents:The test case generator attempts to sample up to\\n100 states per high-risk region. However, some abstract classes contained fewer than 100\\nstates in both CartPole and LunarLander environments, resulting in variation in sample\\nsizes across agents and environments. This may have introduced some imbalance in the\\nstatistical comparisons.\\n• Fixed abstraction level:All experiments were conducted using a fixed abstraction level\\nof d = 4 . While this value provided a reasonable trade-off between expressiveness and\\ngenerality, it is possible that a different abstraction depth might yield better or more\\ninterpretable results depending on the agent and environment.\\n• Limited evaluation scope: The approach was tested only on agents from two envi-\\nronments: CartPole and LunarLander. Although these environments differ in complexity\\nand dynamics, further evaluation on more diverse tasks—particularly continuous control\\nand high-dimensional environments—is needed to assess the general applicability of the\\nmethod.\\n• Transformer conditioning is limited:The transformer model was trained conditioned\\nonly on the average reward and failure probability. While this was sufficient for the current\\nexperiments, future versions could incorporate richer contextual information (e.g., policy\\n44',\n",
       " '6.3. Future Work\\nentropy, time horizon, or agent architecture) to improve the diversity and precision of\\ngenerated test cases.\\n• Mismatchwithleaderboardexpectations: Insomecases, theperformanceobservedin\\nnormal execution did not match the agent’s leaderboard ranking. For example, the Shakti\\nagent, which held the top rank among CartPole submissions, showed the lowest success\\nrate in our experiments. This highlights the need to interpret leaderboard positions with\\ncaution and emphasizes the value of independent and reproducible testing frameworks.\\n6.3. Future Work\\nOne potential extension is to design abstraction mechanisms that are adaptive—i.e., where the\\nabstraction depth is learned based on agent behavior or performance metrics. Another important\\ndirection for future work is to implement alternative definitions of abstract classes. This is\\nespecially relevant, as it may provide better insight into the interaction between the agent and the\\nenvironment. Lastly, incorporating richer conditioning signals into the Transformer, or exploring\\narchitectures specifically tailored for causal reasoning in RL environments, may improve the\\nquality of the generated test cases and help uncover more nuanced weaknesses in agent policies.\\nThis work was guided by the intuition thatreward may be a consequence of a shared language\\nbetween the agent and the environment. While this idea is still in an early and exploratory stage,\\nit suggests several possible directions for future research.\\nOne such direction is to investigate the relationship between the level of abstraction used\\nto represent agent-environment interactions and the rewards obtained. It may be possible to\\nformulate an optimization process—either analytical or learned—that selects the abstraction\\nlevel most appropriate for a given task or agent architecture.\\nAnother line of inquiry involves a closer integration between abstract symbolic representations\\nand transformer-based models. Rather than conditioning the transformer on simple reward\\nstatistics, future models could be trained to evaluate or even generate abstract behavior trajec-\\ntories as structured linguistic sequences—potentially assigning meaning and structure to patterns\\nof interaction.\\n45',\n",
       " '6. Conclusion, Limitations and Future Work\\nAlthough preliminary, these ideas point to a broader aspiration: to better understand the role\\nof representation, communication, and interpretation in reinforcement learning. By approaching\\nthe agent-environment loop as a form of emergent communication, we hope to take small steps\\ntoward a more unified view of learning, abstraction, and meaning.\\n46',\n",
       " '47\\nChapter A\\nAppendix\\nA.1. MartTest Pipeline Requirements\\nabsl -py ==1.4.0\\naccelerate ==1.6.0\\nanyio ==3.6.2\\nargon2 - cffi ==21.3.0\\nargon2 -cffi - bindings ==21.2.0\\narrow ==1.2.3\\nasttokens ==2.2.1\\nastunparse ==1.6.3\\nattrs ==22.2.0\\nBabel ==2.11.0\\nbackcall ==0.2.0\\nbeautifulsoup4 ==4.11.1\\nbleach ==5.0.1\\nbox2d -py ==2.3.5\\ncachetools ==5.2.1\\ncertifi ==2022.12.7\\ncffi ==1.15.1\\ncharset - normalizer ==3.0.1\\ncloudpickle ==3.1.1',\n",
       " 'A. Appendix\\ncomm ==0.1.2\\ncontourpy ==1.0.7\\ncycler ==0.11.0\\ndebugpy ==1.6.5\\ndecorator ==5.1.1\\ndefusedxml ==0.7.1\\nentrypoints ==0.4\\nexecuting ==1.2.0\\nfastjsonschema ==2.16.2\\nfilelock ==3.18.0\\nflatbuffers ==23.1.4\\nfonttools ==4.38.0\\nfqdn ==1.5.1\\nfsspec ==2025.3.2\\ngast ==0.4.0\\ngoogle - auth ==2.16.0\\ngoogle -auth - oauthlib ==0.4.6\\ngoogle - pasta ==0.2.0\\ngrpcio ==1.51.1\\ngym ==0.26.2\\ngym - notices ==0.0.8\\nh5py ==3.7.0\\nhuggingface - hub ==0.30.2\\nidna ==3.4\\nipykernel ==6.20.2\\nipython ==8.8.0\\nipython - genutils ==0.2.0\\nisoduration ==20.11.0\\njedi ==0.18.2\\nJinja2 ==3.1.2\\njoblib ==1.4.2\\njson5 ==0.9.11\\n48',\n",
       " 'A.1. MartTest Pipeline Requirements\\njsonpointer ==2.3\\njsonschema ==4.17.3\\njupyter - events ==0.6.3\\njupyter_client ==7.4.9\\njupyter_core ==5.1.3\\njupyter_server ==2.1.0\\njupyter_server_terminals ==0.4.4\\njupyterlab ==3.5.2\\njupyterlab - pygments ==0.2.2\\njupyterlab_server ==2.19.0\\nkeras ==2.11.0\\nkiwisolver ==1.4.4\\nlibclang ==15.0.6.1\\nMarkdown ==3.4.1\\nMarkupSafe ==2.1.1\\nmatplotlib ==3.6.3\\nmatplotlib - inline ==0.1.6\\nmistune ==2.0.4\\nmpmath ==1.3.0\\nnbclassic ==0.4.8\\nnbclient ==0.7.2\\nnbconvert ==7.2.8\\nnbformat ==5.7.3\\nnest - asyncio ==1.5.6\\nnetworkx ==3.3\\nnotebook ==6.5.2\\nnotebook_shim ==0.2.2\\nnumpy ==1.24.1\\nnvidia - cublas - cu11 ==11.10.3.66\\nnvidia -cuda - nvrtc - cu11 ==11.7.99\\nnvidia -cuda - runtime - cu11 ==11.7.99\\nnvidia - cudnn - cu11 ==8.5.0.96\\n49',\n",
       " 'A. Appendix\\noauthlib ==3.2.2\\nopt - einsum ==3.3.0\\npackaging ==23.0\\npandas ==2.2.3\\npandocfilters ==1.5.0\\nparso ==0.8.3\\npexpect ==4.8.0\\npickleshare ==0.7.5\\nPillow ==9.4.0\\nplatformdirs ==2.6.2\\nprometheus - client ==0.15.0\\nprompt - toolkit ==3.0.36\\nprotobuf ==3.19.6\\npsutil ==5.9.4\\nptyprocess ==0.7.0\\npure -eval ==0.2.2\\npyasn1 ==0.4.8\\npyasn1 - modules ==0.2.8\\npycparser ==2.21\\npygame ==2.1.0\\nPygments ==2.14.0\\npyparsing ==3.0.9\\npyrsistent ==0.19.3\\npython - dateutil ==2.8.2\\npython -json - logger ==2.0.4\\npytz ==2022.7.1\\nPyYAML ==6.0\\npyzmq ==25.0.0\\nregex ==2024.11.6\\nrequests ==2.28.2\\nrequests - oauthlib ==1.3.1\\nrfc3339 - validator ==0.1.4\\n50',\n",
       " 'A.1. MartTest Pipeline Requirements\\nrfc3986 - validator ==0.1.1\\nrsa ==4.9\\nsafetensors ==0.5.3\\nscikit - learn ==1.6.1\\nscipy ==1.15.2\\nSend2Trash ==1.8.0\\nsix ==1.16.0\\nsniffio ==1.3.0\\nsoupsieve ==2.3.2. post1\\nstack - data ==0.6.2\\nswig ==4.3.0\\nsympy ==1.13.3\\ntensorboard ==2.11.2\\ntensorboard -data - server ==0.6.1\\ntensorboard - plugin - wit ==1.8.1\\ntensorflow ==2.11.0\\ntensorflow - estimator ==2.11.0\\ntensorflow -io -gcs - filesystem ==0.29.0\\ntermcolor ==2.2.0\\nterminado ==0.17.1\\nthreadpoolctl ==3.6.0\\ntinycss2 ==1.2.1\\ntokenizers ==0.21.1\\ntomli ==2.0.1\\ntorch ==2.1.2+ cu118\\ntorchaudio ==2.1.2+ cu118\\ntorchvision ==0.16.2+ cu118\\ntornado ==6.2\\ntqdm ==4.67.1\\ntraitlets ==5.8.1\\ntransformers ==4.51.3\\ntriton ==2.1.0\\n51',\n",
       " 'A. Appendix\\ntyping_extensions ==4.4.0\\ntzdata ==2025.1\\nuri - template ==1.2.0\\nurllib3 ==1.26.14\\nwcwidth ==0.2.6\\nwebcolors ==1.12\\nwebencodings ==0.5.1\\nwebsocket - client ==1.4.2\\nWerkzeug ==2.2.2\\nwrapt ==1.14.1\\nSnippet A.1: Paquetes y versiones\\nA.2. Conditioned Transformer Model CondGPT2\\nCondGPT2 (\\n( transformer ): GPT2Model (\\n( wte ): Embedding (996 , 1024)\\n( wpe ): Embedding (1024 , 1024)\\n( drop ): Dropout (p =0.1 , inplace = False )\\n(h): ModuleList (\\n(0 -11) : 12 x GPT2Block (\\n( ln_1 ): LayerNorm ((1024 ,) , eps =1e -05 , elementwise_affine = True )\\n( attn ): GPT2Attention (\\n( c_attn ): Conv1D (nf =3072 , nx =1024)\\n( c_proj ): Conv1D (nf =1024 , nx =1024)\\n( attn_dropout ): Dropout (p =0.1 , inplace = False )\\n( resid_dropout ): Dropout (p =0.1 , inplace = False )\\n)\\n( ln_2 ): LayerNorm ((1024 ,) , eps =1e -05 , elementwise_affine = True )\\n( mlp ): GPT2MLP (\\n( c_fc ): Conv1D (nf =4096 , nx =1024)\\n52',\n",
       " 'A.3. Requirements Shakti\\n( c_proj ): Conv1D (nf =1024 , nx =4096)\\n( act ): NewGELUActivation ()\\n( dropout ): Dropout (p =0.1 , inplace = False )\\n)\\n)\\n)\\n( ln_f ): LayerNorm ((1024 ,) , eps =1e -05 , elementwise_affine = True )\\n)\\n( lm_head ): Linear ( in_features =1024 , out_features =996 , bias = False )\\n( cond_proj ): Sequential (\\n(0) : Linear ( in_features =2 , out_features =1024 , bias = True )\\n(1) : ReLU ()\\n(2) : Linear ( in_features =1024 , out_features =1024 , bias = True )\\n)\\n)\\nA.3. Requirements Shakti\\nabsl -py ==2.1.0\\nastor ==0.8.1\\ncachetools ==4.2.4\\ncertifi ==2025.1.31\\ncharset - normalizer ==3.4.1\\ncloudpickle ==1.2.2\\ncycler ==0.11.0\\nfuture ==1.0.0\\ngast ==0.2.2\\ngoogle - auth ==1.35.0\\ngoogle -auth - oauthlib ==0.4.6\\ngoogle - pasta ==0.2.0\\ngrpcio ==1.62.3\\n53',\n",
       " 'A. Appendix\\ngym ==0.15.4\\nh5py ==3.8.0\\nidna ==3.10\\nimportlib - metadata ==6.7.0\\nKeras ==2.3.1\\nKeras - Applications ==1.0.8\\nKeras - Preprocessing ==1.1.2\\nkiwisolver ==1.4.5\\nMarkdown ==3.4.4\\nMarkupSafe ==2.1.5\\nmatplotlib ==3.1.2\\nnumpy ==1.18.1\\noauthlib ==3.2.2\\nopencv - python ==4.11.0.86\\nopt - einsum ==3.3.0\\nPillow ==9.5.0\\nprotobuf ==3.20.3\\npyasn1 ==0.5.1\\npyasn1 - modules ==0.3.0\\npyglet ==1.3.2\\npyparsing ==3.1.4\\npython - dateutil ==2.9.0. post0\\nPyYAML ==6.0.1\\nrequests ==2.31.0\\nrequests - oauthlib ==2.0.0\\nrsa ==4.9\\nscipy ==1.4.1\\nsix ==1.17.0\\ntensorboard ==2.1.1\\ntensorflow - estimator ==2.1.0\\ntensorflow - gpu ==2.1.0\\ntermcolor ==2.3.0\\n54',\n",
       " 'A.4. Requirements Kapil\\ntorch ==1.2.0+ cu92\\ntorchvision ==0.4.0+ cu92\\ntyping_extensions ==4.7.1\\nurllib3 ==2.0.7\\nWerkzeug ==2.2.3\\nwrapt ==1.16.0\\nzipp ==3.15.0\\nA.4. Requirements Kapil\\nabsl -py ==2.1.0\\nastor ==0.8.1\\nbackcall ==0.2.0\\ncloudpickle ==1.2.2\\ncycler ==0.11.0\\ndebugpy ==1.7.0\\ndecorator ==5.1.1\\nentrypoints ==0.4\\nfuture ==1.0.0\\ngast ==0.2.2\\ngoogle - pasta ==0.2.0\\ngrpcio ==1.62.3\\ngym ==0.14.0\\nh5py ==3.8.0\\nimportlib - metadata ==6.7.0\\nipykernel ==6.16.2\\nipython ==7.34.0\\njedi ==0.19.2\\njupyter_client ==7.4.9\\njupyter_core ==4.12.0\\nKeras ==2.3.1\\n55',\n",
       " 'A. Appendix\\nKeras - Applications ==1.0.8\\nKeras - Preprocessing ==1.1.2\\nkiwisolver ==1.4.5\\nMarkdown ==3.4.4\\nMarkupSafe ==2.1.5\\nmatplotlib ==2.2.4\\nmatplotlib - inline ==0.1.6\\nnest - asyncio ==1.6.0\\nnumpy ==1.21.6\\nopt - einsum ==3.3.0\\npackaging ==24.0\\nparso ==0.8.4\\npexpect ==4.9.0\\npickleshare ==0.7.5\\nprompt_toolkit ==3.0.48\\nprotobuf ==3.20.3\\npsutil ==7.0.0\\nptyprocess ==0.7.0\\npyglet ==1.3.2\\nPygments ==2.17.2\\npyparsing ==3.1.4\\npython - dateutil ==2.9.0. post0\\npytz ==2025.1\\nPyYAML ==6.0.1\\npyzmq ==26.2.1\\nscipy ==1.7.3\\nsix ==1.17.0\\ntensorboard ==1.15.0\\ntensorflow ==1.15.0\\ntensorflow - estimator ==1.15.1\\ntermcolor ==2.3.0\\ntornado ==6.2\\n56',\n",
       " 'A.5. Requirements Nihal\\ntraitlets ==5.9.0\\ntyping_extensions ==4.7.1\\nwcwidth ==0.2.13\\nWerkzeug ==2.2.3\\nwrapt ==1.16.0\\nzipp ==3.15.0\\nA.5. Requirements Nihal\\nabsl -py ==2.1.0\\nastor ==0.8.1\\nbackcall ==0.2.0\\ncachetools ==4.2.4\\ncertifi ==2025.1.31\\ncharset - normalizer ==3.4.1\\ncloudpickle ==1.2.2\\ncycler ==0.11.0\\ndebugpy ==1.7.0\\ndecorator ==5.1.1\\nentrypoints ==0.4\\nfuture ==1.0.0\\ngast ==0.2.2\\ngoogle - auth ==1.35.0\\ngoogle -auth - oauthlib ==0.4.6\\ngoogle - pasta ==0.2.0\\ngrpcio ==1.62.3\\ngym ==0.15.4\\nh5py ==3.8.0\\nidna ==3.10\\nimportlib - metadata ==6.7.0\\nipykernel ==6.16.2\\n57',\n",
       " 'A. Appendix\\nipython ==7.34.0\\njedi ==0.19.2\\njupyter_client ==7.4.9\\njupyter_core ==4.12.0\\nKeras - Applications ==1.0.8\\nKeras - Preprocessing ==1.1.2\\nkiwisolver ==1.4.5\\nMarkdown ==3.4.4\\nMarkupSafe ==2.1.5\\nmatplotlib ==3.1.2\\nmatplotlib - inline ==0.1.6\\nnest - asyncio ==1.6.0\\nnumpy ==1.21.6\\noauthlib ==3.2.2\\nopencv - python ==4.11.0.86\\nopt - einsum ==3.3.0\\npackaging ==24.0\\nparso ==0.8.4\\npexpect ==4.9.0\\npickleshare ==0.7.5\\nprompt_toolkit ==3.0.48\\nprotobuf ==3.20.3\\npsutil ==7.0.0\\nptyprocess ==0.7.0\\npyasn1 ==0.5.1\\npyasn1 - modules ==0.3.0\\npyglet ==1.3.2\\nPygments ==2.17.2\\npyparsing ==3.1.4\\npython - dateutil ==2.9.0. post0\\npyzmq ==26.2.1\\nrequests ==2.31.0\\n58',\n",
       " 'A.6. Requirements Khordoo\\nrequests - oauthlib ==2.0.0\\nrsa ==4.9\\nscipy ==1.4.1\\nsix ==1.17.0\\ntensorboard ==2.1.1\\ntensorflow - estimator ==2.1.0\\ntensorflow - gpu ==2.1.0\\ntermcolor ==2.3.0\\ntornado ==6.2\\ntraitlets ==5.9.0\\ntyping_extensions ==4.7.1\\nurllib3 ==2.0.7\\nwcwidth ==0.2.13\\nWerkzeug ==2.2.3\\nwrapt ==1.16.0\\nzipp ==3.15.0\\nA.6. Requirements Khordoo\\nbox2d -py ==2.3.8\\ncloudpickle ==1.2.2\\nfuture ==1.0.0\\ngym ==0.15.4\\nnumpy ==1.18.1\\nopencv - python ==4.11.0.86\\nPillow ==9.5.0\\npyglet ==1.3.2\\nscipy ==1.7.3\\nsix ==1.17.0\\ntorch ==1.2.0+ cu92\\ntorchvision ==0.4.0+ cu92\\n59',\n",
       " 'A. Appendix\\nA.7. Requirements Sanket\\nbox2d -py ==2.3.4\\ncloudpickle ==1.2.2\\nfuture ==1.0.0\\ngym ==0.15.4\\nnumpy ==1.21.6\\nopencv - python ==4.11.0.86\\nPillow ==9.5.0\\npyglet ==1.3.2\\nscipy ==1.7.3\\nsix ==1.17.0\\ntorch ==1.2.0+ cu92\\ntorchvision ==0.4.0+ cu92\\nA.8. Requirements Rokenes\\nabsl -py ==2.1.0\\nastor ==0.8.1\\nbox2d -py ==2.3.4\\ncloudpickle ==1.2.2\\nfuture ==1.0.0\\ngast ==0.6.0\\ngoogle - pasta ==0.2.0\\ngrpcio ==1.62.3\\ngym ==0.15.4\\nh5py ==3.8.0\\nimportlib - metadata ==6.7.0\\nKeras - Applications ==1.0.8\\nKeras - Preprocessing ==1.1.2\\nMarkdown ==3.4.4\\nMarkupSafe ==2.1.5\\n60',\n",
       " 'A.8. Requirements Rokenes\\nnumpy ==1.21.6\\nopencv - python ==4.11.0.86\\nprotobuf ==3.20.3\\npyglet ==1.3.2\\nscipy ==1.7.3\\nsix ==1.17.0\\ntensorboard ==1.14.0\\ntensorflow ==1.14.0\\ntensorflow - estimator ==1.14.0\\ntermcolor ==2.3.0\\ntyping_extensions ==4.7.1\\nWerkzeug ==2.2.3\\nwrapt ==1.16.0\\nzipp ==3.15.0\\n61',\n",
       " '',\n",
       " '63\\nBibliography\\nThe references are sorted alphabetically by first author.\\n[1] Lionel C. Briand Fellow IEEE Mojtaba Bagherzadeh Amirhossein Zolfagharian, Manel Ab-\\ndellatif and Ramesh S.A Search-Based Testing Approach for Deep Reinforcement Learning\\nAgents. IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, vol. 49, no. 7 edition,\\n2023. ISBN 0-201-37921-X.\\n[2] Matteo Biagiola and Paolo Tonella.Testing the Plasticity of Reinforcement Learning Based\\nSystems. Università della Svizzera italiana, Switzerland, revised edition, 2022. ISBN 0-201-\\n37921-X.\\n[3] Matteo Biagiola and Paolo Tonella.Testing of Deep Reinforcement Learning Agents with\\nSurrogate Models. Università della Svizzera italiana, Switzerland, revised edition, 2023.\\nISBN 0-201-37921-X.\\n[4] Kapil Chauhan. CartPole_DQN: CartPole_v0 Jupyter Notebook. https://github.\\ncom/kapilnchauhan77/CartPole_DQN/blob/master/CartPole_v0.ipynb, 2019. Accessed:\\nMay 15, 2025.\\n[5] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.Deep Learning. MIT Press, 2016.\\nhttp://www.deeplearningbook.org.\\n[6] IBM. What is natural language processing? https://www.ibm.com/think/topics/\\nnatural-language-processing, n.d. Accessed: May 23, 2025.\\n[7] Mahmood Khordoo. Deep-Reinforcement-Learning-with-PyTorch: n-\\nstep DQN for LunarLander-v2. https://github.com/khordoo/',\n",
       " 'Bibliography\\nDeep-Reinforcement-Learning-with-PyTorch/blob/example/examples/DQN/\\nlunarlander_v2-dqn-n-step.py , 2020. Accessed: May 15,2025.\\n[8] Shakti Kumar. adaptiveSystems: RL_Benchmarks README. https://github.\\ncom/shaktikshri/adaptiveSystems/blob/master/RL_Benchmarks/README.md, 2019. Ac-\\ncessed: May 15, 2025.\\n[9] Emmanouil D. Oikonomou, Petros Karvelis, Nikolaos Giannakeas, Aristidis Vrachatis, Evri-\\npidis Glavas, and Alexandros T. Tzallas. How natural language processing derived tech-\\nniques are used on biological data: a systematic review. Network Modeling Analysis in\\nHealth Informatics and Bioinformatics, 13(23), 2024. DOI 10.1007/s13721-024-00458-1.\\n[10] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\nunderstanding by generative pre-training. https://cdn.openai.com/research-covers/\\nlanguage-unsupervised/language_understanding_paper.pdf, 2018. OpenAI Report.\\n[11] Nihal T. Rao. RL-Double-DQN: Double DQN Implementation for CartPole-v0. https:\\n//github.com/nihal-rao/RL-Double-DQN , 2020. Accessed: May 15, 2025.\\n[12] SigveRokenes. learning-rl/gym/lunarlander-v2: DQNExampleforLunarLander-v2. https:\\n//github.com/evgiz/learning-rl/tree/master/gym/lunarlander-v2, 2019. Accessed:\\nMay 15, 2025.\\n[13] Richard S. Sutton and Andrew G. Barto.Reinforcement Learning: An Introduction. The\\nMIT Press Cambridge, Massachusetts,London, England, revised edition, 2015. ISBN 0-201-\\n37921-X.\\n[14] Sanket Thakur. LunarLander_DQN: DQN Implementation for LunarLander-\\nv2. https://github.com/sanketsans/openAIenv/blob/master/DQN/LunarLander/\\nLunarLander_DQN.ipynb, 2020. Accessed: May 15, 2025.\\n[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\\nGomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In\\nAdvances in Neural Information Processing Systems , volume 30. Curran Asso-\\n64',\n",
       " 'Bibliography\\nciates, Inc., 2017. URL https://papers.nips.cc/paper_files/paper/2017/file/\\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\n65',\n",
       " 'Bibliography\\n66']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6674bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    SentenceTransformersTokenTextSplitter,\n",
    ")\n",
    "\n",
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\",\"\\n\",\". \", \" \", \"\"],\n",
    "    chunk_size = 1000, \n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "##### split recursive method\n",
    "character_split_texts = character_splitter.split_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a71ece76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_map_characters = map(lambda x: len(x) , character_split_texts)\n",
    "max(len_map_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce70ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "## \n",
    "######\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_overlap=0,tokens_per_chunk=256\n",
    ")\n",
    "\n",
    "token_split_texts = []\n",
    "for text in character_split_texts:\n",
    "    token_split_texts += token_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c5739f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maestria en ingenieria de sistemas y computacion dissertation constructing a language for testing reinforcement learning programs using nlp techniques luis alejandro medina july 19, 2025 this thesis is submitted in partial fulfillment of the requirements for a degree of master in systems and computing engineering ( misis ). thesis committee : prof. nicolas cardozo ( promotor ) universidad de los andes, colombia prof. reviewer 1ivana dusparic trinity college dublin, ireland prof. reviewer 2ruben manrique universidad de los andes, colombia constructing a language for testing reinforcement learning pro - grams using nlp techniques © year luis alejandro medina systems and computing engineering department flag lab faculty of engineering universidad de los andes bogota, colombia',\n",
       " 'with all my heart : to my dear dad, luis hernando medina, everything i have achieved is thanks to you. to all my family, thank you for never abandoning me. and to my beloved martin, i will always love you.... and i will always be two steps behind you. “ two steps behind ”, def leppard.',\n",
       " 'iv abstract reinforcementlearning ( rl ) hasgarneredsignificantattentionfromtheresearchcommunitydue to its expanding applications across various fields, including recommendation systems, robotics, autonomous vehicles, and more. consequently, there has been a growing interest in testing deep reinforcement learning ( drl ) agents and identifying potential faults that could lead to critical failures during their execution. various techniques exist for testing systems, including white - box, black - box, and data - quality testing methods. white - box techniques rely on the assumption that complete information about the system ( e. g., code, data, and architecture ) is available. black - box techniques, on the other hand, operate under the premise that no internal details of the system are accessible, focusing instead on assessing the system ’ s performance and correctness based solely on specified require - ments. due to the potentially vast amount of data generated by these systems, data - quality',\n",
       " 'techniques leverage this information to infer system behavior and detect possible issues. in ( rl ), various methods exist for the testing and evaluation of agents. adversarial attacks, focusing on perturbing the raw input of the rl agent, mutation testing and debuggers focus on assessing the code, statistical methods for building performance metrics and search - based testing focusing on the data produced by the agent as genetics algorithms, plasticity maps, and constructing language and natural language process ( nlp ) techniques. constructing language models using nlp techniques for testing in reinforcement learning is based on the idea of communication between the agent and the environment, thereby establishing a shared language. an inference language is key to finding ways to detect possible agent failures, incorrect behaviors, and low - reward outcomes. our evaluation uses different implementations from two rl application benchmarks — cartpole',\n",
       " 'and lunarlander — to generate testing scenarios for each. we analyze two key metrics : the',\n",
       " 'average rewardand theprobability of fault. this is important because agents can achieve high average rewards that may overcompensate for cases with lower performance. however, our results suggest that the proposed technique is capable of identifyingregions of lower reward. for example, in the case of theshakti agent, the average reward during normal execution is 185. 3, whereas in our testing scenarios, it drops to 96. 98. this indicates that our test case generator can uncover regions of reduced performance. on the other hand, even for thekapil agent, which consistently shows a perfect success rate during normal execution, we were able to identify testing regions where the success rate drops significantly — to 42. 8 %. the results indicate that the proposed test case generator was able to identify failure - prone regions and lower rewards regions in multiple agents — even in those with perfect performance',\n",
       " 'under normal conditions. statistical analysis confirmed that, for most agents, the generated test cases led to significantly lower success rates or reduced average rewards compared to standard execution. v',\n",
       " 'vi contents abstract............................................. iv list of figures......................................... viii 1. introduction......................................... 1 2. background......................................... 5 2. 1. reinforment learning, makov process desicion ( mdp )............... 5 2. 2. natural language processing............................. 6',\n",
       " '2. 2. 1. early neural network - based models..................... 6 2. 3. abstract classes.................................... 10 2. 3. 1. definition π∗ - irrelevance abstraction..................... 10 2. 3. 2. definition q∗ - irrelevance abstraction..................... 10',\n",
       " '2. 3. 3. relax q∗ - irrelevance abstraction....................... 10 3. martest : an nlp approach for test sequence generation............... 11 3. 1. vocabulary....................................... 11 3. 2. corpus.......................................... 12 3. 3. dataset......................................... 12 3. 3. 1. average reward................................. 12 3. 3. 2',\n",
       " '. failure probability............................... 12 3. 3. 3. random forest................................. 13 3. 4. model - conditioned transformer............................ 14 3. 4. 1. assumptions.................................. 14',\n",
       " '4. martest pipeline...................................... 15 4. 1. pipeline implementation................................ 15 4. 2. module 1 : data acquisition and preparation..................... 16 4. 2. 1. inputs training and execution logs...................... 16 4. 2. 2. data balacing................................. 16 4. 2. 3. q - values.................................... 17 4. 3. module 2 : abstract classes........',\n",
       " '...................... 17 4. 3. 1. construction.................................. 17 4. 4. module 3........................................ 17 4. 4. 1. abstract episodes generation......................... 17',\n",
       " '4. 5. module 4........................................ 18 4. 5. 1. random forest model............................. 18 4. 6. module 5........................................ 18 4. 6. 1. conditioned transformer........................... 18 4. 6. 2. test regions cases............................... 19 5. experimental design.................................... 21 5. 1.',\n",
       " 'cartpole - v0 agents................................... 22',\n",
       " 'contents 5. 2. lunarlander - v2 agents................................ 24 5. 2. 1. sanket thakur ( sanket )............................ 24 5. 2. 2. mahmood khordoo ( khordoo )........................ 24 5. 2. 3. sigve rokenes ( rokenes )............................ 25 5. 3. conditioned transformer configurations and metrics................ 25 5. 3. 1. conditional transformer parameters for cartpole.............. 26 5. 3. 2. conditional transformer parameters for lunar - lander........',\n",
       " '... 26 5. 3. 3. training and validations loss......................... 26 5. 4. concrete test case generator............................. 29 5. 5. an example....................................... 30',\n",
       " '5. 6. evaluation and results................................. 31 5. 6. 1. cartpole..................................... 31 5. 6. 2. lunar lander.................................. 33 5. 6. 3. a small example................................ 36 5. 6. 4. threats to validity............................... 41 6. conclusion, limitations and future work........................ 43 6. 1. conclusion....',\n",
       " '................................... 43 6. 2. limitations....................................... 44 6. 3. future work...................................... 45 a. appendix.......................................... 47',\n",
       " 'a. 1. marttest pipeline requirements........................... 47 a. 2. conditioned transformer model condgpt2..................... 52 a. 3. requirements shakti.................................. 53 a. 4. requirements kapil.................................. 55 a. 5. requirements nihal.................................. 57 a. 6. requirements khordoo................................. 59 a. 7. requirements sanket........',\n",
       " '.......................... 60 a. 8. requirements rokenes................................. 60 bibliography.......................................... 63 vii',\n",
       " 'viii list of figures 2. 1. the transformer architecture, attention is all you need, 2017........... 8 2. 2. transformer achitecture, improving language understanding by generative pre - training.......................................... 9 2. 3. conditionedtransformer, [UNK] pf arefirstprocessedby an auxiliary neural network, then added element - wise to the word - token embeddings. 10 4. 1. martest pipeline..................................... 16 5. 1. cartpole - v0........................................ 23 5. 2. lunarlander - v2................',\n",
       " '...................... 25 5. 3. training and evaluation loss per epoch, shakti.................. 27 5. 4. training and evaluation loss per epoch, kapil................... 27',\n",
       " '5. 5. training and evaluation loss per epoch, nihal................... 28 5. 6. training and evaluation loss per epoch, sanket.................. 28 5. 7. training and evaluation loss per epoch, khordoo................. 29 5. 8. training and evaluation loss per epoch, rokenes.................. 29 5. 9. initial state of the cartpole environment with cart position−0. 73915684, cart velocity −0. 89601650, pole angle−0. 03282847, and pole angular velocity0. 23753740. 37 5. 10. final state of the cartpole environment with cart position−2. 41205427, cart ve - locity −1. 42968531, pole angle−0. 06067818, and pole angular velocity−0. 02501499. 38',\n",
       " '1 chapter 1 introduction reinforment learning ( rl ) and natural language processing ( nlp ) have seen increasing inter - est and research advances in recent years. the combination of deep learning and rl, referred to as deep reinforcement learning ( drl ), has achieved success to solve various decision - making problems such as autonomous driving and robotics, while nlp has experienced a notable suc - cess with the introduction of transformer architecture, particularly the decoder - only transformer model and other architectures, such as recurrent neural networks ( rnns ) and long short - term memory ( lstms ) with their primary distinction lying in their capacity to handle memory with long sequences and parallel processing. drl has recently been applied in many practical contexts. for instance, netflix uses it to recommend which movie to show to a user in order to maximize engagement ( system of recom - mendation ) [ 3 ], microsoft developed personalizer, a service developer that can be use for content',\n",
       " 'recomendation [ 3 ]. other applications include the mastery of strategy games such as dota 2 and starcraft 2 in which it has defeated human players [ 2 ]. with the introduction of the paper \" attention is all you need \" vaswani et al. [ 15 ] published in 2017 where they introduce the transformer model laying down the foundation for modern nlp and sequence modeling, such techniques have influenced the development of architectures like bert, gpt, and other state - of - the - art nlp models, gaining huge popularity in all fields. the utility and use of nlp - based tools has been a great advantage for society and probably a principal engine for progress. for the constantly growing use of rl models in many fields, there has been a growing need',\n",
       " '1. introduction of testing such models. therefore, we have built a novel way for testing rl agents by bridging two fields. to reconcile both fields, we need to reconcile the infinite state spaces in the case of ( continuous ) rl agents, and the finite vocabulary / alphabet used in nlp models. we put forward a language as a means to reconcile rl and nlp. through the execution of an agent, this is in continuous interaction with the environment establishing a language the proposed approach build such language is based on the mathematical concept of abstraction, often called ’ equivalences classes ’ or ’ abstracts classes ’. the vocabulary for testing an rl program consist of abstract classes inputs for nlp models, wich then can produce scenarios ( i. e., paths of abstract states - abstract paths ) with possible fails or lower rewards. nlp models model / generate abstracts paths, that is, sequences of state representative ab -',\n",
       " 'stracts classes and the actions. afterwards we validate that the generated abstract paths indeed are possible and lead to a diminished agent performance. to do this, we developed a test gen - erator cases program extracts regions from abstract paths and disrupt representative states in each class to observe the agent ’ s behavior. we evaluate our approach using agents selected from the github openai gym leaderboard. for each agent, we first generate logs from both training and execution, which serve as inputs to our implementation, martest - pipeline. based on the forecasted regions produced by the pipeline, we then construct concrete test cases to evaluate the agent ’ s performance. our results show that the implementation is able to identify regions associated with lower average rewards and reduced success rates. moreover, we demonstrate that the performance on the testing samples is statistically significantly lower than that observed under normal execution,',\n",
       " 'highlighting that high performance of an agent trained in a normal execution does not guarantee robustness under targeted testing conditions. thesis outline the remainder of this thesis is organized as follows : • chapter 2 – background : introduces the theoretical foundations relevant to this work, including reinforcement learning and markov decision processes ( mdps ), natural language processing models, and various forms of abstraction used to represent agent - environment 2',\n",
       " 'interactions symbolically. • chapter 3 – approach : describes the conceptual framework of the proposed method, including the symbolic vocabulary, abstract corpus construction, and the output model based on a conditioned transformer. assumptions and design decisions are also discussed. • chapter 4 – implementation : presents the detailed structure of themartest - pipeline, explaining each module in the pipeline, from the processing of logs and q - values to the generation of abstract episodes, classification models, and the conditioned transformer used to produce test case regions. • chapter 5 – experimental design : details the experimental setup, including the selected agents from thecartpole and lunarlander environments. it also describes how concrete test cases are generated from the abstract representations and evaluates the agents using statistical analysis. the chapter concludes with a discussion on threats to validity.',\n",
       " '• chapter 6 – limitations and future work : discusses the current limitations of the approach and outlines possible future directions, including the refinement of abstraction levels, integrationoflearning - basedrepresentations, anddeeperunificationofreinforcement learning and natural language processing techniques. 3',\n",
       " '1. introduction 4',\n",
       " '5 chapter 2 background to motivate the problem and to provide an overview of our approach we give the definitions of the concepts required to understand this work, regarding rl, nlp and abstract classes. 2. 1. reinforment learning, makov process desicion ( mdp ) reinforcement learning trains an agent that interacts with an environmento maximize the ob - served reward obtained from the environment after action execution, rl uses a trial and error strategy to explore the environment. at every state, the agent chooses an action to execute from the set of all possible actions for the state, and receive a corresponding reward for the executed action. a markov decision process ( mdp ) is defined as a 4 - tuple < s, a, t, r > where s is the set of continuous non - enumerable states ( | s | ≥ | n | ), a is the set of ( continuous ) actions, t is the transition function wheret : sxaxs → [ 0, 1 ] such thatt ( s, a, s ′ ) determines the probability',\n",
       " 'of reaching a states ′ by performing an actiona in state s, r : sxa →r a reward function that determines a reward for a pair of an action and a state. at each time step, the agent uses a mapping from state to probabilities of selecting each possible action. this mapping is called the agent ’ s policyπ : sxa → [ 0, 1 ] considered the solution of the mdp.',\n",
       " '2. background markov property markov propertyis defined as the environment ’ s response att + 1 depends only on the state and action representations att : pr { rt + 1 = r, st + 1 = s ′ | s0, a0, r1,.... rt, st, at } = pr { rt + 1 = r, st + 1 | st, at } for allr, s ′, st, at. [ 13 ]. 2. 2. natural language processing natural language processing ( nlp ) [ 6 ] is a subfield of artificial intelligence ( ai ) and linguistics focused on enabling computers to understand, interpret, and generate human language in a way that is both meaningful and useful. the key goals of nlp are understanding language, generating language and interacting with humans, the applications of nlp techniques are on text classification, machine translation, speech recognition, text generation, among others. nlp techniques has been used in other fields as dna ( or rna ) sequences, the field is often referred to as computational biology or bioinformatics, specically in the subfield of genomic data analysis',\n",
       " 'where dna and rna sequences can be thought of as \" biological texts \" [ 9 ]. there are a considerably amount of models in nlp, we will mention those are in the beginning a good start for the purpose of our approach. 2. 2. 1. early neural network - based models early neural network - based models refer to the first wave of neural network architectures and methods applied nlp tasks before the dominance of transformer - based models. these models primarily relied on feedforward neural networks, recurrent architectures, and word embeddings, offering improvements over traditional statistical and rule - based methods. rnns recurrent neural networks ( rnns ) belong to a family of neural networks designed for processing sequential data. they can process inputs of variable size, such as images, and can scale to much longer sequences than networks without sequence - based specialization [ 5 ]. 6',\n",
       " '2. 2. natural language processing rnns process data sequentially, meaning the output at a given step depends not only on the current input but also on information from previous steps, they have a hidden state that serves as a \" memory, \" capturing information from previous time steps. the architecture for rnns given a input sequencex = [ x1, x2,..., x t ] where t is the sequence length, we have : ht = f ( wtht−1 + wxxt + b ) yt = g ( wyht + c ) where wt, wx, wy, b, c are parameters of the model andg is the output activation function ( e. g, softmax for classification ) lstms long short - term memory ( lstms ) are a type of recurrent neural network ( rnn ) known as gated rnns. they are based on the idea of introducing self - loops to create paths through which the gradient can flow for long durations. lstms have been found to be extremely suc - cessful in many applications, such as unconstrained handwriting recognition, speech recognition,',\n",
       " 'handwriting generation, machine translation, image captioning, and parsing [ 5 ]. transformer the transformer architecture ( vaswani et al., 2017 ), introduced in the seminal paper \" attention is all you need \" ( 2017 ), revolutionized the field of nlp and later impacted other domains like computer vision. it replaced traditional sequential processing models, such as rnns and lstms, by introducing self - attention mechanisms, which enable efficient parallelization and long - range dependency modeling see figure 2. 1. 7',\n",
       " '2. background figure 2. 1 : the transformer architecture, attention is all you need, 2017. transformer decoder only thetransformerdecoder - onlyarchitecture, popularizedbygpt ( generativepre - trainedtrans - former ), is a variant of the original transformer architecture introduced by vaswani et al. in \" attention is all you need \" ( 2017 ). gpt uses only the decoder part of the transformer for its design, focusing on autoregressive text generation tasks [ 10 ]. for the achitecture see figure 2. 2 8',\n",
       " '2. 2. natural language processing figure 2. 2 : transformer achitecture, improving language understanding by generative pre - training. model - conditioned transformer in this work, we employ the model - conditioned transformer, a gpt - based architecture that processes no only conventional word token but also additional conditioning variables. analogous to music - generation models that incorporate parameters such as tempo and velocity, our model receives two supplementary numerical inputs : 1. performance reward. 2. failure probability. both values are generated during agent execution : the performance reward quantifies the agent ’ s effectiveness, while the failure probability estimates the likelihood of erroneous actions. specifically, both the performance reward and the failure probability are first processed by an auxiliary neural network to produce conditioning vectors. these vectors are then added element -',\n",
       " 'wise to the word - token embeddings, and the resulting combined representations are passed into the transformer decoder, see figure 2. 3. 9',\n",
       " '2. background figure 2. 3 : conditioned transformer, [UNK] and failure probabilitypf are first processed by an auxiliary neural network, then added element - wise to the word - token embeddings. 2. 3. abstract classes the definition of abstracts classes relies in the definition of equivalences class, a partition of a set. a state abstraction is defined as a mapping from an original states ∈s to an abstract state [UNK] [UNK] [UNK] : s [UNK] where [UNK] ∈p ( s ) and fulfills equivalence conditions ( reflexive, symmetric, and transitive ) [ 1 ]. 2. 3. 1. definition π∗ - irrelevance abstraction s1 and s2 are in the same abstract [UNK] ( s1 ) = [UNK] ( s2 ) if π∗ ( s1 ) = π ( s2 ) where π∗is the optimal policy. 2. 3. 2. definition q∗ - irrelevance abstraction s1 ands2 are in the same abstract [UNK] ( s1 ) = [UNK] ( s2 ) if for all actiona∈a, q∗ ( s1, a ) = q∗ ( s2, a ) where q∗ ( s, a ) is the optimalq - value function the maximum expected reward. 2',\n",
       " '. 3. 3. relax q∗ - irrelevance abstraction s1 and s2 are in the same abstract [UNK] ( s1 ) = [UNK] ( s2 ) if :',\n",
       " '[UNK] : [UNK] q∗ ( s1, a ) / d [UNK] = [UNK] q∗ ( s2, a ) / d [UNK] where d is a control parameter ( abstraction level ) [ 1 ]. 10',\n",
       " '11 chapter 3 martest : an nlp approach for test sequence generation we aim to identify the principal potential failures or “ vulnerabilities ” that an agent may en - counter during execution of its policy within an environment. to this end, we leverage all available execution data — hereinafter referred to as logs ( in the context of data testing ). these logs consist of records of episodes, i. e. sequences of states, actions and rewards, from which we seek to predict possible failures. as noted above, if the state spaces is uncountable then the set of all possible episodes is likewise uncountable | sn | > | n | for n = 1, 2, 3, 4.... hence, the task of enumerating all potential failures is unfeasible. as a first step toward tractability, we introduce a reduction via abstract classes, mappingsinto a countable abstraction [UNK] : [UNK] : s [UNK] and | [UNK] | < | n | oncethisreductionisimplemented, theproblemcanbeusingtheresultingcountablespaceand',\n",
       " 'addressed from a language - processing perspective by regarding each abstract class as a distinct lexical token. 3. 1. vocabulary we define a vocabulary for getting a finite set of sequences : let v be a vocabulary, ifw∈v then w = [UNK] or w = a for [UNK] and [UNK] is a set of partitions ofs',\n",
       " '3. martest : an nlp approach for test sequence generation note that defining the vocabulary is the first step in framing our problem as an nlp problem. then, we define the collection of all texts, known as the corpus. 3. 2. corpus let τ be a concrete sequence as s0a0s1a1... snan of state - action tuples and [UNK] an abstraction function, we [UNK] is an abstraction sequence or abstract path for a sequenceτ under function [UNK] ( τ ) = [UNK] if : for [UNK] = [UNK] [UNK]... [UNK] we have [UNK] ( si ) = [UNK] for i = 1, 2, 3... n let [UNK] be a corpus or abstract dataset such [UNK] is composed of abstract [UNK]. if we were to take the [UNK] as the training set for an nlp model, the model would most likely imitate the agent ’ s behavior in an abstract way, since we would be training it on behavior patterns associated with each abstract class. however, our goal is not to replicate the agent ’ s behavior, but rather to identify low - reward regions and potential failures. for this purpose, we',\n",
       " 'extend the definition of our dataset. 3. 3. dataset let d be the training dataset for the conditioned transformer model, such that ( [UNK], [UNK], pf ) ∈d if [UNK] ∈ [UNK], [UNK] is the average reward [UNK], andpf is the failure probability [UNK]. 3. 3. 1. average reward since [UNK] may represent multiple concrete sequences — that is, it may happen [UNK] ( τ1 ) = [UNK] ( τ2 ) = · · · = [UNK] ( τn ) = [UNK] for sequences τ1, τ2,..., τ n of state - action tuples — we take the total rewards associated with eachτi and compute their average to obtain the mean [UNK]. 3. 3. 2. failure probability we can associate [UNK] ∈ [UNK] with a failure probability. given a sequence of concrete trajectories τ1, τ2,..., τ n executed by an agent, we know that each trajectory either resulted in failure or not. 12',\n",
       " '3. 3. dataset that is, we can construct a list of pairs ( τ1, j1 ), ( τ2, j2 ),..., ( τn, jn ), wherejk = 1 if the execution of τk was successful, andjk = 0 otherwise. therefore, wecanderiveacorrespondinglistofpairs ( [UNK], j1 ), ( [UNK], j2 ),..., ( [UNK], jn ), [UNK] ( τk ) = [UNK] for k = 1, 2,..., n. in this way, we obtain a dataset of abstract trajectories labeled as success or failure. with the appropriate model, we can then predict the failure probability for [UNK]. 3. 3. 3. random forest to predict the failure probability of an abstract [UNK], we follow the approach proposed by zolfagharian [ 1, section iv, subsection 7 ], which consists of [UNK] as a binary sequence determined by the presence of abstract states [UNK]. this representation, together with the corresponding failure label, serves as input for arandom forestmodel. assume that [UNK], [UNK],..., [UNK] are all the abstract classes produced by a [UNK]',\n",
       " '. then, for a',\n",
       " 'given abstract [UNK], we can construct the following binary representation : abstract sequence [UNK] [UNK] [UNK]... [UNK] [UNK] 0 1 1... 0 table 3. 1 : binary representation of an abstract trajectory therefore, by constructing a binary representation for each sequence followed by the agent, and labeling the corresponding performance outcome as either failure ( 1 ) or success ( 0 ), we obtain a dataset suitable for training a classification algorithm : index [UNK] [UNK] [UNK]... [UNK] failure i 0 1 1... 0 1 i + 1 1 0 0... 0 0 table 3. 2 : training data representation for a classification model 13',\n",
       " '3. martest : an nlp approach for test sequence generation 3. 4. model - conditioned transformer in this approach, any natural language processing ( nlp ) technique can be employed to generate abstract sequences associated with a certain probability of failure. however, as previously stated, this work proposes the use of a conditioned transformer to identify risk regions. given the dataset d, composed of 3 - tuples ( [UNK], [UNK], pf ), we have a training set for a model - conditioned transformer, as described in section 2. 2. 1. once the conditioned transformer is trained, it can be used to generate abstract sequences given a rewardr and a failure probability pf. these sequences, composed of abstract [UNK] interpreted by the transformer as tokensw, can also be seen as regions associated with a certain likelihood of failure and low reward. the selection and handling of these regions may vary depending on the objective. in section 5. 4, we',\n",
       " 'introduce a method for generating test cases based on these sequences as part of our experimental setup. 3. 4. 1. assumptions in this work, we focus on rl agents with discrete actions we could extend the theory for infinity of set of actions, but for simplicity we will do in a future work. 14',\n",
       " '15 chapter 4 martest pipeline in this chapter, we present the implementation of our approach, called martest - pipeline, a software tool designed to produce risk regions for testing reinforcement learning agents. the implementation is available athttps : / / github. com / lamedinaauniandes / martest - pipeline. we describe the purpose and functionality of each module at a high level, emphasizing that all components are configurable and support module - specific arguments. this overview is intended tohelpthereadernavigatetheimplementationwithgreaterclarity. fordetailsregardingsoftware requirements and environment setup, please refer to appendix a. 1. 4. 1. pipeline implementation our implementation comprises five principal modules, each corresponding to a distinct stage in the log - processing pipeline for any agent ( shown in figure 4. 1 ). each module performs its designated processing tasks and produces the artifacts required by',\n",
       " 'the subsequent module. this modular design offers us considerable flexibility for future work : for instance, researchers wishing to focus on the abstract - class graph can directly leverage the outputs generated by the second module, 2. abstract classes, as the basis for their analyses.',\n",
       " '4. martest pipeline figure 4. 1 : martest pipeline. 4. 2. module 1 : data acquisition and preparation 4. 2. 1. inputs training and execution logs the pipeline begins with two types of logs : training logs, which record the agent ’ s behavior during the training phase, and execution logs, which capture its behavior after training ( i. e., during real - world deployment ). although it is not strictly necessary to provide both types of logs, combining them can help balance the dataset. in particular, if the execution logs contain an insufficient number of failure episodes — as may happen in some cases where the agent performs perfectly with no observed failures — an effective strategy is to augment them with records from the training phase. 4. 2. 2. data balacing in this stage of the pipeline, we specify a mixture ratiop∈ [ 0, 1 ] and sample from the training logs accordingly. since early training episodes may not accurately represent the agent ’ s behavior,',\n",
       " 'we drawm episodes from training data wherem = p · ntrain and are sample according to the probability distribution over episodes introduced by zolfagharian [ 1, seccion iv, ec. ( 6 ) ] : p ( ei ) = i [UNK] j = 1 j, i = 1, 2,..., m, where ntrain denotes the total number of training episodes. this weighting favors latter ( more representative ) episodes while still incorporating early - stage data with more state exploration. the output of this stage is an execution log combining episodes of both training and execution. 16',\n",
       " '4. 3. module 2 : abstract classes 4. 2. 3. q - values during this step in the process, we calculate the total reward obtain for each state and action for all episodes in the logs, based in the definition of theaction - value for a states, actiona, and policy π [ 13, seccion 3. 7, ec. ( 3. 11 ) ] : qπ ( s, a ) = eπ [ gt | st = s, at = a ] = eπ [ [UNK] k = 0 γkrt + k + 1 [UNK] = s, at = a ] this equation calculates the value expected to take an actiona in state s, accounting for future rewards, in our case we do not use expected values because the policy has already been implemented. furthermore, as we have already obtained the rewards, we do not discount the rewards. as a consequence we calculate the q - values as : γ = 1, qπ ( s, a ) = [UNK] k = 0 rt + k + 1 the end result of this is the q - table with the q - value for each state - action pair. 4. 3. module 2 : abstract classes 4. 3. 1. construction once we have the q -',\n",
       " 'table, we proceed to build the abstract classes using the definition of relaxed',\n",
       " 'q∗ - irrelevance abstraction ( see section 2. 3. 3 ). this module generates a json file containing a dictionary, where the keys are the representatives of the abstract classes and the values are lists of the states grouped within each class. since the definition ofq∗ - irrelevance abstraction depends on the abstraction level — a config - urable parameter — the user can set this value according to the desired granularity. note that at this stage, we are constructing the vocabulary referenced in section 3. 1. 4. 4. module 3 4. 4. 1. abstract episodes generation in this module, we proceed to build the [UNK], as defined in section 3. 2, and compute the average reward of each abstract episode. the output consists of csv files containing abstract 17',\n",
       " '4. martest pipeline sequences along with their corresponding average rewards. 4. 5. module 4 4. 5. 1. random forest model in this module, we compute the failure probability for each abstract sequence obtained in the previous module. to achieve this, we generate csv files containing the binary representations of each abstract sequence along with their corresponding failure labels, as described in section 3. 3. 3, in order to estimate the failure probability of each abstract sequence. finally, we generate csv files containing the datasetd, as described in section 3. 3, which serves as input for the transformer model. inthismoduleweusethelibrary sklearnbyscikit - learn, asimpleandefficienttoolforpredictive analysis, we use the classsklearn. ensemble. randomforestclassifierprecisely and show us a great precision and performance. 4. 6. module 5 4. 6. 1. conditioned transformer in this module, we train the conditioned transformer using the datasetd produced in the',\n",
       " 'previous module. once the model is trained, we can also generate abstract sequences given a reward valuer and a failure probabilitypf, thereby identifying regions with potential risk of failure or low reward, as described in section 3. 4. we use thetransformers library fromhugging face, which integrates with pytorch. specifi - cally, we base our implementation on thetransformers. gpt2lmheadmodel class, which follows the gpt - 2 architecture — a decoder - only transformer commonly used for text generation. we modified this architecture to also receive the average reward and failure probability as inputs in order to predict tokens ( i. e., abstract states or regions ) associated with specific levels of risk or failure likelihood. we refer to this modified model as theconditioned transformer. the conditioned transformer is implemented by extending thegpt2lmheadmodel class to 18',\n",
       " '4. 6. module 5 a new class namedcondgpt2. the core idea is to preprocess the average reward and failure probability through a small feedforward network, producing a vector representation that is then added to the token embeddings of the abstract sequence before being passed to the gpt - 2 model. for further details, see section a. 2. 4. 6. 2. test regions cases at the end of the pipeline, we obtain an output in the form of a list ofpossible abstract episodes — that is, sequences of abstract states and actions ( e. g., ’ w2 1 w1 2 w3 1 w6... w5 1 true ’ ) interpreted by the transformer astokens or words. however, since these tokens wi represent abstract classes, they can also be understood assequences of regions and actions. given that the transformer has been designed to receive the average reward and fail - ure probabilityas input parameters, we interpret these region – action sequences aspossible episodes conditioned on those parameters.',\n",
       " 'based on this representation, multiple strategies can be devised to generateconcrete and actionable test cases. in the following chapter, we present the specific testing approaches applied to the selected agents. 19',\n",
       " '4. martest pipeline 20',\n",
       " '21 chapter 5 experimental design we evaluate our implementation on six agents selected from the publicly editable openai gym leaderboard wiki page1. three of these agents implement in thecartpole - v0 environment, and the remaining three are implement thelunarlander - v2. for all agents, we apply the definition ofrelaxedq∗ - irrelevance abstraction ( section 2. 3. 3 ), as we consider this abstraction suitable for capturing the agent ’ s general perception formed through its interaction with the environment and its learning dynamics. the constructed abstraction assigns avalue to each experience and allows for controlling the granularity of the abstract classes through the abstraction - level parameter. in our experiments, this parameter was set tod = 4, chosen as a midpoint based on the experimental settings reported by zolfagharian [ 1, section v, table i ]. for each agent, we introduced a lightweight instrumentation module into the codebase to',\n",
       " 'record logs during both training and execution. from this point onward, we refer to this logging process asinstrumentation. the server used to run the environments in the experiment has the following specifications : • cpu : 3×intel ( r ) xeon ( r ) cpu e5 - 2680 v4 @ 2. 40ghz ( model 79 ) • ram : 7. 8gib ( memtotal ) • primary storage : 1× 60gb ( device sda ) 1https : / / github. com / openai / gym / wiki / leaderboard',\n",
       " '5. experimental design • operating system : ubuntu 22. 04. 5 lts ( jammy ) all experimental data produced by the martest - pipeline software, as a result of each agent ’ s execution, is available athttps : / / zenodo. org / records / 15485620. the specifications of the machines used to run the martest - pipeline software were as follows : • cpu : 48 logical cores ( 2×intel® xeon® silver 4310 cpu @ 2. 10ghz ; 12 cores per socket, 2 threads per core ) • ram : 251. 3gib ( memtotal ) • primary storage : 1 ×7 tb ( devicesda ) • os : ubuntu 22. 04. 4 lts ( jammy ) • gpu : 4×nvidia a40 ( 46068mib vram each ) ; driver version : 560. 35. 03, cuda version : 12. 6 5. 1. cartpole - v0 agents we begin by presenting a description of thecartpole - v0 environment provided by openai gym : “ a pole is attached by an un - actuated joint to a cart, which moves along a frictionless',\n",
       " 'track. the pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart. ” 2',\n",
       " 'therefore, the only actions available to the agent are pushing the cart to the left or to the right, represented by the action seta = { 0, 1 }. the agent observes the state of the environment as a list of four continuous values corresponding to : cart position, cart velocity, pole angle, and pole velocity at the tip ( see figure 5. 1 ). 2https : / / gymnasium. farama. org / environments / classic _ control / cart _ pole / 22',\n",
       " '5. 1. cartpole - v0 agents figure 5. 1 : cartpole - v0. for this environment the selected agents are : shakti kumar ( shakti ) ranked fourth on the openai gym leaderboard, this agent is reported to solve the environment in 0 episodes. the code [ 8, shakti ], dated from december 2019, requiring to set up a custom environment based on python 3. 7. the necessary dependencies are listed in appendix a. 3. due to the legacy state of the code, we installed pytorch manually from pytorch. org using the following command for ubuntu : pip install torch = = 1. 2. 0 + cu92 torchvision = = 0. 4. 0 + cu92 - f https : / / download. pytorch. org / whl / torch _ stable. html kapil chauhan ( kapil ) ranked ninth on the openai gym leaderboard, this agent is reported to solve the environment in 4 episodes. the code [ 4, kapil ], dated from december 2019, also requires setting up a custom environment based on python 3. 7. the required dependencies are listed in appendix a. 4. ni',\n",
       " '##hal t. rao ( nihal )',\n",
       " 'ranked twenty - ninth on the openai gym leaderboard, this agent is reported to solve the en - vironment in 184 episodes. the code [ 11, nihal ], dated from june 2020, also requires setting 23',\n",
       " '5. experimental design up a custom environment based on python 3. 7. the necessary dependencies are listed in ap - pendix a. 5. 5. 2. lunarlander - v2 agents the description of thelunarlander - v2 environment, as provided by openai gym is : “ this environment is a classic rocket trajectory optimization problem. according to pontryagin ’ s maximum principle, it is optimal to fire the engine at full throttle or turn it off. this is the reason why this environment has discrete actions : engine on or off... the landing pad is always at coordinates ( 0, 0 ). the coordinates are the first two numbers in the state vector. landing outside of the landing pad is possible. fuel is infinite, so an agent can learn to fly and then land on its first attempt. ” 3 there are four discrete actions available : 0 – do nothing, 1 – fire left orientation engine, 2 – fire main engine, 3 – fire right orientation engine. the state is represented as an 8 - dimensional vector, which includes : the lander ’ s position',\n",
       " 'coordinates ( x, y ), its linear velocities ( vx, vy ), its angle and angular velocity, and two boolean values indicating whether the left and right legs are in contact with the ground. 4 ( see figure 5. 2. ) for this environment, the selected agents are : 5. 2. 1. sanket thakur ( sanket ) ranked ninth on the openai gym leaderboard, this agent is reported to solve the environment in 454 episodes. the code [ 14, sanket ], dated from april 2020, requires setting up a custom environment based on python 3. 7. the required dependencies are listed in appendix a. 7. 5. 2. 2. mahmood khordoo ( khordoo ) ranked tenth on the openai gym leaderboard, this agent is reported to solve the environment in 602 episodes. the code [ 7, khordoo ], dated from april 2020, also requires setting up a custom 3https : / / gymnasium. farama. org / environments / box2d / lunar _ lander / # description 4https : / / gymnasium. farama. org / environments / box2d / lunar _ lander /',\n",
       " '# description 24',\n",
       " '5. 3. conditioned transformer configurations and metrics figure 5. 2 : lunarlander - v2. environment based on python 3. 7. the required dependencies are listed in appendix a. 6. 5. 2. 3. sigve rokenes ( rokenes ) rankednineteenontheopenaigymleaderboard, thisagentisreportedtosolvetheenvironment in 1, 590 episodes. the code [ 12, rokenes ], dated from january 2019, requires setting up a custom environment based on python 3. 7. the necessary dependencies are listed in appendix a. 8. 5. 3. conditioned transformer configurations and metrics next, we present the configurations and training results of the conditioned transformer for each experiment. we reportvocab _ size, the number of tokens in the model ’ s vocabulary ; n _ embd, the dimensionality of the token embeddings and hidden states ; n _ layer, the number of transformer decoder blocks ( also called layers ) ; and n _ head, the number of attention heads in each self - attention layer. 25',\n",
       " '5. experimental design 5. 3. 1. conditional transformer parameters for cartpole agent vocab _ size n _ embd n _ layer n _ head shakti 101 1024 12 16 kapil 101 1024 12 16 nihal 101 1024 12 16 table 5. 1 : parameters of conditional transformers for cartpole. 5. 3. 2. conditional transformer parameters for lunar - lander agent vocab _ size n _ embd n _ layer n _ head sanket 840 512 6 8 khordoo 711 512 6 8 rokenes 868 512 6 8 table 5. 2 : parameters of conditional transformer for lunar - lander. 5. 3. 3. training and validations loss next, we present the loss metrics for each agent in the cartpole and lunarlander environments. as shown, the loss for all agents decreases over time and converges to zero. this suggests that nlp techniques are sufficient to solve these tasks, which is likely due to the relatively small vocabulary size compared to other nlp problems, such as machine translation or large language models. 26',\n",
       " '5. 3. conditioned transformer configurations and metrics training and evaluation loss per epoch, shakti figure 5. 3 : training and evaluation loss per epoch, shakti training and evaluation loss per epoch, kapil figure 5. 4 : training and evaluation loss per epoch, kapil 27 5. experimental design training and evaluation loss per epoch, nihal figure 5. 5 : training and evaluation loss per epoch, nihal training and evaluation loss per epoch, sanket figure 5. 6 : training and evaluation loss per epoch, sanket 28 5. 4. concrete test case generator training and evaluation loss per epoch, khordoo figure 5. 7 : training and evaluation loss per epoch, khordoo training and evaluation loss per epoch, rokenes figure 5. 8 : training and evaluation loss per epoch, rokenes 5. 4. concrete test case generator once the martest pipeline regionsmodule is deployed, we obtain a list of possible regions associated with failure risk or low rewards. the next step is to take advantage of this output in 29',\n",
       " '5. experimental design order to effectively test the agents. to define our strategy, we refer to the markov property, which states that the environment ’ s response at timet + 1 depends only on the current state and action at timet ( see section 2. 1 ). following this principle, we consider it valid to infer possible failures or low - reward outcomes starting from an initial state. our model provides a list of such high - risk initial regions, indicating situations where failure may occur within a limited number of steps. from each initial region within an abstract sequence produced by the conditioned trans - former ( e. g., w50 in [UNK] = w50 1 w12 0... w 223 ), we sampled 100 states and applied perturbations to each point. specifically, we added noise sampled from a uniform distribution in the range [ −0. 001, 0. 001 ] to each coordinate. this perturbation step enhances the testing process by intro - ducing slight variability into the state space.',\n",
       " 'for each perturbed state, we initialized the environment and executed the agent starting from that point. we then compared the resulting rewards with those obtained by executing the agent under normal conditions. due to the nature of thelunarlander - v2 environment, an additional consideration is required. in this environment, the lander always starts at coordinatex = 0. 0 and approximatelyy≈1. 41 in normalized coordinates. in the internalbox2d coordinate system, this corresponds to a fixed starting point near ( 10. 0, 13. 33 ). therefore, it is necessary to validate whether a given state can be considered a valid initial state for the environment. only those states satisfying this condition are used, and perturbations are applied to them within the uniform random range described earlier. these perturbed states are then used to initialize the environment during testing. 5. 5. an example in this section we show a example produced by test generator, we show you how start and all',\n",
       " 'taking of decisions through the sequence of states. 30',\n",
       " '5. 6. evaluation and results 5. 6. evaluation and results in this section, we present the evaluation and results for each agent. two types of analyses are included : first, we report statistical comparisons of the average rewards. the goal is to determine whether, in general, the regions proposed by the test case generator result in lower rewards compared to normal agent execution. second, we evaluate the probability of success, defined as the proportion of episodes where the agent achieves a total reward greater than or equal to 200. it is important to clarify that using the mean reward alone may be misleading in some cases : an agent could obtain a few episodes with very high rewards, raising the average, while still failing to consistently solve the task ( i. e., achieving fewer successful episodes ). therefore, both the average performance and the success rate are considered in the analysis. 5. 6. 1. cartpole',\n",
       " 'the following section provides a summary of the vocabulary size ( i. e., number of abstract classes ) generated for each agent in cartpole environment, as well as the number of regions identified as initial regions with a high probability of failure or low expected reward by the transformer. we also report the sample size, defined as the number of test cases generated by the concrete test case generator. agent vocabulary size number of initial regions sample size shakti 101 42 4000 kapil 101 42 4200 nihal 101 42 3900 table 5. 3 : summary of vocabulary size, number of initial regions and sample size used for each agent, cartpole. the sample size represents too the total number of episodes executed for both normal execution 31',\n",
       " '5. experimental design and simulated testing. the variation in sample size across agents arises from the design of the test case generator, which attempts to sample up to 100 states per risk region. however, in practice, some abstract classes contain fewer than 100 states, limiting the number of available test cases. we now present the table of statistical comparisons based on average rewards : agent mean sd sem t - statistic p - value µtesting < µexecution shakti sample testing 96. 98 67. 95 1. 07 - 80. 845 p < 1 ×10−10 yes normal execution 185. 30 12. 52 0. 20 kapil sample testing 190. 86 35. 06 0. 54 - 76. 536 p < 1 ×10−10 yes normal execution 235. 06 13. 09 0. 20 nihal sample testing 182. 47 82. 53 1. 32 7. 371 p = 1. 000 no normal execution 169. 23 75. 92 1. 22 table 5. 4 : comparison of sample testing and normal execution for each agent in cartpole envi - ronment. for the agents kapil and shakti ( cartpole ), we can conclude that there is sufficient statistical',\n",
       " 'evidence to support the claim that the test case generator identifies regions of significantly lower reward. in particular, note that for kapil — an agent with a perfect success rate under normal conditions ( as we will discuss later ) — the generator produced test cases with an average reward of 190. 86, compared to 235. 06 under normal execution. similarly, for shakti, the mean reward under generated test cases dropped to 96. 98, nearly half of the 185. 30 average under normal conditions, indicating that the generator successfully identified regions associated with very low performance. we now present the success rate comparison for all agents, where success is defined as achieving 32',\n",
       " '5. 6. evaluation and results a total reward of at least 200 per episode. condition success rate z - statistic p - value ptesting < pexecution shakti sample testing 0. 047 - 14. 418 p = 2. 005 ×10−47 yes normal execution 0. 141 kapil sample testing 0. 428 - 57. 982 p < 1 ×10−10 yes normal execution 1. 000 nihal sample testing 0. 305 8. 135 p = 1. 000 no normal execution 0. 224 table 5. 5 : summary of success rate across agents and conditions in cartpole environment. based on the results, we can conclude that for the agents kapil and shakti ( cartpole ), there is sufficient statistical evidence to affirm that the probability of success is significantly lower in the test case scenarios than under normal execution conditions. notably, even for kapil — an agent that achieved perfect performance under normal conditions, never obtaining a reward below 200 — the test case generator was able to identify scenarios where failures were possible, demonstrating its capacity to expose potential weaknesses in otherwise',\n",
       " 'robust policies. 5. 6. 2. lunar lander the following section provides a summary of the vocabulary size ( i. e., number of abstract classes ) generated for each agent in lunar lander environment, as well the number of regions identified as initial regions with a high probability of failure or low expected reward by the transformer. we also report the sample size, defined as the number of test cases generated by the concrete test case generator. 33',\n",
       " '5. experimental design agent vocabulary size number of initial regions sample size sanket 840 58 781 khordoo 711 58 1892 rokenes 868 64 779 table 5. 6 : summary of vocabulary size, number of initial regions and sample size used for each agent, lunar lander. the sample size represents too the total number of episodes executed for both normal execution and simulated testing. the variation in sample size across agents arises from the design of the test case generator, which attempts to sample up to 100 states per risk region. however, in practice, some abstract classes contain fewer than 100 states, limiting the number of available test cases. this effect is more pronounced in the lunarlander agents, where not all states within an abstract class can be considered valid initial states for the environment. as a result, the effective number of testable samples per region may be reduced, impacting the total sample size.',\n",
       " 'we now present the table of statistical comparisons based on average rewards : 34',\n",
       " '5. 6. evaluation and results agent mean sd sem t - statistic p - value µtesting < µexecution sanket sample testing 234. 24 34. 91 1. 25 - 2. 738 p = 3. 132×10−3 no normal execution 239. 81 44. 94 1. 61 khordoo sample testing 200. 58 71. 18 1. 64 - 4. 980 p < 3. 328×10−7 yes normal execution 212. 07 70. 77 1. 63 rokenes sample testing 263. 70 76. 17 2. 73 - 1. 487 p = 6. 857×10−2 no normal execution 268. 92 61. 64 2. 21 table 5. 7 : comparison of sample testing and normal execution for each agent in lunar lander environment. for the agent khordoo ( lunarlander ), we can conclude that there is sufficient statistical evidence to support the claim that the test case generator identifies regions with significantly lower rewards. however, for the agents sanket and rokenes, we cannot assert that the generator found regions with lower rewards compared to a normal execution. we now present the success rate comparison for all agents, where success is defined as achieving a total reward of at',\n",
       " 'least 200 per episode. 35',\n",
       " '5. experimental design condition success rate z - statistic p - value ptesting < pexecution sanket sample testing 0. 862 - 4. 964 p = 3. 444 ×10−7 yes normal execution 0. 937 khordoo sample testing 0. 627 - 7. 295 p = 1. 490 ×10−13 yes normal execution 0. 737 rokenes sample testing 0. 910 - 2. 529 p = 5. 713 ×10−3 yes normal execution 0. 944 table 5. 8 : summary of success rate across agents and conditions in lunar lander. based on the results, we can conclude that for the agents khordoo, sanket, and rokenes ( lunarlander ), there is sufficient statistical evidence to affirm that the probability of success is significantly lower in the test case scenarios than under normal execution conditions. 5. 6. 3. a small example in this subsection, we present an example in which the shakti agent fails, achieving a total reward of 81. 0. recall that a state in the cartpole environment is represented by anndarray of four components, as described in table 5. 9 : 36',\n",
       " '5. 6. evaluation and results index observation minimum maximum 0 cart position −4. 8 4. 8 1 cart velocity −∞ ∞ 2 pole angle −0. 418 rad ( −24 [UNK] ) 0. 418 rad ( 24 [UNK] ) 3 pole angular velocity −∞ ∞ table 5. 9 : observation space of the cartpole environment : anndarray of shape ( 4, ). we initialize the environment to the following state, generated by our test generator : x0 = [ −0. 73915684 −0. 89601650 −0. 03282847 0. 23753740 ]. figure 5. 9 illustrates this initial configuration : figure 5. 9 : initialstateofthecartpoleenvironmentwithcartposition −0. 73915684, cartvelocity −0. 89601650, pole angle−0. 03282847, and pole angular velocity0. 23753740. figure 5. 10 shows the final state of the episode, wherein the agent has pushed the cart beyond the left boundary : 37',\n",
       " '5. experimental design figure 5. 10 : final state of the cartpole environment with cart position−2. 41205427, cart veloc - ity −1. 42968531, pole angle−0. 06067818, and pole angular velocity−0. 02501499. consequently, the agent performs more “ left ” actions ( 0 ) than “ right ” actions ( 1 ), demon - strating an inability to counteract the tendency toward negative velocity. below, we present the complete sequence of actions taken during the episode : [ −0. 73915684 −0. 8960165 −0. 03282847 0. 2375374 ] 1 1. 0 false [ −0. 75707717 −0. 70044128 −0. 02807772 −0. 06531717 ] 1 1. 0 false [ −0. 771086 −0. 50492826 −0. 02938406 −0. 36672488 ] 0 1. 0 false [ −0. 78118456 −0. 6996206 −0. 03671856 −',\n",
       " '##0. 0834501 ] 0 1. 0 false [ −0. 79517697 −0. 89419749 −0. 03838756 0. 19742567 ] 1 1. 0 false [ −0. 81306092 −0. 69854809 −0. 03443905 −0. 1071154 ] 0 1. 0 false [ −0. 82703189 −0. 89316003 −0. 03658136 0. 17450633 ] 1 1. 0 false [ −0. 84489509 −0. 69753415 −0. 03309123 −0. 12948869 ] 0 1. 0 false [ −0. 85884577 −0. 89216682 −0. 03568101 0. 15257344 ] 1 1. 0 false',\n",
       " '[ −0. 87668911 −0. 69655261 −0. 03262954 −0. 1511491 ] 0 1. 0 false [ −0. 89062016 −0. 89119251 −0. 03565252 0. 13106395 ] 1 1. 0 false [ −0. 90844401 −0. 69557846 −0. 03303124 −0. 17265027 ] 0 1. 0 false 38',\n",
       " '5. 6. evaluation and results [ −0. 92235558 −0. 89021247 −0. 03648425 0. 10943207 ] 1 1. 0 false [ −0. 94015983 −0. 69458722 −0. 0342956 −0. 19453452 ] 0 1. 0 false [ −0. 95405157 −0. 88920223 −0. 03818629 0. 0871354 ] 1 1. 0 false [ −0. 97183562 −0. 69355431 −0. 03644359 −0. 21734658 ] 0 1. 0 false [ −0. 9857067 −0. 88813685 −0. 04079052 0. 06362139 ] 0 1. 0 false [ −1. 00346944 −1. 08265094 −0. 03951809 0. 34316074 ] 1 1. 0 false [ −1. 02512246 −0. 88698972 −0. 03265488 0. 038',\n",
       " '##28275 ] 1 1. 0 false [ −1. 04286225 −0. 69141508 −0. 03188922 −0. 26452163 ] 0 1. 0 false [ −1. 05669055 −0. 8860677 −0. 03717965 0. 01793501 ] 0 1. 0 false [ −1. 07441191 −1. 08063728 −0. 03682095 0. 29865938 ] 1 1. 0 false [ −1. 09602465 −0. 88501034 −0. 03084776 −0. 00540505 ] 1 1. 0 false [ −1. 11372486 −0. 68945988 −0. 03095587 −0. 30765899 ] 0 1. 0 false [ −1. 12751406 −0. 88412737 −0. 03710905 −0. 02489722 ] 0 1. 0 false [ −1. 14519661 −1. 07869804 −0.',\n",
       " '03760699 0. 2558503 ] 1 1. 0 false [ −1. 16677057 −0. 8830599 −0. 03248998 −0. 04845328 ] 1 1. 0 false',\n",
       " '[ −1. 18443176 −0. 68748749 −0. 03345905 −0. 35120743 ] 0 1. 0 false [ −1. 19818151 −0. 88211804 −0. 0404832 −0. 06926014 ] 0 1. 0 false [ −1. 21582387 −1. 07663691 −0. 0418684 0. 21038029 ] 1 1. 0 false [ −1. 23735661 −0. 8809421 −0. 0376608 −0. 09521039 ] 0 1. 0 false [ −1. 25497546 −1. 0755046 −0. 039565 0. 18535677 ] 1 1. 0 false [ −1. 27648555 −0. 87983955 −0. 03585787 −0. 1195402 ] 0 1. 0 false [ −1. 29408234 −1. 07442989 −0. 03824867 0. 16161772 ] 1 1. 0 false',\n",
       " '[ −1. 31557094 −0. 87878184 −0. 03501632 −0. 14288208 ] 0 1. 0 false [ −1. 33314657 −1. 07338526 −0. 03787396 0. 13855142 ] 1 1. 0 false [ −1. 35461428 −0. 87774189 −0. 03510293 −0. 16583546 ] 0 1. 0 false [ −1. 37216912 −1. 07234423 −0. 03841964 0. 11557008 ] 1 1. 0 false [ −1. 393616 −0. 87669345 −0. 03610824 −0. 18898212 ] 0 1. 0 false [ −1. 41114987 −1. 07128071 −0. 03988788 0. 092095 ] 1 1. 0 false [ −1. 43257548 −0. 87561041 −0. 03804598 −0. 21290092 ] 0',\n",
       " '1. 0 false 39',\n",
       " '5. experimental design [ −1. 45008769 −1. 07016834 −0. 042304 0. 06754196 ] 1 1. 0 false [ −1. 47149106 −0. 87446621 −0. 04095316 −0. 23818226 ] 0 1. 0 false [ −1. 48898038 −1. 0689799 −0. 0457168 0. 04130677 ] 1 1. 0 false [ −1. 51035998 −0. 87323321 −0. 04489067 −0. 26544255 ] 0 1. 0 false [ −1. 52782465 −1. 06768666 −0. 05019952 0. 01275036 ] 0 1. 0 false [ −1. 54917838 −1. 26205408 −0. 04994451 0. 28918176 ] 1 1. 0 false [ −1. 57441946 −1. 06625682 −0. 04416088 −0.',\n",
       " '01882548 ] 1 1. 0 false [ −1. 5957446 −0. 87053029 −0. 04453739 −0. 32510813 ] 0 1. 0 false [ −1. 6131552 −1. 06499074 −0. 05103955 −0. 04679636 ] 0 1. 0 false [ −1. 63445502 −1. 25934507 −0. 05197548 0. 22935638 ] 1 1. 0 false [ −1. 65964192 −1. 0635204 −0. 04738835 −0. 07925787 ] 0 1. 0 false [ −1. 68091233 −1. 25793212 −0. 04897351 0. 19810537 ] 1 1. 0 false [ −1. 70607097 −1. 06214514 −0. 0450114 −0. 10961544 ] 0 1. 0 false [ −1. 72731387 −1. 25659416 −0. 04720371 0.',\n",
       " '16853378 ] 1 1. 0 false [ −1. 75244575 −1. 06082944 −0. 04383303 −0. 13865894 ] 1 1. 0 false',\n",
       " '[ −1. 77366234 −0. 86510799 −0. 04660621 −0. 44484192 ] 0 1. 0 false [ −1. 7909645 −1. 05954062 −0. 05550305 −0. 16720693 ] 0 1. 0 false [ −1. 81215532 −1. 25382597 −0. 05884719 0. 10746281 ] 1 1. 0 false [ −1. 83723184 −1. 05791226 −0. 05669793 −0. 20319016 ] 0 1. 0 false [ −1. 85839008 −1. 25217946 −0. 06076174 0. 07108213 ] 1 1. 0 false [ −1. 88343367 −1. 05624141 −0. 05934009 −0. 24013553 ] 0 1. 0 false [ −1. 9045585 −1. 25046768 −0. 0641428 0.',\n",
       " '03325534 ] 0 1. 0 false [ −1. 92956785 −1. 44461395 −0. 0634777 0. 30503082 ] 1 1. 0 false [ −1. 95846013 −1. 24864754 −0. 05737708 −0. 00697668 ] 0 1. 0 false [ −1. 98343308 −1. 44290169 −0. 05751661 0. 26706543 ] 1 1. 0 false [ −2. 01229112 −1. 24700805 −0. 05217531 −0. 04318969 ] 0 1. 0 false [ −2. 03723128 −1. 44134451 −0. 0530391 0. 23258573 ] 1 1. 0 false [ −2. 06605817 −1. 24550639 −0. 04838738 −0. 07634453 ] 1 1. 0 false [ −2. 09096829 −1. 04972537 −0. 04',\n",
       " '##991428 −0. 38389269 ] 0 1. 0 false 40',\n",
       " '5. 6. evaluation and results [ −2. 1119628 −1. 24410443 −0. 05759213 −0. 10735593 ] 0 1. 0 false [ −2. 13684489 −1. 43835581 −0. 05973925 0. 16661531 ] 1 1. 0 false [ −2. 16561201 −1. 24243186 −0. 05640694 −0. 14429925 ] 0 1. 0 false [ −2. 19046064 −1. 43670254 −0. 05929293 0. 13006845 ] 1 1. 0 false [ −2. 21919469 −1. 24078355 −0. 05669156 −0. 18071551 ] 0 1. 0 false [ −2. 24401037 −1. 43505037 −0. 06030587 0. 09355819 ] 1 1. 0 false [ −2. 27271137 −1. 23911823 −0. 0584347 −0. 21752494 ] 0 1. 0 false',\n",
       " '[ −2. 29749374 −1. 43335827 −0. 0627852 0. 05616779 ] 1 1. 0 false [ −2. 3261609 −1. 23739489 −0. 06166185 −0. 25564483 ] 0 1. 0 false [ −2. 3509088 −1. 43158473 −0. 06677474 0. 01696925 ] 0 1. 0 false [ −2. 3795405 −1. 62568867 −0. 06643536 0. 28785909 ] 1 1. 0 true [ −2. 41205427 −1. 42968531 −0. 06067818 −0. 02501499 ] total _ reward : 81. 0 5. 6. 4. threats to validity threats to external validityour experiment aims to validate the use of the test case gener - ator for evaluating reinforcement learning agents. we selected a total of six agents from two',\n",
       " 'different environments — cartpole and lunarlander — ranked at different positions in the public leaderboard. this selection strategy was designed to capture a range of agent performances and architectures, thereby reducing the risk of bias and improving the generalizability of our findings. however, we observed that in some cases, the agents ’ behavior did not align with expectations based on their leaderboard rankings. the most notable example was the agent shakti, which, despite being ranked highest among the cartpole agents, showed the worst performance during normal execution, with a success rate of only 14 %. similarly, although the selected lunarlander agents were distributed across different leader - board positions, all of them achieved average rewards above 200 in our experiments. in partic - ular, the agent rokenes achieved both the highest average reward and the highest success rate, despite not being the top - ranked agent in the public leaderboard. 41',\n",
       " '5. experimental design these discrepancies suggest that leaderboard rankings may not fully reflect agent robustness un - der different testing conditions, and highlight the importance of evaluating agents using diverse and controlled scenarios. threats to internal validityone potential threat to internal validity arises from the specific handling required for the lunarlander environment. in this case, it was necessary to validate whether the sampled states could be considered valid initial states, since the environment can only be initialized from a limited region of the state space. by default, lunarlander starts from normalized coordinates ( x = 0. 0, y≈1. 41 ), which corre - spond to approximately ( 10. 0, 13. 33 ) in box2d coordinates. any perturbation or state sampled outside of this valid initialization region would not be accepted by the environment as a true starting state. as a result, the number of valid samples per region was often reduced, leading to smaller overall',\n",
       " 'sample sizes for these agents. this constraint may have introduced a bias in the testing data or affected the statistical power of the results in the lunarlander experiments. 42',\n",
       " '43 chapter 6 conclusion, limitations and future work 6. 1. conclusion this thesis explores the problem of testing reinforcement learning agents through abstract classes and partitions. we proposedmartest - pipeline, an implementation that generates abstract representations of agent behavior, uses a transformer conditioned on reward and failure probability to identify failure - prone regions, and constructs targeted test cases from those regions. our results demonstrated that this approach can uncover regions of reduced performance in multiple agents, including those with perfect scores under normal execution. these findings support the idea that standard agent evaluations may overlook critical edge cases and that symbolic representations — when combined with generative models — can enhance the testing and understanding of agent behavior. beyondtheimplementation, thisworkproposesabroaderconceptualview : thattheinteraction',\n",
       " 'between an agent and its environment can be seen as a form of emergent language. reward, in this context, is not only a signal of performance, but a reflection of how well the agent has internalized a language and this thesis constitutes the first step in a research direction.',\n",
       " '6. conclusion, limitations and future work 6. 2. limitations this work presents a novel approach to testing reinforcement learning agents using a symbolic generator based on transformer models and abstract state - action representations. while the results are promising and demonstrate the generator ’ s ability to identify failure - prone regions, several limitations must be acknowledged : • dependence on initial state space constraints : in the lunarlander environment, only a limited subset of states can be used as valid initial states. this required an explicit validation step to ensure sampled states could be used for environment initialization. as a consequence, the effective sample size for lunarlander agents was reduced, potentially limiting the statistical power of the results. • uneven sample sizes across agents : the test case generator attempts to sample up to 100 states per high - risk region. however, some abstract classes contained fewer than 100',\n",
       " 'states in both cartpole and lunarlander environments, resulting in variation in sample sizes across agents and environments. this may have introduced some imbalance in the statistical comparisons. • fixed abstraction level : all experiments were conducted using a fixed abstraction level of d = 4. while this value provided a reasonable trade - off between expressiveness and generality, it is possible that a different abstraction depth might yield better or more interpretable results depending on the agent and environment. • limited evaluation scope : the approach was tested only on agents from two envi - ronments : cartpole and lunarlander. although these environments differ in complexity and dynamics, further evaluation on more diverse tasks — particularly continuous control and high - dimensional environments — is needed to assess the general applicability of the method. • transformer conditioning is limited : the transformer model was trained conditioned',\n",
       " 'only on the average reward and failure probability. while this was sufficient for the current experiments, future versions could incorporate richer contextual information ( e. g., policy 44',\n",
       " '6. 3. future work entropy, time horizon, or agent architecture ) to improve the diversity and precision of generated test cases. • mismatchwithleaderboardexpectations : insomecases, theperformanceobservedin normal execution did not match the agent ’ s leaderboard ranking. for example, the shakti agent, which held the top rank among cartpole submissions, showed the lowest success rate in our experiments. this highlights the need to interpret leaderboard positions with caution and emphasizes the value of independent and reproducible testing frameworks. 6. 3. future work one potential extension is to design abstraction mechanisms that are adaptive — i. e., where the abstraction depth is learned based on agent behavior or performance metrics. another important direction for future work is to implement alternative definitions of abstract classes. this is especially relevant, as it may provide better insight into the interaction between the agent and the',\n",
       " 'environment. lastly, incorporating richer conditioning signals into the transformer, or exploring architectures specifically tailored for causal reasoning in rl environments, may improve the quality of the generated test cases and help uncover more nuanced weaknesses in agent policies. this work was guided by the intuition thatreward may be a consequence of a shared language between the agent and the environment. while this idea is still in an early and exploratory stage, it suggests several possible directions for future research. one such direction is to investigate the relationship between the level of abstraction used to represent agent - environment interactions and the rewards obtained. it may be possible to formulate an optimization process — either analytical or learned — that selects the abstraction level most appropriate for a given task or agent architecture. another line of inquiry involves a closer integration between abstract symbolic representations',\n",
       " 'and transformer - based models. rather than conditioning the transformer on simple reward statistics, future models could be trained to evaluate or even generate abstract behavior trajec - tories as structured linguistic sequences — potentially assigning meaning and structure to patterns of interaction. 45',\n",
       " '6. conclusion, limitations and future work although preliminary, these ideas point to a broader aspiration : to better understand the role of representation, communication, and interpretation in reinforcement learning. by approaching the agent - environment loop as a form of emergent communication, we hope to take small steps toward a more unified view of learning, abstraction, and meaning. 46 47 chapter a appendix a. 1. marttest pipeline requirements absl - py = = 1. 4. 0 accelerate = = 1. 6. 0 anyio = = 3. 6. 2 argon2 - cffi = = 21. 3. 0 argon2 - cffi - bindings = = 21. 2. 0 arrow = = 1. 2. 3 asttokens = = 2. 2. 1 astunparse = = 1. 6. 3 attrs = = 22. 2. 0 babel = = 2. 11. 0 backcall = = 0. 2. 0 beautifulsoup4 = = 4. 11. 1 bleach = = 5. 0. 1 box2d - py = = 2. 3. 5 cachetools = = 5. 2. 1 certifi = = 2022. 12. 7',\n",
       " 'cffi = = 1. 15. 1 charset - normalizer = = 3. 0. 1 cloudpickle = = 3. 1. 1',\n",
       " 'a. appendix comm = = 0. 1. 2 contourpy = = 1. 0. 7 cycler = = 0. 11. 0 debugpy = = 1. 6. 5 decorator = = 5. 1. 1 defusedxml = = 0. 7. 1 entrypoints = = 0. 4 executing = = 1. 2. 0 fastjsonschema = = 2. 16. 2 filelock = = 3. 18. 0 flatbuffers = = 23. 1. 4 fonttools = = 4. 38. 0 fqdn = = 1. 5. 1 fsspec = = 2025. 3. 2 gast = = 0. 4. 0 google - auth = = 2. 16. 0 google - auth - oauthlib = = 0. 4. 6 google - pasta = = 0. 2. 0 grpcio = = 1. 51. 1 gym = = 0. 26. 2 gym - notices = = 0. 0. 8 h5py = = 3. 7. 0 huggingface - hub = = 0. 30. 2 idna = = 3. 4 ipykernel = = 6. 20. 2 ip',\n",
       " '##ython = = 8. 8. 0 ipython - genutils = = 0. 2. 0 isoduration = = 20. 11. 0 jedi = = 0. 18. 2 jinja2 = = 3. 1. 2 joblib = = 1. 4. 2 json5 = = 0. 9. 11 48',\n",
       " 'a. 1. marttest pipeline requirements jsonpointer = = 2. 3 jsonschema = = 4. 17. 3 jupyter - events = = 0. 6. 3 jupyter _ client = = 7. 4. 9 jupyter _ core = = 5. 1. 3 jupyter _ server = = 2. 1. 0 jupyter _ server _ terminals = = 0. 4. 4 jupyterlab = = 3. 5. 2 jupyterlab - pygments = = 0. 2. 2 jupyterlab _ server = = 2. 19. 0 keras = = 2. 11. 0 kiwisolver = = 1. 4. 4 libclang = = 15. 0. 6. 1 markdown = = 3. 4. 1 markupsafe = = 2. 1. 1 matplotlib = = 3. 6. 3 matplotlib - inline = = 0. 1. 6 mistune = = 2. 0. 4 mpmath = = 1. 3. 0 nbclassic = = 0. 4. 8 nbclient = = 0. 7. 2 nbconvert = = 7. 2. 8 n',\n",
       " '##bformat = = 5. 7. 3 nest - asyncio = = 1. 5. 6 networkx = = 3. 3 notebook = = 6. 5. 2 notebook _ shim = = 0. 2. 2 numpy = = 1. 24. 1 nvidia - cublas - cu11 = = 11. 10. 3. 66 nvidia - cuda - nvrtc - cu11 = = 11. 7. 99 nvidia - cuda - runtime - cu11 = = 11. 7. 99 nvidia - cudnn - cu11 = = 8. 5. 0. 96 49',\n",
       " 'a. appendix oauthlib = = 3. 2. 2 opt - einsum = = 3. 3. 0 packaging = = 23. 0 pandas = = 2. 2. 3 pandocfilters = = 1. 5. 0 parso = = 0. 8. 3 pexpect = = 4. 8. 0 pickleshare = = 0. 7. 5 pillow = = 9. 4. 0 platformdirs = = 2. 6. 2 prometheus - client = = 0. 15. 0 prompt - toolkit = = 3. 0. 36 protobuf = = 3. 19. 6 psutil = = 5. 9. 4 ptyprocess = = 0. 7. 0 pure - eval = = 0. 2. 2 pyasn1 = = 0. 4. 8 pyasn1 - modules = = 0. 2. 8 pycparser = = 2. 21 pygame = = 2. 1. 0 pygments = = 2. 14. 0 pyparsing = = 3. 0. 9 pyrsistent = = 0. 19. 3 python - dateutil = = 2. 8',\n",
       " '. 2 python - json - logger = = 2. 0. 4 pytz = = 2022. 7. 1 pyyaml = = 6. 0 pyzmq = = 25. 0. 0 regex = = 2024. 11. 6 requests = = 2. 28. 2 requests - oauthlib = = 1. 3. 1 rfc3339 - validator = = 0. 1. 4 50',\n",
       " 'a. 1. marttest pipeline requirements rfc3986 - validator = = 0. 1. 1 rsa = = 4. 9 safetensors = = 0. 5. 3 scikit - learn = = 1. 6. 1 scipy = = 1. 15. 2 send2trash = = 1. 8. 0 six = = 1. 16. 0 sniffio = = 1. 3. 0 soupsieve = = 2. 3. 2. post1 stack - data = = 0. 6. 2 swig = = 4. 3. 0 sympy = = 1. 13. 3 tensorboard = = 2. 11. 2 tensorboard - data - server = = 0. 6. 1 tensorboard - plugin - wit = = 1. 8. 1 tensorflow = = 2. 11. 0 tensorflow - estimator = = 2. 11. 0 tensorflow - io - gcs - filesystem = = 0. 29. 0 termcolor = = 2. 2. 0 terminado = = 0. 17. 1 threadpoolctl = = 3. 6. 0 tinycss2 = = 1. 2. 1 tokenizers = = 0. 21. 1 tom',\n",
       " '##li = = 2. 0. 1 torch = = 2. 1. 2 + cu118 torchaudio = = 2. 1. 2 + cu118 torchvision = = 0. 16. 2 + cu118 tornado = = 6. 2 tqdm = = 4. 67. 1 traitlets = = 5. 8. 1 transformers = = 4. 51. 3 triton = = 2. 1. 0 51',\n",
       " 'a. appendix typing _ extensions = = 4. 4. 0 tzdata = = 2025. 1 uri - template = = 1. 2. 0 urllib3 = = 1. 26. 14 wcwidth = = 0. 2. 6 webcolors = = 1. 12 webencodings = = 0. 5. 1 websocket - client = = 1. 4. 2 werkzeug = = 2. 2. 2 wrapt = = 1. 14. 1 snippet a. 1 : paquetes y versiones a. 2. conditioned transformer model condgpt2 condgpt2 ( ( transformer ) : gpt2model ( ( wte ) : embedding ( 996, 1024 ) ( wpe ) : embedding ( 1024, 1024 ) ( drop ) : dropout ( p = 0. 1, inplace = false ) ( h ) : modulelist ( ( 0 - 11 ) : 12 x gpt2block ( ( ln _ 1 ) : layernorm ( ( 1024, ), eps = 1e - 05, elementwise _ affine = true ) ( attn )',\n",
       " ': gpt2attention ( ( c _ attn ) : conv1d ( nf = 3072, nx = 1024 ) ( c _ proj ) : conv1d ( nf = 1024, nx = 1024 ) ( attn _ dropout ) : dropout ( p = 0. 1, inplace = false ) ( resid _ dropout ) : dropout ( p = 0. 1, inplace = false ) ) ( ln _ 2 ) : layernorm ( ( 1024, ), eps = 1e - 05, elementwise _ affine = true ) ( mlp ) : gpt2mlp ( ( c _ fc ) : conv1d ( nf = 4096, nx = 1024 ) 52',\n",
       " 'a. 3. requirements shakti ( c _ proj ) : conv1d ( nf = 1024, nx = 4096 ) ( act ) : newgeluactivation ( ) ( dropout ) : dropout ( p = 0. 1, inplace = false ) ) ) ) ( ln _ f ) : layernorm ( ( 1024, ), eps = 1e - 05, elementwise _ affine = true ) ) ( lm _ head ) : linear ( in _ features = 1024, out _ features = 996, bias = false ) ( cond _ proj ) : sequential ( ( 0 ) : linear ( in _ features = 2, out _ features = 1024, bias = true ) ( 1 ) : relu ( ) ( 2 ) : linear ( in _ features = 1024, out _ features = 1024, bias = true ) ) ) a. 3. requirements shakti absl - py = = 2. 1. 0 astor = = 0. 8. 1 cachetools = = 4. 2. 4 certifi = = 2025. 1. 31 charset - normalizer = = 3. 4. 1 cloudpickle',\n",
       " '= = 1. 2. 2 cycler = = 0. 11. 0 future = = 1. 0. 0 gast = = 0. 2. 2 google - auth = = 1. 35. 0 google - auth - oauthlib = = 0. 4. 6 google - pasta = = 0. 2. 0 grpcio = = 1. 62. 3 53',\n",
       " 'a. appendix gym = = 0. 15. 4 h5py = = 3. 8. 0 idna = = 3. 10 importlib - metadata = = 6. 7. 0 keras = = 2. 3. 1 keras - applications = = 1. 0. 8 keras - preprocessing = = 1. 1. 2 kiwisolver = = 1. 4. 5 markdown = = 3. 4. 4 markupsafe = = 2. 1. 5 matplotlib = = 3. 1. 2 numpy = = 1. 18. 1 oauthlib = = 3. 2. 2 opencv - python = = 4. 11. 0. 86 opt - einsum = = 3. 3. 0 pillow = = 9. 5. 0 protobuf = = 3. 20. 3 pyasn1 = = 0. 5. 1 pyasn1 - modules = = 0. 3. 0 pyglet = = 1. 3. 2 pyparsing = = 3. 1. 4 python - dateutil = = 2. 9. 0. post0 pyyaml = = 6. 0. 1 requests = = 2. 31',\n",
       " '. 0 requests - oauthlib = = 2. 0. 0 rsa = = 4. 9 scipy = = 1. 4. 1 six = = 1. 17. 0 tensorboard = = 2. 1. 1 tensorflow - estimator = = 2. 1. 0 tensorflow - gpu = = 2. 1. 0 termcolor = = 2. 3. 0 54',\n",
       " 'a. 4. requirements kapil torch = = 1. 2. 0 + cu92 torchvision = = 0. 4. 0 + cu92 typing _ extensions = = 4. 7. 1 urllib3 = = 2. 0. 7 werkzeug = = 2. 2. 3 wrapt = = 1. 16. 0 zipp = = 3. 15. 0 a. 4. requirements kapil absl - py = = 2. 1. 0 astor = = 0. 8. 1 backcall = = 0. 2. 0 cloudpickle = = 1. 2. 2 cycler = = 0. 11. 0 debugpy = = 1. 7. 0 decorator = = 5. 1. 1 entrypoints = = 0. 4 future = = 1. 0. 0 gast = = 0. 2. 2 google - pasta = = 0. 2. 0 grpcio = = 1. 62. 3 gym = = 0. 14. 0 h5py = = 3. 8. 0 importlib - metadata = = 6. 7. 0 ipykernel = = 6. 16. 2 ipython = = 7. 34. 0 jedi =',\n",
       " '= 0. 19. 2 jupyter _ client = = 7. 4. 9 jupyter _ core = = 4. 12. 0 keras = = 2. 3. 1 55',\n",
       " 'a. appendix keras - applications = = 1. 0. 8 keras - preprocessing = = 1. 1. 2 kiwisolver = = 1. 4. 5 markdown = = 3. 4. 4 markupsafe = = 2. 1. 5 matplotlib = = 2. 2. 4 matplotlib - inline = = 0. 1. 6 nest - asyncio = = 1. 6. 0 numpy = = 1. 21. 6 opt - einsum = = 3. 3. 0 packaging = = 24. 0 parso = = 0. 8. 4 pexpect = = 4. 9. 0 pickleshare = = 0. 7. 5 prompt _ toolkit = = 3. 0. 48 protobuf = = 3. 20. 3 psutil = = 7. 0. 0 ptyprocess = = 0. 7. 0 pyglet = = 1. 3. 2 pygments = = 2. 17. 2 pyparsing = = 3. 1. 4 python - dateutil = = 2. 9. 0. post0 pytz = = 2025. 1 pyyaml',\n",
       " '= = 6. 0. 1 pyzmq = = 26. 2. 1 scipy = = 1. 7. 3 six = = 1. 17. 0 tensorboard = = 1. 15. 0 tensorflow = = 1. 15. 0 tensorflow - estimator = = 1. 15. 1 termcolor = = 2. 3. 0 tornado = = 6. 2 56',\n",
       " 'a. 5. requirements nihal traitlets = = 5. 9. 0 typing _ extensions = = 4. 7. 1 wcwidth = = 0. 2. 13 werkzeug = = 2. 2. 3 wrapt = = 1. 16. 0 zipp = = 3. 15. 0 a. 5. requirements nihal absl - py = = 2. 1. 0 astor = = 0. 8. 1 backcall = = 0. 2. 0 cachetools = = 4. 2. 4 certifi = = 2025. 1. 31 charset - normalizer = = 3. 4. 1 cloudpickle = = 1. 2. 2 cycler = = 0. 11. 0 debugpy = = 1. 7. 0 decorator = = 5. 1. 1 entrypoints = = 0. 4 future = = 1. 0. 0 gast = = 0. 2. 2 google - auth = = 1. 35. 0 google - auth - oauthlib = = 0. 4. 6 google - pasta = = 0. 2. 0 grpcio = = 1. 62. 3 gym = = 0. 15. 4 h5py',\n",
       " '= = 3. 8. 0 idna = = 3. 10 importlib - metadata = = 6. 7. 0 ipykernel = = 6. 16. 2 57',\n",
       " 'a. appendix ipython = = 7. 34. 0 jedi = = 0. 19. 2 jupyter _ client = = 7. 4. 9 jupyter _ core = = 4. 12. 0 keras - applications = = 1. 0. 8 keras - preprocessing = = 1. 1. 2 kiwisolver = = 1. 4. 5 markdown = = 3. 4. 4 markupsafe = = 2. 1. 5 matplotlib = = 3. 1. 2 matplotlib - inline = = 0. 1. 6 nest - asyncio = = 1. 6. 0 numpy = = 1. 21. 6 oauthlib = = 3. 2. 2 opencv - python = = 4. 11. 0. 86 opt - einsum = = 3. 3. 0 packaging = = 24. 0 parso = = 0. 8. 4 pexpect = = 4. 9. 0 pickleshare = = 0. 7. 5 prompt _ toolkit = = 3. 0. 48 protobuf = = 3. 20. 3 psutil = = 7. 0. 0 ptyprocess',\n",
       " '= = 0. 7. 0 pyasn1 = = 0. 5. 1 pyasn1 - modules = = 0. 3. 0 pyglet = = 1. 3. 2 pygments = = 2. 17. 2 pyparsing = = 3. 1. 4 python - dateutil = = 2. 9. 0. post0 pyzmq = = 26. 2. 1 requests = = 2. 31. 0 58',\n",
       " 'a. 6. requirements khordoo requests - oauthlib = = 2. 0. 0 rsa = = 4. 9 scipy = = 1. 4. 1 six = = 1. 17. 0 tensorboard = = 2. 1. 1 tensorflow - estimator = = 2. 1. 0 tensorflow - gpu = = 2. 1. 0 termcolor = = 2. 3. 0 tornado = = 6. 2 traitlets = = 5. 9. 0 typing _ extensions = = 4. 7. 1 urllib3 = = 2. 0. 7 wcwidth = = 0. 2. 13 werkzeug = = 2. 2. 3 wrapt = = 1. 16. 0 zipp = = 3. 15. 0 a. 6. requirements khordoo box2d - py = = 2. 3. 8 cloudpickle = = 1. 2. 2 future = = 1. 0. 0 gym = = 0. 15. 4 numpy = = 1. 18. 1 opencv - python = = 4. 11. 0. 86 pillow = = 9. 5. 0 pyglet = = 1. 3. 2 sci',\n",
       " '##py = = 1. 7. 3 six = = 1. 17. 0 torch = = 1. 2. 0 + cu92 torchvision = = 0. 4. 0 + cu92 59',\n",
       " 'a. appendix a. 7. requirements sanket box2d - py = = 2. 3. 4 cloudpickle = = 1. 2. 2 future = = 1. 0. 0 gym = = 0. 15. 4 numpy = = 1. 21. 6 opencv - python = = 4. 11. 0. 86 pillow = = 9. 5. 0 pyglet = = 1. 3. 2 scipy = = 1. 7. 3 six = = 1. 17. 0 torch = = 1. 2. 0 + cu92 torchvision = = 0. 4. 0 + cu92 a. 8. requirements rokenes absl - py = = 2. 1. 0 astor = = 0. 8. 1 box2d - py = = 2. 3. 4 cloudpickle = = 1. 2. 2 future = = 1. 0. 0 gast = = 0. 6. 0 google - pasta = = 0. 2. 0 grpcio = = 1. 62. 3 gym = = 0. 15. 4 h5py = = 3. 8. 0 importlib - metadata = = 6. 7. 0 keras - applications =',\n",
       " '= 1. 0. 8 keras - preprocessing = = 1. 1. 2 markdown = = 3. 4. 4 markupsafe = = 2. 1. 5 60 a. 8. requirements rokenes numpy = = 1. 21. 6 opencv - python = = 4. 11. 0. 86 protobuf = = 3. 20. 3 pyglet = = 1. 3. 2 scipy = = 1. 7. 3 six = = 1. 17. 0 tensorboard = = 1. 14. 0 tensorflow = = 1. 14. 0 tensorflow - estimator = = 1. 14. 0 termcolor = = 2. 3. 0 typing _ extensions = = 4. 7. 1 werkzeug = = 2. 2. 3 wrapt = = 1. 16. 0 zipp = = 3. 15. 0 61',\n",
       " '63 bibliography the references are sorted alphabetically by first author. [ 1 ] lionel c. briand fellow ieee mojtaba bagherzadeh amirhossein zolfagharian, manel ab - dellatif and ramesh s. a search - based testing approach for deep reinforcement learning agents. ieee transactions on software engineering, vol. 49, no. 7 edition, 2023. isbn 0 - 201 - 37921 - x. [ 2 ] matteo biagiola and paolo tonella. testing the plasticity of reinforcement learning based systems. universita della svizzera italiana, switzerland, revised edition, 2022. isbn 0 - 201 - 37921 - x. [ 3 ] matteo biagiola and paolo tonella. testing of deep reinforcement learning agents with surrogate models. universita della svizzera italiana, switzerland, revised edition, 2023. isbn 0 - 201 - 37921 - x. [ 4 ] kapil chauhan. cartpole _ dqn : cartpole _ v0 jupyter notebook. https : / / github. com / kapilnchauhan77 / cartpole _ dqn / blob / master / cartpole',\n",
       " '_ v0. ipynb, 2019. accessed : may 15, 2025.',\n",
       " '[ 5 ] ian goodfellow, yoshua bengio, and aaron courville. deep learning. mit press, 2016. http : / / www. deeplearningbook. org. [ 6 ] ibm. what is natural language processing? https : / / www. ibm. com / think / topics / natural - language - processing, n. d. accessed : may 23, 2025. [ 7 ] mahmood khordoo. deep - reinforcement - learning - with - pytorch : n - step dqn for lunarlander - v2. https : / / github. com / khordoo /',\n",
       " 'bibliography deep - reinforcement - learning - with - pytorch / blob / example / examples / dqn / lunarlander _ v2 - dqn - n - step. py, 2020. accessed : may 15, 2025. [ 8 ] shakti kumar. adaptivesystems : rl _ benchmarks readme. https : / / github. com / shaktikshri / adaptivesystems / blob / master / rl _ benchmarks / readme. md, 2019. ac - cessed : may 15, 2025. [ 9 ] emmanouil d. oikonomou, petros karvelis, nikolaos giannakeas, aristidis vrachatis, evri - pidis glavas, and alexandros t. tzallas. how natural language processing derived tech - niques are used on biological data : a systematic review. network modeling analysis in health informatics and bioinformatics, 13 ( 23 ), 2024. doi 10. 1007 / s13721 - 024 - 00458 - 1. [ 10 ] alec radford, karthik narasimhan, tim salimans, and ilya su',\n",
       " '##tskever. improving language understanding by generative pre - training. https : / / cdn. openai. com / research - covers / language - unsupervised / language _ understanding _ paper. pdf, 2018. openai report.',\n",
       " '[ 11 ] nihal t. rao. rl - double - dqn : double dqn implementation for cartpole - v0. https : / / github. com / nihal - rao / rl - double - dqn, 2020. accessed : may 15, 2025. [ 12 ] sigverokenes. learning - rl / gym / lunarlander - v2 : dqnexampleforlunarlander - v2. https : / / github. com / evgiz / learning - rl / tree / master / gym / lunarlander - v2, 2019. accessed : may 15, 2025. [ 13 ] richard s. sutton and andrew g. barto. reinforcement learning : an introduction. the mit press cambridge, massachusetts, london, england, revised edition, 2015. isbn 0 - 201 - 37921 - x. [ 14 ] sanket thakur. lunarlander _ dqn : dqn implementation for lunarlander - v2. https : / / github. com / sanketsans / openaienv / blob / master / dqn / lunarlander / lunarlander _ dqn. ipynb, 2020. accessed : may 15, 202',\n",
       " '##5. [ 15 ] ashish vaswani, noam shazeer, niki parmar, jakob uszkoreit, llion jones, aidan n. gomez, łukasz kaiser, and illia polosukhin. attention is all you need. in advances in neural information processing systems, volume 30. curran asso - 64',\n",
       " 'bibliography ciates, inc., 2017. url https : / / papers. nips. cc / paper _ files / paper / 2017 / file / 3f5ee243547dee91fbd053c1c4a845aa - paper. pdf. 65 bibliography 66']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_split_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9453f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.create_index(\n",
    "    name=\"tweens\",\n",
    "    dimension =384,\n",
    "    metric=\"dotproduct\", \n",
    "    spec=ServerlessSpec(cloud=\"aws\",region=\"us-east-1\")\n",
    ")\n",
    "print(pc.list_indexes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393f0db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pc.Index(\"tweens\")\n",
    "description = pc.describe_index(\"tweens\")\n",
    "print(description.status) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "088430f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"__default__\",\n",
      "    \"record_count\": \"158\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for namespace in idx.list_namespaces():\n",
    "    print(namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1216369c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 158}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\",device='cpu')\n",
    "embeddings = list(map(lambda x: model.encode(x).tolist(),token_split_texts))\n",
    "idx.delete(delete_all=True )\n",
    "embddings_list = [(str(i),embeddings[i],{\"text\":token_split_texts[i]}) for i in range(len(embeddings))]\n",
    "idx.upsert(embddings_list,namespace=\"Thesis testing RL with NLP techniques\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "061f91eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_0 = idx.query(\n",
    "    vector = [0 for _ in range(384)], \n",
    "    top_k = 2, \n",
    "    include_values = True, \n",
    "    include_metadata=True,\n",
    "    # filter = {\n",
    "    #     \"topic\":{\"$eq\":\"Master thesis\"},\n",
    "    # }    \n",
    ")\n",
    "\n",
    "query_about = idx.query(\n",
    "    vector = model.encode(\"which is the problem?\").tolist(),\n",
    "    top_k = 100, \n",
    "    include_values = True, \n",
    "    include_metadata=True, \n",
    "    # filter = {\n",
    "    #     \"topic\":{\"$eq\": \"Master thesis\"}\n",
    "    # }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bda552b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.249157906: {'text': '................................... 43 6. 2. limitations....................................... 44 6. 3. future work...................................... 45 a. appendix.......................................... 47'}\n",
      "score: 0.202689171: {'text': '1. introduction 4'}\n",
      "score: 0.189448357: {'text': 'taking of decisions through the sequence of states. 30'}\n",
      "score: 0.175964355: {'text': '5. 6. evaluation and results................................. 31 5. 6. 1. cartpole..................................... 31 5. 6. 2. lunar lander.................................. 33 5. 6. 3. a small example................................ 36 5. 6. 4. threats to validity............................... 41 6. conclusion, limitations and future work........................ 43 6. 1. conclusion....'}\n",
      "score: 0.174908161: {'text': '_ v0. ipynb, 2019. accessed : may 15, 2025.'}\n",
      "score: 0.170476913: {'text': '. failure probability............................... 12 3. 3. 3. random forest................................. 13 3. 4. model - conditioned transformer............................ 14 3. 4. 1. assumptions.................................. 14'}\n",
      "score: 0.163377762: {'text': 'a. 1. marttest pipeline requirements........................... 47 a. 2. conditioned transformer model condgpt2..................... 52 a. 3. requirements shakti.................................. 53 a. 4. requirements kapil.................................. 55 a. 5. requirements nihal.................................. 57 a. 6. requirements khordoo................................. 59 a. 7. requirements sanket........'}\n",
      "score: 0.160573959: {'text': '1. 0 false 39'}\n",
      "score: 0.146558762: {'text': '. then, for a'}\n",
      "score: 0.144281387: {'text': '= = 0. 7. 0 pyasn1 = = 0. 5. 1 pyasn1 - modules = = 0. 3. 0 pyglet = = 1. 3. 2 pygments = = 2. 17. 2 pyparsing = = 3. 1. 4 python - dateutil = = 2. 9. 0. post0 pyzmq = = 26. 2. 1 requests = = 2. 31. 0 58'}\n",
      "score: 0.143201828: {'text': 'a. 3. requirements shakti ( c _ proj ) : conv1d ( nf = 1024, nx = 4096 ) ( act ) : newgeluactivation ( ) ( dropout ) : dropout ( p = 0. 1, inplace = false ) ) ) ) ( ln _ f ) : layernorm ( ( 1024, ), eps = 1e - 05, elementwise _ affine = true ) ) ( lm _ head ) : linear ( in _ features = 1024, out _ features = 996, bias = false ) ( cond _ proj ) : sequential ( ( 0 ) : linear ( in _ features = 2, out _ features = 1024, bias = true ) ( 1 ) : relu ( ) ( 2 ) : linear ( in _ features = 1024, out _ features = 1024, bias = true ) ) ) a. 3. requirements shakti absl - py = = 2. 1. 0 astor = = 0. 8. 1 cachetools = = 4. 2. 4 certifi = = 2025. 1. 31 charset - normalizer = = 3. 4. 1 cloudpickle'}\n",
      "score: 0.141241074: {'text': '= = 1. 2. 2 cycler = = 0. 11. 0 future = = 1. 0. 0 gast = = 0. 2. 2 google - auth = = 1. 35. 0 google - auth - oauthlib = = 0. 4. 6 google - pasta = = 0. 2. 0 grpcio = = 1. 62. 3 53'}\n",
      "score: 0.134838104: {'text': '.......................... 60 a. 8. requirements rokenes................................. 60 bibliography.......................................... 63 vii'}\n",
      "score: 0.13080883: {'text': '. 0 requests - oauthlib = = 2. 0. 0 rsa = = 4. 9 scipy = = 1. 4. 1 six = = 1. 17. 0 tensorboard = = 2. 1. 1 tensorflow - estimator = = 2. 1. 0 tensorflow - gpu = = 2. 1. 0 termcolor = = 2. 3. 0 54'}\n",
      "score: 0.124004841: {'text': '...................... 25 5. 3. training and evaluation loss per epoch, shakti.................. 27 5. 4. training and evaluation loss per epoch, kapil................... 27'}\n",
      "score: 0.1239748: {'text': '= 0. 19. 2 jupyter _ client = = 7. 4. 9 jupyter _ core = = 4. 12. 0 keras = = 2. 3. 1 55'}\n",
      "score: 0.122004509: {'text': '##bformat = = 5. 7. 3 nest - asyncio = = 1. 5. 6 networkx = = 3. 3 notebook = = 6. 5. 2 notebook _ shim = = 0. 2. 2 numpy = = 1. 24. 1 nvidia - cublas - cu11 = = 11. 10. 3. 66 nvidia - cuda - nvrtc - cu11 = = 11. 7. 99 nvidia - cuda - runtime - cu11 = = 11. 7. 99 nvidia - cudnn - cu11 = = 8. 5. 0. 96 49'}\n",
      "score: 0.112722397: {'text': '4. 5. module 4........................................ 18 4. 5. 1. random forest model............................. 18 4. 6. module 5........................................ 18 4. 6. 1. conditioned transformer........................... 18 4. 6. 2. test regions cases............................... 19 5. experimental design.................................... 21 5. 1.'}\n",
      "score: 0.112050056: {'text': '4. martest pipeline 20'}\n",
      "score: 0.111673355: {'text': '##py = = 1. 7. 3 six = = 1. 17. 0 torch = = 1. 2. 0 + cu92 torchvision = = 0. 4. 0 + cu92 59'}\n",
      "score: 0.1092453: {'text': 'a. 5. requirements nihal traitlets = = 5. 9. 0 typing _ extensions = = 4. 7. 1 wcwidth = = 0. 2. 13 werkzeug = = 2. 2. 3 wrapt = = 1. 16. 0 zipp = = 3. 15. 0 a. 5. requirements nihal absl - py = = 2. 1. 0 astor = = 0. 8. 1 backcall = = 0. 2. 0 cachetools = = 4. 2. 4 certifi = = 2025. 1. 31 charset - normalizer = = 3. 4. 1 cloudpickle = = 1. 2. 2 cycler = = 0. 11. 0 debugpy = = 1. 7. 0 decorator = = 5. 1. 1 entrypoints = = 0. 4 future = = 1. 0. 0 gast = = 0. 2. 2 google - auth = = 1. 35. 0 google - auth - oauthlib = = 0. 4. 6 google - pasta = = 0. 2. 0 grpcio = = 1. 62. 3 gym = = 0. 15. 4 h5py'}\n",
      "score: 0.101766586: {'text': '...................... 17 4. 3. 1. construction.................................. 17 4. 4. module 3........................................ 17 4. 4. 1. abstract episodes generation......................... 17'}\n",
      "score: 0.0953865051: {'text': 'least 200 per episode. 35'}\n",
      "score: 0.0924844742: {'text': 'ranked twenty - ninth on the openai gym leaderboard, this agent is reported to solve the en - vironment in 184 episodes. the code [ 11, nihal ], dated from june 2020, also requires setting 23'}\n",
      "score: 0.0919303894: {'text': '5. experimental design these discrepancies suggest that leaderboard rankings may not fully reflect agent robustness un - der different testing conditions, and highlight the importance of evaluating agents using diverse and controlled scenarios. threats to internal validityone potential threat to internal validity arises from the specific handling required for the lunarlander environment. in this case, it was necessary to validate whether the sampled states could be considered valid initial states, since the environment can only be initialized from a limited region of the state space. by default, lunarlander starts from normalized coordinates ( x = 0. 0, y≈1. 41 ), which corre - spond to approximately ( 10. 0, 13. 33 ) in box2d coordinates. any perturbation or state sampled outside of this valid initialization region would not be accepted by the environment as a true starting state. as a result, the number of valid samples per region was often reduced, leading to smaller overall'}\n",
      "score: 0.0918417: {'text': '. 2 python - json - logger = = 2. 0. 4 pytz = = 2022. 7. 1 pyyaml = = 6. 0 pyzmq = = 25. 0. 0 regex = = 2024. 11. 6 requests = = 2. 28. 2 requests - oauthlib = = 1. 3. 1 rfc3339 - validator = = 0. 1. 4 50'}\n",
      "score: 0.0879049301: {'text': '5. experimental design • operating system : ubuntu 22. 04. 5 lts ( jammy ) all experimental data produced by the martest - pipeline software, as a result of each agent ’ s execution, is available athttps : / / zenodo. org / records / 15485620. the specifications of the machines used to run the martest - pipeline software were as follows : • cpu : 48 logical cores ( 2×intel® xeon® silver 4310 cpu @ 2. 10ghz ; 12 cores per socket, 2 threads per core ) • ram : 251. 3gib ( memtotal ) • primary storage : 1 ×7 tb ( devicesda ) • os : ubuntu 22. 04. 4 lts ( jammy ) • gpu : 4×nvidia a40 ( 46068mib vram each ) ; driver version : 560. 35. 03, cuda version : 12. 6 5. 1. cartpole - v0 agents we begin by presenting a description of thecartpole - v0 environment provided by openai gym : “ a pole is attached by an un - actuated joint to a cart, which moves along a frictionless'}\n",
      "score: 0.0856304169: {'text': '4. martest pipeline...................................... 15 4. 1. pipeline implementation................................ 15 4. 2. module 1 : data acquisition and preparation..................... 16 4. 2. 1. inputs training and execution logs...................... 16 4. 2. 2. data balacing................................. 16 4. 2. 3. q - values.................................... 17 4. 3. module 2 : abstract classes........'}\n",
      "score: 0.0854349136: {'text': 'a. appendix typing _ extensions = = 4. 4. 0 tzdata = = 2025. 1 uri - template = = 1. 2. 0 urllib3 = = 1. 26. 14 wcwidth = = 0. 2. 6 webcolors = = 1. 12 webencodings = = 0. 5. 1 websocket - client = = 1. 4. 2 werkzeug = = 2. 2. 2 wrapt = = 1. 14. 1 snippet a. 1 : paquetes y versiones a. 2. conditioned transformer model condgpt2 condgpt2 ( ( transformer ) : gpt2model ( ( wte ) : embedding ( 996, 1024 ) ( wpe ) : embedding ( 1024, 1024 ) ( drop ) : dropout ( p = 0. 1, inplace = false ) ( h ) : modulelist ( ( 0 - 11 ) : 12 x gpt2block ( ( ln _ 1 ) : layernorm ( ( 1024, ), eps = 1e - 05, elementwise _ affine = true ) ( attn )'}\n",
      "score: 0.085067749: {'text': '2. 3. 3. relax q∗ - irrelevance abstraction....................... 10 3. martest : an nlp approach for test sequence generation............... 11 3. 1. vocabulary....................................... 11 3. 2. corpus.......................................... 12 3. 3. dataset......................................... 12 3. 3. 1. average reward................................. 12 3. 3. 2'}\n",
      "score: 0.0831155777: {'text': '5. 3. conditioned transformer configurations and metrics training and evaluation loss per epoch, shakti figure 5. 3 : training and evaluation loss per epoch, shakti training and evaluation loss per epoch, kapil figure 5. 4 : training and evaluation loss per epoch, kapil 27 5. experimental design training and evaluation loss per epoch, nihal figure 5. 5 : training and evaluation loss per epoch, nihal training and evaluation loss per epoch, sanket figure 5. 6 : training and evaluation loss per epoch, sanket 28 5. 4. concrete test case generator training and evaluation loss per epoch, khordoo figure 5. 7 : training and evaluation loss per epoch, khordoo training and evaluation loss per epoch, rokenes figure 5. 8 : training and evaluation loss per epoch, rokenes 5. 4. concrete test case generator once the martest pipeline regionsmodule is deployed, we obtain a list of possible regions associated with failure risk or low rewards. the next step is to take advantage of this output in 29'}\n",
      "score: 0.0824103355: {'text': '##ython = = 8. 8. 0 ipython - genutils = = 0. 2. 0 isoduration = = 20. 11. 0 jedi = = 0. 18. 2 jinja2 = = 3. 1. 2 joblib = = 1. 4. 2 json5 = = 0. 9. 11 48'}\n",
      "score: 0.0820398331: {'text': 'a. 1. marttest pipeline requirements rfc3986 - validator = = 0. 1. 1 rsa = = 4. 9 safetensors = = 0. 5. 3 scikit - learn = = 1. 6. 1 scipy = = 1. 15. 2 send2trash = = 1. 8. 0 six = = 1. 16. 0 sniffio = = 1. 3. 0 soupsieve = = 2. 3. 2. post1 stack - data = = 0. 6. 2 swig = = 4. 3. 0 sympy = = 1. 13. 3 tensorboard = = 2. 11. 2 tensorboard - data - server = = 0. 6. 1 tensorboard - plugin - wit = = 1. 8. 1 tensorflow = = 2. 11. 0 tensorflow - estimator = = 2. 11. 0 tensorflow - io - gcs - filesystem = = 0. 29. 0 termcolor = = 2. 2. 0 terminado = = 0. 17. 1 threadpoolctl = = 3. 6. 0 tinycss2 = = 1. 2. 1 tokenizers = = 0. 21. 1 tom'}\n",
      "score: 0.082013607: {'text': '= = 3. 8. 0 idna = = 3. 10 importlib - metadata = = 6. 7. 0 ipykernel = = 6. 16. 2 57'}\n",
      "score: 0.0805177689: {'text': 'vi contents abstract............................................. iv list of figures......................................... viii 1. introduction......................................... 1 2. background......................................... 5 2. 1. reinforment learning, makov process desicion ( mdp )............... 5 2. 2. natural language processing............................. 6'}\n",
      "score: 0.0757026672: {'text': 'a. 4. requirements kapil torch = = 1. 2. 0 + cu92 torchvision = = 0. 4. 0 + cu92 typing _ extensions = = 4. 7. 1 urllib3 = = 2. 0. 7 werkzeug = = 2. 2. 3 wrapt = = 1. 16. 0 zipp = = 3. 15. 0 a. 4. requirements kapil absl - py = = 2. 1. 0 astor = = 0. 8. 1 backcall = = 0. 2. 0 cloudpickle = = 1. 2. 2 cycler = = 0. 11. 0 debugpy = = 1. 7. 0 decorator = = 5. 1. 1 entrypoints = = 0. 4 future = = 1. 0. 0 gast = = 0. 2. 2 google - pasta = = 0. 2. 0 grpcio = = 1. 62. 3 gym = = 0. 14. 0 h5py = = 3. 8. 0 importlib - metadata = = 6. 7. 0 ipykernel = = 6. 16. 2 ipython = = 7. 34. 0 jedi ='}\n",
      "score: 0.0751476288: {'text': ': gpt2attention ( ( c _ attn ) : conv1d ( nf = 3072, nx = 1024 ) ( c _ proj ) : conv1d ( nf = 1024, nx = 1024 ) ( attn _ dropout ) : dropout ( p = 0. 1, inplace = false ) ( resid _ dropout ) : dropout ( p = 0. 1, inplace = false ) ) ( ln _ 2 ) : layernorm ( ( 1024, ), eps = 1e - 05, elementwise _ affine = true ) ( mlp ) : gpt2mlp ( ( c _ fc ) : conv1d ( nf = 4096, nx = 1024 ) 52'}\n",
      "score: 0.0727558136: {'text': 'sample sizes for these agents. this constraint may have introduced a bias in the testing data or affected the statistical power of the results in the lunarlander experiments. 42'}\n",
      "score: 0.0716619492: {'text': 'a. appendix a. 7. requirements sanket box2d - py = = 2. 3. 4 cloudpickle = = 1. 2. 2 future = = 1. 0. 0 gym = = 0. 15. 4 numpy = = 1. 21. 6 opencv - python = = 4. 11. 0. 86 pillow = = 9. 5. 0 pyglet = = 1. 3. 2 scipy = = 1. 7. 3 six = = 1. 17. 0 torch = = 1. 2. 0 + cu92 torchvision = = 0. 4. 0 + cu92 a. 8. requirements rokenes absl - py = = 2. 1. 0 astor = = 0. 8. 1 box2d - py = = 2. 3. 4 cloudpickle = = 1. 2. 2 future = = 1. 0. 0 gast = = 0. 6. 0 google - pasta = = 0. 2. 0 grpcio = = 1. 62. 3 gym = = 0. 15. 4 h5py = = 3. 8. 0 importlib - metadata = = 6. 7. 0 keras - applications ='}\n",
      "score: 0.068939209: {'text': '... 26 5. 3. 3. training and validations loss......................... 26 5. 4. concrete test case generator............................. 29 5. 5. an example....................................... 30'}\n",
      "score: 0.0671815872: {'text': '• chapter 6 – limitations and future work : discusses the current limitations of the approach and outlines possible future directions, including the refinement of abstraction levels, integrationoflearning - basedrepresentations, anddeeperunificationofreinforcement learning and natural language processing techniques. 3'}\n",
      "score: 0.061852932: {'text': '5. experimental design up a custom environment based on python 3. 7. the necessary dependencies are listed in ap - pendix a. 5. 5. 2. lunarlander - v2 agents the description of thelunarlander - v2 environment, as provided by openai gym is : “ this environment is a classic rocket trajectory optimization problem. according to pontryagin ’ s maximum principle, it is optimal to fire the engine at full throttle or turn it off. this is the reason why this environment has discrete actions : engine on or off... the landing pad is always at coordinates ( 0, 0 ). the coordinates are the first two numbers in the state vector. landing outside of the landing pad is possible. fuel is infinite, so an agent can learn to fly and then land on its first attempt. ” 3 there are four discrete actions available : 0 – do nothing, 1 – fire left orientation engine, 2 – fire main engine, 3 – fire right orientation engine. the state is represented as an 8 - dimensional vector, which includes : the lander ’ s position'}\n",
      "score: 0.060593605: {'text': 'a. 6. requirements khordoo requests - oauthlib = = 2. 0. 0 rsa = = 4. 9 scipy = = 1. 4. 1 six = = 1. 17. 0 tensorboard = = 2. 1. 1 tensorflow - estimator = = 2. 1. 0 tensorflow - gpu = = 2. 1. 0 termcolor = = 2. 3. 0 tornado = = 6. 2 traitlets = = 5. 9. 0 typing _ extensions = = 4. 7. 1 urllib3 = = 2. 0. 7 wcwidth = = 0. 2. 13 werkzeug = = 2. 2. 3 wrapt = = 1. 16. 0 zipp = = 3. 15. 0 a. 6. requirements khordoo box2d - py = = 2. 3. 8 cloudpickle = = 1. 2. 2 future = = 1. 0. 0 gym = = 0. 15. 4 numpy = = 1. 18. 1 opencv - python = = 4. 11. 0. 86 pillow = = 9. 5. 0 pyglet = = 1. 3. 2 sci'}\n",
      "score: 0.0601472855: {'text': '6. conclusion, limitations and future work although preliminary, these ideas point to a broader aspiration : to better understand the role of representation, communication, and interpretation in reinforcement learning. by approaching the agent - environment loop as a form of emergent communication, we hope to take small steps toward a more unified view of learning, abstraction, and meaning. 46 47 chapter a appendix a. 1. marttest pipeline requirements absl - py = = 1. 4. 0 accelerate = = 1. 6. 0 anyio = = 3. 6. 2 argon2 - cffi = = 21. 3. 0 argon2 - cffi - bindings = = 21. 2. 0 arrow = = 1. 2. 3 asttokens = = 2. 2. 1 astunparse = = 1. 6. 3 attrs = = 22. 2. 0 babel = = 2. 11. 0 backcall = = 0. 2. 0 beautifulsoup4 = = 4. 11. 1 bleach = = 5. 0. 1 box2d - py = = 2. 3. 5 cachetools = = 5. 2. 1 certifi = = 2022. 12. 7'}\n",
      "score: 0.0596199036: {'text': '= = 6. 0. 1 pyzmq = = 26. 2. 1 scipy = = 1. 7. 3 six = = 1. 17. 0 tensorboard = = 1. 15. 0 tensorflow = = 1. 15. 0 tensorflow - estimator = = 1. 15. 1 termcolor = = 2. 3. 0 tornado = = 6. 2 56'}\n",
      "score: 0.0590724945: {'text': 'table, we proceed to build the abstract classes using the definition of relaxed'}\n",
      "score: 0.05742836: {'text': '5. 1. cartpole - v0 agents figure 5. 1 : cartpole - v0. for this environment the selected agents are : shakti kumar ( shakti ) ranked fourth on the openai gym leaderboard, this agent is reported to solve the environment in 0 episodes. the code [ 8, shakti ], dated from december 2019, requiring to set up a custom environment based on python 3. 7. the necessary dependencies are listed in appendix a. 3. due to the legacy state of the code, we installed pytorch manually from pytorch. org using the following command for ubuntu : pip install torch = = 1. 2. 0 + cu92 torchvision = = 0. 4. 0 + cu92 - f https : / / download. pytorch. org / whl / torch _ stable. html kapil chauhan ( kapil ) ranked ninth on the openai gym leaderboard, this agent is reported to solve the environment in 4 episodes. the code [ 4, kapil ], dated from december 2019, also requires setting up a custom environment based on python 3. 7. the required dependencies are listed in appendix a. 4. ni'}\n",
      "score: 0.0538835526: {'text': 'therefore, the only actions available to the agent are pushing the cart to the left or to the right, represented by the action seta = { 0, 1 }. the agent observes the state of the environment as a list of four continuous values corresponding to : cart position, cart velocity, pole angle, and pole velocity at the tip ( see figure 5. 1 ). 2https : / / gymnasium. farama. org / environments / classic _ control / cart _ pole / 22'}\n",
      "score: 0.0534362793: {'text': 'the subsequent module. this modular design offers us considerable flexibility for future work : for instance, researchers wishing to focus on the abstract - class graph can directly leverage the outputs generated by the second module, 2. abstract classes, as the basis for their analyses.'}\n",
      "score: 0.0517530441: {'text': '= 1. 0. 8 keras - preprocessing = = 1. 1. 2 markdown = = 3. 4. 4 markupsafe = = 2. 1. 5 60 a. 8. requirements rokenes numpy = = 1. 21. 6 opencv - python = = 4. 11. 0. 86 protobuf = = 3. 20. 3 pyglet = = 1. 3. 2 scipy = = 1. 7. 3 six = = 1. 17. 0 tensorboard = = 1. 14. 0 tensorflow = = 1. 14. 0 tensorflow - estimator = = 1. 14. 0 termcolor = = 2. 3. 0 typing _ extensions = = 4. 7. 1 werkzeug = = 2. 2. 3 wrapt = = 1. 16. 0 zipp = = 3. 15. 0 61'}\n",
      "score: 0.0511007309: {'text': '# description 24'}\n",
      "score: 0.0480699539: {'text': '5. 6. evaluation and results index observation minimum maximum 0 cart position −4. 8 4. 8 1 cart velocity −∞ ∞ 2 pole angle −0. 418 rad ( −24 [UNK] ) 0. 418 rad ( 24 [UNK] ) 3 pole angular velocity −∞ ∞ table 5. 9 : observation space of the cartpole environment : anndarray of shape ( 4, ). we initialize the environment to the following state, generated by our test generator : x0 = [ −0. 73915684 −0. 89601650 −0. 03282847 0. 23753740 ]. figure 5. 9 illustrates this initial configuration : figure 5. 9 : initialstateofthecartpoleenvironmentwithcartposition −0. 73915684, cartvelocity −0. 89601650, pole angle−0. 03282847, and pole angular velocity0. 23753740. figure 5. 10 shows the final state of the episode, wherein the agent has pushed the cart beyond the left boundary : 37'}\n",
      "score: 0.0477600098: {'text': 'under normal conditions. statistical analysis confirmed that, for most agents, the generated test cases led to significantly lower success rates or reduced average rewards compared to standard execution. v'}\n",
      "score: 0.0475769043: {'text': '##li = = 2. 0. 1 torch = = 2. 1. 2 + cu118 torchaudio = = 2. 1. 2 + cu118 torchvision = = 0. 16. 2 + cu118 tornado = = 6. 2 tqdm = = 4. 67. 1 traitlets = = 5. 8. 1 transformers = = 4. 51. 3 triton = = 2. 1. 0 51'}\n",
      "score: 0.0473790169: {'text': 'and lunarlander — to generate testing scenarios for each. we analyze two key metrics : the'}\n",
      "score: 0.04708004: {'text': 'a. appendix comm = = 0. 1. 2 contourpy = = 1. 0. 7 cycler = = 0. 11. 0 debugpy = = 1. 6. 5 decorator = = 5. 1. 1 defusedxml = = 0. 7. 1 entrypoints = = 0. 4 executing = = 1. 2. 0 fastjsonschema = = 2. 16. 2 filelock = = 3. 18. 0 flatbuffers = = 23. 1. 4 fonttools = = 4. 38. 0 fqdn = = 1. 5. 1 fsspec = = 2025. 3. 2 gast = = 0. 4. 0 google - auth = = 2. 16. 0 google - auth - oauthlib = = 0. 4. 6 google - pasta = = 0. 2. 0 grpcio = = 1. 51. 1 gym = = 0. 26. 2 gym - notices = = 0. 0. 8 h5py = = 3. 7. 0 huggingface - hub = = 0. 30. 2 idna = = 3. 4 ipykernel = = 6. 20. 2 ip'}\n",
      "score: 0.0468993187: {'text': 'coordinates ( x, y ), its linear velocities ( vx, vy ), its angle and angular velocity, and two boolean values indicating whether the left and right legs are in contact with the ground. 4 ( see figure 5. 2. ) for this environment, the selected agents are : 5. 2. 1. sanket thakur ( sanket ) ranked ninth on the openai gym leaderboard, this agent is reported to solve the environment in 454 episodes. the code [ 14, sanket ], dated from april 2020, requires setting up a custom environment based on python 3. 7. the required dependencies are listed in appendix a. 7. 5. 2. 2. mahmood khordoo ( khordoo ) ranked tenth on the openai gym leaderboard, this agent is reported to solve the environment in 602 episodes. the code [ 7, khordoo ], dated from april 2020, also requires setting up a custom 3https : / / gymnasium. farama. org / environments / box2d / lunar _ lander / # description 4https : / / gymnasium. farama. org / environments / box2d / lunar _ lander /'}\n",
      "score: 0.0460996628: {'text': '2. 2. 1. early neural network - based models..................... 6 2. 3. abstract classes.................................... 10 2. 3. 1. definition π∗ - irrelevance abstraction..................... 10 2. 3. 2. definition q∗ - irrelevance abstraction..................... 10'}\n",
      "score: 0.0457677841: {'text': 'a. appendix oauthlib = = 3. 2. 2 opt - einsum = = 3. 3. 0 packaging = = 23. 0 pandas = = 2. 2. 3 pandocfilters = = 1. 5. 0 parso = = 0. 8. 3 pexpect = = 4. 8. 0 pickleshare = = 0. 7. 5 pillow = = 9. 4. 0 platformdirs = = 2. 6. 2 prometheus - client = = 0. 15. 0 prompt - toolkit = = 3. 0. 36 protobuf = = 3. 19. 6 psutil = = 5. 9. 4 ptyprocess = = 0. 7. 0 pure - eval = = 0. 2. 2 pyasn1 = = 0. 4. 8 pyasn1 - modules = = 0. 2. 8 pycparser = = 2. 21 pygame = = 2. 1. 0 pygments = = 2. 14. 0 pyparsing = = 3. 0. 9 pyrsistent = = 0. 19. 3 python - dateutil = = 2. 8'}\n",
      "score: 0.0457363129: {'text': 'contents 5. 2. lunarlander - v2 agents................................ 24 5. 2. 1. sanket thakur ( sanket )............................ 24 5. 2. 2. mahmood khordoo ( khordoo )........................ 24 5. 2. 3. sigve rokenes ( rokenes )............................ 25 5. 3. conditioned transformer configurations and metrics................ 25 5. 3. 1. conditional transformer parameters for cartpole.............. 26 5. 3. 2. conditional transformer parameters for lunar - lander........'}\n",
      "score: 0.0427207947: {'text': '16853378 ] 1 1. 0 false [ −1. 75244575 −1. 06082944 −0. 04383303 −0. 13865894 ] 1 1. 0 false'}\n",
      "score: 0.0415706635: {'text': 'for each perturbed state, we initialized the environment and executed the agent starting from that point. we then compared the resulting rewards with those obtained by executing the agent under normal conditions. due to the nature of thelunarlander - v2 environment, an additional consideration is required. in this environment, the lander always starts at coordinatex = 0. 0 and approximatelyy≈1. 41 in normalized coordinates. in the internalbox2d coordinate system, this corresponds to a fixed starting point near ( 10. 0, 13. 33 ). therefore, it is necessary to validate whether a given state can be considered a valid initial state for the environment. only those states satisfying this condition are used, and perturbations are applied to them within the uniform random range described earlier. these perturbed states are then used to initialize the environment during testing. 5. 5. an example in this section we show a example produced by test generator, we show you how start and all'}\n",
      "score: 0.0409946442: {'text': '5. experimental design agent vocabulary size number of initial regions sample size sanket 840 58 781 khordoo 711 58 1892 rokenes 868 64 779 table 5. 6 : summary of vocabulary size, number of initial regions and sample size used for each agent, lunar lander. the sample size represents too the total number of episodes executed for both normal execution and simulated testing. the variation in sample size across agents arises from the design of the test case generator, which attempts to sample up to 100 states per risk region. however, in practice, some abstract classes contain fewer than 100 states, limiting the number of available test cases. this effect is more pronounced in the lunarlander agents, where not all states within an abstract class can be considered valid initial states for the environment. as a result, the effective number of testable samples per region may be reduced, impacting the total sample size.'}\n",
      "score: 0.0407562256: {'text': '##991428 −0. 38389269 ] 0 1. 0 false 40'}\n",
      "score: 0.0393657684: {'text': 'track. the pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart. ” 2'}\n",
      "score: 0.0379285812: {'text': 'the following section provides a summary of the vocabulary size ( i. e., number of abstract classes ) generated for each agent in cartpole environment, as well as the number of regions identified as initial regions with a high probability of failure or low expected reward by the transformer. we also report the sample size, defined as the number of test cases generated by the concrete test case generator. agent vocabulary size number of initial regions sample size shakti 101 42 4000 kapil 101 42 4200 nihal 101 42 3900 table 5. 3 : summary of vocabulary size, number of initial regions and sample size used for each agent, cartpole. the sample size represents too the total number of episodes executed for both normal execution 31'}\n",
      "score: 0.03647089: {'text': 'of reaching a states ′ by performing an actiona in state s, r : sxa →r a reward function that determines a reward for a pair of an action and a state. at each time step, the agent uses a mapping from state to probabilities of selecting each possible action. this mapping is called the agent ’ s policyπ : sxa → [ 0, 1 ] considered the solution of the mdp.'}\n",
      "score: 0.0352840424: {'text': 'robust policies. 5. 6. 2. lunar lander the following section provides a summary of the vocabulary size ( i. e., number of abstract classes ) generated for each agent in lunar lander environment, as well the number of regions identified as initial regions with a high probability of failure or low expected reward by the transformer. we also report the sample size, defined as the number of test cases generated by the concrete test case generator. 33'}\n",
      "score: 0.0349836349: {'text': '. 3. 3. relax q∗ - irrelevance abstraction s1 and s2 are in the same abstract [UNK] ( s1 ) = [UNK] ( s2 ) if :'}\n",
      "score: 0.0322961807: {'text': 'bibliography ciates, inc., 2017. url https : / / papers. nips. cc / paper _ files / paper / 2017 / file / 3f5ee243547dee91fbd053c1c4a845aa - paper. pdf. 65 bibliography 66'}\n",
      "score: 0.031701088: {'text': '03760699 0. 2558503 ] 1 1. 0 false [ −1. 16677057 −0. 8830599 −0. 03248998 −0. 04845328 ] 1 1. 0 false'}\n",
      "score: 0.0315885544: {'text': '5. 5. training and evaluation loss per epoch, nihal................... 28 5. 6. training and evaluation loss per epoch, sanket.................. 28 5. 7. training and evaluation loss per epoch, khordoo................. 29 5. 8. training and evaluation loss per epoch, rokenes.................. 29 5. 9. initial state of the cartpole environment with cart position−0. 73915684, cart velocity −0. 89601650, pole angle−0. 03282847, and pole angular velocity0. 23753740. 37 5. 10. final state of the cartpole environment with cart position−2. 41205427, cart ve - locity −1. 42968531, pole angle−0. 06067818, and pole angular velocity−0. 02501499. 38'}\n",
      "score: 0.0300893784: {'text': '[UNK] : [UNK] q∗ ( s1, a ) / d [UNK] = [UNK] q∗ ( s2, a ) / d [UNK] where d is a control parameter ( abstraction level ) [ 1 ]. 10'}\n",
      "score: 0.0278596878: {'text': '5. experimental design figure 5. 10 : final state of the cartpole environment with cart position−2. 41205427, cart veloc - ity −1. 42968531, pole angle−0. 06067818, and pole angular velocity−0. 02501499. consequently, the agent performs more “ left ” actions ( 0 ) than “ right ” actions ( 1 ), demon - strating an inability to counteract the tendency toward negative velocity. below, we present the complete sequence of actions taken during the episode : [ −0. 73915684 −0. 8960165 −0. 03282847 0. 2375374 ] 1 1. 0 false [ −0. 75707717 −0. 70044128 −0. 02807772 −0. 06531717 ] 1 1. 0 false [ −0. 771086 −0. 50492826 −0. 02938406 −0. 36672488 ] 0 1. 0 false [ −0. 78118456 −0. 6996206 −0. 03671856 −'}\n",
      "score: 0.0270280838: {'text': 'a. 1. marttest pipeline requirements jsonpointer = = 2. 3 jsonschema = = 4. 17. 3 jupyter - events = = 0. 6. 3 jupyter _ client = = 7. 4. 9 jupyter _ core = = 5. 1. 3 jupyter _ server = = 2. 1. 0 jupyter _ server _ terminals = = 0. 4. 4 jupyterlab = = 3. 5. 2 jupyterlab - pygments = = 0. 2. 2 jupyterlab _ server = = 2. 19. 0 keras = = 2. 11. 0 kiwisolver = = 1. 4. 4 libclang = = 15. 0. 6. 1 markdown = = 3. 4. 1 markupsafe = = 2. 1. 1 matplotlib = = 3. 6. 3 matplotlib - inline = = 0. 1. 6 mistune = = 2. 0. 4 mpmath = = 1. 3. 0 nbclassic = = 0. 4. 8 nbclient = = 0. 7. 2 nbconvert = = 7. 2. 8 n'}\n",
      "score: 0.0255260468: {'text': 'cartpole - v0 agents................................... 22'}\n",
      "score: 0.0227918625: {'text': 'viii list of figures 2. 1. the transformer architecture, attention is all you need, 2017........... 8 2. 2. transformer achitecture, improving language understanding by generative pre - training.......................................... 9 2. 3. conditionedtransformer, [UNK] pf arefirstprocessedby an auxiliary neural network, then added element - wise to the word - token embeddings. 10 4. 1. martest pipeline..................................... 16 5. 1. cartpole - v0........................................ 23 5. 2. lunarlander - v2................'}\n",
      "score: 0.0204563141: {'text': 'evidence to support the claim that the test case generator identifies regions of significantly lower reward. in particular, note that for kapil — an agent with a perfect success rate under normal conditions ( as we will discuss later ) — the generator produced test cases with an average reward of 190. 86, compared to 235. 06 under normal execution. similarly, for shakti, the mean reward under generated test cases dropped to 96. 98, nearly half of the 185. 30 average under normal conditions, indicating that the generator successfully identified regions associated with very low performance. we now present the success rate comparison for all agents, where success is defined as achieving 32'}\n",
      "score: 0.0195727348: {'text': 'average rewardand theprobability of fault. this is important because agents can achieve high average rewards that may overcompensate for cases with lower performance. however, our results suggest that the proposed technique is capable of identifyingregions of lower reward. for example, in the case of theshakti agent, the average reward during normal execution is 185. 3, whereas in our testing scenarios, it drops to 96. 98. this indicates that our test case generator can uncover regions of reduced performance. on the other hand, even for thekapil agent, which consistently shows a perfect success rate during normal execution, we were able to identify testing regions where the success rate drops significantly — to 42. 8 %. the results indicate that the proposed test case generator was able to identify failure - prone regions and lower rewards regions in multiple agents — even in those with perfect performance'}\n",
      "score: 0.0195484161: {'text': 'a. appendix gym = = 0. 15. 4 h5py = = 3. 8. 0 idna = = 3. 10 importlib - metadata = = 6. 7. 0 keras = = 2. 3. 1 keras - applications = = 1. 0. 8 keras - preprocessing = = 1. 1. 2 kiwisolver = = 1. 4. 5 markdown = = 3. 4. 4 markupsafe = = 2. 1. 5 matplotlib = = 3. 1. 2 numpy = = 1. 18. 1 oauthlib = = 3. 2. 2 opencv - python = = 4. 11. 0. 86 opt - einsum = = 3. 3. 0 pillow = = 9. 5. 0 protobuf = = 3. 20. 3 pyasn1 = = 0. 5. 1 pyasn1 - modules = = 0. 3. 0 pyglet = = 1. 3. 2 pyparsing = = 3. 1. 4 python - dateutil = = 2. 9. 0. post0 pyyaml = = 6. 0. 1 requests = = 2. 31'}\n",
      "score: 0.018661499: {'text': '3. 3. dataset that is, we can construct a list of pairs ( τ1, j1 ), ( τ2, j2 ),..., ( τn, jn ), wherejk = 1 if the execution of τk was successful, andjk = 0 otherwise. therefore, wecanderiveacorrespondinglistofpairs ( [UNK], j1 ), ( [UNK], j2 ),..., ( [UNK], jn ), [UNK] ( τk ) = [UNK] for k = 1, 2,..., n. in this way, we obtain a dataset of abstract trajectories labeled as success or failure. with the appropriate model, we can then predict the failure probability for [UNK]. 3. 3. 3. random forest to predict the failure probability of an abstract [UNK], we follow the approach proposed by zolfagharian [ 1, section iv, subsection 7 ], which consists of [UNK] as a binary sequence determined by the presence of abstract states [UNK]. this representation, together with the corresponding failure label, serves as input for arandom forestmodel. assume that [UNK], [UNK],..., [UNK] are all the abstract classes produced by a [UNK]'}\n",
      "score: 0.0167274475: {'text': '1 chapter 1 introduction reinforment learning ( rl ) and natural language processing ( nlp ) have seen increasing inter - est and research advances in recent years. the combination of deep learning and rl, referred to as deep reinforcement learning ( drl ), has achieved success to solve various decision - making problems such as autonomous driving and robotics, while nlp has experienced a notable suc - cess with the introduction of transformer architecture, particularly the decoder - only transformer model and other architectures, such as recurrent neural networks ( rnns ) and long short - term memory ( lstms ) with their primary distinction lying in their capacity to handle memory with long sequences and parallel processing. drl has recently been applied in many practical contexts. for instance, netflix uses it to recommend which movie to show to a user in order to maximize engagement ( system of recom - mendation ) [ 3 ], microsoft developed personalizer, a service developer that can be use for content'}\n",
      "score: 0.0163679123: {'text': '[ −2. 29749374 −1. 43335827 −0. 0627852 0. 05616779 ] 1 1. 0 false [ −2. 3261609 −1. 23739489 −0. 06166185 −0. 25564483 ] 0 1. 0 false [ −2. 3509088 −1. 43158473 −0. 06677474 0. 01696925 ] 0 1. 0 false [ −2. 3795405 −1. 62568867 −0. 06643536 0. 28785909 ] 1 1. 0 true [ −2. 41205427 −1. 42968531 −0. 06067818 −0. 02501499 ] total _ reward : 81. 0 5. 6. 4. threats to validity threats to external validityour experiment aims to validate the use of the test case gener - ator for evaluating reinforcement learning agents. we selected a total of six agents from two'}\n",
      "score: 0.0160574913: {'text': '5. 6. evaluation and results a total reward of at least 200 per episode. condition success rate z - statistic p - value ptesting < pexecution shakti sample testing 0. 047 - 14. 418 p = 2. 005 ×10−47 yes normal execution 0. 141 kapil sample testing 0. 428 - 57. 982 p < 1 ×10−10 yes normal execution 1. 000 nihal sample testing 0. 305 8. 135 p = 1. 000 no normal execution 0. 224 table 5. 5 : summary of success rate across agents and conditions in cartpole environment. based on the results, we can conclude that for the agents kapil and shakti ( cartpole ), there is sufficient statistical evidence to affirm that the probability of success is significantly lower in the test case scenarios than under normal execution conditions. notably, even for kapil — an agent that achieved perfect performance under normal conditions, never obtaining a reward below 200 — the test case generator was able to identify scenarios where failures were possible, demonstrating its capacity to expose potential weaknesses in otherwise'}\n",
      "score: 0.0133657455: {'text': '[ −0. 87668911 −0. 69655261 −0. 03262954 −0. 1511491 ] 0 1. 0 false [ −0. 89062016 −0. 89119251 −0. 03565252 0. 13106395 ] 1 1. 0 false [ −0. 90844401 −0. 69557846 −0. 03303124 −0. 17265027 ] 0 1. 0 false 38'}\n",
      "score: 0.0120415688: {'text': 'between an agent and its environment can be seen as a form of emergent language. reward, in this context, is not only a signal of performance, but a reflection of how well the agent has internalized a language and this thesis constitutes the first step in a research direction.'}\n",
      "score: 0.0112552643: {'text': 'a. appendix ipython = = 7. 34. 0 jedi = = 0. 19. 2 jupyter _ client = = 7. 4. 9 jupyter _ core = = 4. 12. 0 keras - applications = = 1. 0. 8 keras - preprocessing = = 1. 1. 2 kiwisolver = = 1. 4. 5 markdown = = 3. 4. 4 markupsafe = = 2. 1. 5 matplotlib = = 3. 1. 2 matplotlib - inline = = 0. 1. 6 nest - asyncio = = 1. 6. 0 numpy = = 1. 21. 6 oauthlib = = 3. 2. 2 opencv - python = = 4. 11. 0. 86 opt - einsum = = 3. 3. 0 packaging = = 24. 0 parso = = 0. 8. 4 pexpect = = 4. 9. 0 pickleshare = = 0. 7. 5 prompt _ toolkit = = 3. 0. 48 protobuf = = 3. 20. 3 psutil = = 7. 0. 0 ptyprocess'}\n",
      "score: 0.00973844528: {'text': 'only on the average reward and failure probability. while this was sufficient for the current experiments, future versions could incorporate richer contextual information ( e. g., policy 44'}\n",
      "score: 0.00825500488: {'text': 'a. appendix keras - applications = = 1. 0. 8 keras - preprocessing = = 1. 1. 2 kiwisolver = = 1. 4. 5 markdown = = 3. 4. 4 markupsafe = = 2. 1. 5 matplotlib = = 2. 2. 4 matplotlib - inline = = 0. 1. 6 nest - asyncio = = 1. 6. 0 numpy = = 1. 21. 6 opt - einsum = = 3. 3. 0 packaging = = 24. 0 parso = = 0. 8. 4 pexpect = = 4. 9. 0 pickleshare = = 0. 7. 5 prompt _ toolkit = = 3. 0. 48 protobuf = = 3. 20. 3 psutil = = 7. 0. 0 ptyprocess = = 0. 7. 0 pyglet = = 1. 3. 2 pygments = = 2. 17. 2 pyparsing = = 3. 1. 4 python - dateutil = = 2. 9. 0. post0 pytz = = 2025. 1 pyyaml'}\n",
      "score: 0.00535583496: {'text': 'cffi = = 1. 15. 1 charset - normalizer = = 3. 0. 1 cloudpickle = = 3. 1. 1'}\n",
      "score: 0.00382709503: {'text': 'environment. lastly, incorporating richer conditioning signals into the transformer, or exploring architectures specifically tailored for causal reasoning in rl environments, may improve the quality of the generated test cases and help uncover more nuanced weaknesses in agent policies. this work was guided by the intuition thatreward may be a consequence of a shared language between the agent and the environment. while this idea is still in an early and exploratory stage, it suggests several possible directions for future research. one such direction is to investigate the relationship between the level of abstraction used to represent agent - environment interactions and the rewards obtained. it may be possible to formulate an optimization process — either analytical or learned — that selects the abstraction level most appropriate for a given task or agent architecture. another line of inquiry involves a closer integration between abstract symbolic representations'}\n",
      "score: 0.00220584869: {'text': '5. 6. evaluation and results 5. 6. evaluation and results in this section, we present the evaluation and results for each agent. two types of analyses are included : first, we report statistical comparisons of the average rewards. the goal is to determine whether, in general, the regions proposed by the test case generator result in lower rewards compared to normal agent execution. second, we evaluate the probability of success, defined as the proportion of episodes where the agent achieves a total reward greater than or equal to 200. it is important to clarify that using the mean reward alone may be misleading in some cases : an agent could obtain a few episodes with very high rewards, raising the average, while still failing to consistently solve the task ( i. e., achieving fewer successful episodes ). therefore, both the average performance and the success rate are considered in the analysis. 5. 6. 1. cartpole'}\n",
      "score: 0.00198364258: {'text': '5 chapter 2 background to motivate the problem and to provide an overview of our approach we give the definitions of the concepts required to understand this work, regarding rl, nlp and abstract classes. 2. 1. reinforment learning, makov process desicion ( mdp ) reinforcement learning trains an agent that interacts with an environmento maximize the ob - served reward obtained from the environment after action execution, rl uses a trial and error strategy to explore the environment. at every state, the agent chooses an action to execute from the set of all possible actions for the state, and receive a corresponding reward for the executed action. a markov decision process ( mdp ) is defined as a 4 - tuple < s, a, t, r > where s is the set of continuous non - enumerable states ( | s | ≥ | n | ), a is the set of ( continuous ) actions, t is the transition function wheret : sxaxs → [ 0, 1 ] such thatt ( s, a, s ′ ) determines the probability'}\n",
      "score: 0.00115919113: {'text': '43 chapter 6 conclusion, limitations and future work 6. 1. conclusion this thesis explores the problem of testing reinforcement learning agents through abstract classes and partitions. we proposedmartest - pipeline, an implementation that generates abstract representations of agent behavior, uses a transformer conditioned on reward and failure probability to identify failure - prone regions, and constructs targeted test cases from those regions. our results demonstrated that this approach can uncover regions of reduced performance in multiple agents, including those with perfect scores under normal execution. these findings support the idea that standard agent evaluations may overlook critical edge cases and that symbolic representations — when combined with generative models — can enhance the testing and understanding of agent behavior. beyondtheimplementation, thisworkproposesabroaderconceptualview : thattheinteraction'}\n",
      "score: 0.00109004974: {'text': '5. experimental design condition success rate z - statistic p - value ptesting < pexecution sanket sample testing 0. 862 - 4. 964 p = 3. 444 ×10−7 yes normal execution 0. 937 khordoo sample testing 0. 627 - 7. 295 p = 1. 490 ×10−13 yes normal execution 0. 737 rokenes sample testing 0. 910 - 2. 529 p = 5. 713 ×10−3 yes normal execution 0. 944 table 5. 8 : summary of success rate across agents and conditions in lunar lander. based on the results, we can conclude that for the agents khordoo, sanket, and rokenes ( lunarlander ), there is sufficient statistical evidence to affirm that the probability of success is significantly lower in the test case scenarios than under normal execution conditions. 5. 6. 3. a small example in this subsection, we present an example in which the shakti agent fails, achieving a total reward of 81. 0. recall that a state in the cartpole environment is represented by anndarray of four components, as described in table 5. 9 : 36'}\n",
      "score: -0.00366449356: {'text': '11 chapter 3 martest : an nlp approach for test sequence generation we aim to identify the principal potential failures or “ vulnerabilities ” that an agent may en - counter during execution of its policy within an environment. to this end, we leverage all available execution data — hereinafter referred to as logs ( in the context of data testing ). these logs consist of records of episodes, i. e. sequences of states, actions and rewards, from which we seek to predict possible failures. as noted above, if the state spaces is uncountable then the set of all possible episodes is likewise uncountable | sn | > | n | for n = 1, 2, 3, 4.... hence, the task of enumerating all potential failures is unfeasible. as a first step toward tractability, we introduce a reduction via abstract classes, mappingsinto a countable abstraction [UNK] : [UNK] : s [UNK] and | [UNK] | < | n | oncethisreductionisimplemented, theproblemcanbeusingtheresultingcountablespaceand'}\n",
      "score: -0.00410556793: {'text': '4. 3. module 2 : abstract classes 4. 2. 3. q - values during this step in the process, we calculate the total reward obtain for each state and action for all episodes in the logs, based in the definition of theaction - value for a states, actiona, and policy π [ 13, seccion 3. 7, ec. ( 3. 11 ) ] : qπ ( s, a ) = eπ [ gt | st = s, at = a ] = eπ [ [UNK] k = 0 γkrt + k + 1 [UNK] = s, at = a ] this equation calculates the value expected to take an actiona in state s, accounting for future rewards, in our case we do not use expected values because the policy has already been implemented. furthermore, as we have already obtained the rewards, we do not discount the rewards. as a consequence we calculate the q - values as : γ = 1, qπ ( s, a ) = [UNK] k = 0 rt + k + 1 the end result of this is the q - table with the q - value for each state - action pair. 4. 3. module 2 : abstract classes 4. 3. 1. construction once we have the q -'}\n",
      "score: -0.00415897369: {'text': '4. 6. module 5 a new class namedcondgpt2. the core idea is to preprocess the average reward and failure probability through a small feedforward network, producing a vector representation that is then added to the token embeddings of the abstract sequence before being passed to the gpt - 2 model. for further details, see section a. 2. 4. 6. 2. test regions cases at the end of the pipeline, we obtain an output in the form of a list ofpossible abstract episodes — that is, sequences of abstract states and actions ( e. g., ’ w2 1 w1 2 w3 1 w6... w5 1 true ’ ) interpreted by the transformer astokens or words. however, since these tokens wi represent abstract classes, they can also be understood assequences of regions and actions. given that the transformer has been designed to receive the average reward and fail - ure probabilityas input parameters, we interpret these region – action sequences aspossible episodes conditioned on those parameters.'}\n",
      "score: -0.00454711914: {'text': 'extend the definition of our dataset. 3. 3. dataset let d be the training dataset for the conditioned transformer model, such that ( [UNK], [UNK], pf ) ∈d if [UNK] ∈ [UNK], [UNK] is the average reward [UNK], andpf is the failure probability [UNK]. 3. 3. 1. average reward since [UNK] may represent multiple concrete sequences — that is, it may happen [UNK] ( τ1 ) = [UNK] ( τ2 ) = · · · = [UNK] ( τn ) = [UNK] for sequences τ1, τ2,..., τ n of state - action tuples — we take the total rewards associated with eachτi and compute their average to obtain the mean [UNK]. 3. 3. 2. failure probability we can associate [UNK] ∈ [UNK] with a failure probability. given a sequence of concrete trajectories τ1, τ2,..., τ n executed by an agent, we know that each trajectory either resulted in failure or not. 12'}\n",
      "score: -0.00858640671: {'text': '2. 2. natural language processing rnns process data sequentially, meaning the output at a given step depends not only on the current input but also on information from previous steps, they have a hidden state that serves as a \" memory, \" capturing information from previous time steps. the architecture for rnns given a input sequencex = [ x1, x2,..., x t ] where t is the sequence length, we have : ht = f ( wtht−1 + wxxt + b ) yt = g ( wyht + c ) where wt, wx, wy, b, c are parameters of the model andg is the output activation function ( e. g, softmax for classification ) lstms long short - term memory ( lstms ) are a type of recurrent neural network ( rnn ) known as gated rnns. they are based on the idea of introducing self - loops to create paths through which the gradient can flow for long durations. lstms have been found to be extremely suc - cessful in many applications, such as unconstrained handwriting recognition, speech recognition,'}\n"
     ]
    }
   ],
   "source": [
    "for result in query_about[\"matches\"]: \n",
    "    print(f\"score: {result['score']}: {result['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "56f76fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_query = [result[\"id\"] for result in query_matches_about]\n",
    "res = idx.fetch(ids=ids_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9efdf08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROL_CHARS = ''.join(map(chr, range(0, 32))) + chr(127)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4a336d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\\x0c\\r\\x0e\\x0f\\x10\\x11\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1a\\x1b\\x1c\\x1d\\x1e\\x1f\\x7f'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTROL_CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4d19b127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\\\\\x0b\\\\\\x0c\\\\\\r\\x0e\\x0f\\x10\\x11\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1a\\x1b\\x1c\\x1d\\x1e\\x1f\\x7f'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.escape(CONTROL_CHARS.replace('\\n','').replace('\\t',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ecc24bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xola mundo'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hola mundo\".replace(\"h\",\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7b3c2f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " in s?  False\n",
      "\t in s?  False\n",
      "'\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\\\\\x0b\\\\\\x0c\\\\\\r\\x0e\\x0f\\x10\\x11\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1a\\x1b\\x1c\\x1d\\x1e\\x1f\\x7f'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "CONTROL_CHARS = ''.join(map(chr, list(range(0,32)) + [127]))\n",
    "\n",
    "# 👇 OJO: usa '\\n' y '\\t' (no raw strings)\n",
    "s = CONTROL_CHARS.replace('\\n', '').replace('\\t', '')\n",
    "\n",
    "print('\\n in s? ', '\\n' in s)  # debe ser False\n",
    "print('\\t in s? ', '\\t' in s)  # debe ser False\n",
    "\n",
    "esc = re.escape(s)\n",
    "print(repr(esc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
